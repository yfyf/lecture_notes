\documentclass[12pt]{article}
\usepackage{amsfonts, amsthm, amsmath}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{amssymb} 
\usepackage{epstopdf}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{pb-diagram}
\usepackage{hyperref}
\usepackage{cancel}
\usepackage{bbold}

\setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{0in}
\setlength{\textheight}{9.5in}
\setlength{\topmargin}{0in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\parskip}{0pt}
\setlength{\parindent}{0pt}

\def\CC{\mathbb{C}}
\def\MM{\mathbb{M}}
\def\FF{\mathbb{F}}
\def\PP{\mathbb{P}}
\def\QQ{\mathbb{Q}}
\def\RR{\mathbb{R}}
\def\ZZ{\mathbb{Z}}
\def\NN{\mathbb{N}}
\def\gotha{\mathfrak{a}}
\def\gothb{\mathfrak{b}}
\def\gothm{\mathfrak{m}}
\def\gotho{\mathfrak{o}}
\def\gothp{\mathfrak{p}}
\def\gothq{\mathfrak{q}}
\DeclareMathOperator{\disc}{Disc}
\DeclareMathOperator{\Gal}{Gal}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\Norm}{Norm}
\DeclareMathOperator{\Trace}{Trace}
\DeclareMathOperator{\Cl}{Cl}
\newcommand{\Perp}{\perp \! \! \! \perp}

\def\head#1{\medskip \noindent \textbf{#1}.}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem*{note}{Note}
\newtheorem*{remark}{Remark}
\newtheorem*{claim}{Claim}
\newtheorem*{recall}{Recall}
\newtheorem*{idea}{Idea}
\newtheorem*{notation}{Notation}
\newtheorem*{ap}{Applications}


\begin{document}
\title{Probability \\ 3105}
\author{Based on lectures by\\Dr N Sidorova}
\date{Jan 2012}
\maketitle

\tableofcontents
\setcounter{tocdepth}{4}
\newpage

\section{Rigorous set up}

\subsection{Probability space, events and random variables}
\begin{definition}[$\sigma$-algebra of sets]
Let $\Omega$ be a set and $\Sigma$ be a collection of sets. Then $\Sigma$ is a $\sigma$ - algebra if
\begin{enumerate}
\item $\Omega, \; \phi \in \Sigma$
\item $A \in \Sigma$ then $\Omega/A \in \Sigma$
\item $A_1, A_2, \dots \in \Sigma$ then $\cup_{1}^{\infty}A_i \in \Sigma$
\end{enumerate}
\end{definition}

\begin{definition}[Measure]
$\mu:\Sigma \rightarrow [0,\infty]$ is called a measure if
\begin{enumerate}
\item $\mu(\phi) = 0$
\item $A_1, A_2, \dots$ are disjoint then $\mu(\cup_{1}^{\infty}A_i) = \sum_{1}^{\infty}\mu(A_i)$
\end{enumerate}
\end{definition}

\begin{definition}[Probability Measure] 
A measure $\mu$ is called a Probability Measure denoted by $P$ if \[P(\Omega) =1\]
\end{definition}

\begin{definition}[Probability Space]
A triple $(\Omega, \Sigma, P)$, where $\Omega$ is a set, $\Sigma$ is a $\sigma$-algebra and $P$ is a probability measure, is called a Probability Space.  
\end{definition}

\begin{definition}[Measurable Function]
a function $X$ is called a Measurable function if 
\[\forall B \in \mathcal{B} \quad X^{-1}(B) = \{w\;:\; X(w) \in \mathcal{B} \} \in \Sigma \]
\end{definition}

\begin{definition}[Random Variable] 
A random variable is called a measurable function
\begin{align*}
X:\Omega &\rightarrow \RR\\
(\Omega, \Sigma) &\rightarrow (\RR , \underbrace{\mathcal{B}}_{\mathclap{borel \; \sigma -algebra}})
\end{align*}
\end{definition}

The idea:\\
$\Omega$ - Random outcomes\\
$\Sigma$ - All possible events\\
$P(E)$ - Probability of the event $E$

\begin{example}
Bernulli = "tossing a coin" = "$0$ or $1$ with probability $\frac{1}{2}$
\begin{align*}
&\Omega = \{H,T\}\\
&\Sigma = \{ \{H\},\{T\}, \{H,T\}, \phi \} = 2^{\Omega}\\
&P(\{H\}) = P(\{T\}) = \frac{1}{2}\\
&P(\{H,T\}) = 1\\
&P(\phi) = 0
\end{align*}
\begin{align*}
X:&H\rightarrow 1\\
&T\rightarrow 0
\end{align*}
\[\text{"Probability that $X=1$"} = P(\omega :X(\omega)=1) = P(\{H\}) = \frac{1}{2}\]
\end{example}

\begin{example}
Roll a die, spell the number, take $\#$ of letters
\begin{align*}
&\Omega = \{1,2,3,4,5,6\}\\
&\Sigma = 2^{\Omega} \quad \text{($\sigma$-algebra of all subsets)}\\
&P(\{1\})= \dots =P(\{6\}) = \frac{1}{6}\\
&P(\{1,3,5\}) = \frac{1}{6}+\frac{1}{6}+\frac{1}{6}= \frac{1}{2} \; etc \dots
\end{align*}
\begin{align*}
X:\quad &1\rightarrow 3\\
&2\rightarrow 3\\
&3\rightarrow 5\\
&4\rightarrow 4\\
&5\rightarrow 4\\
&6\rightarrow 3
\end{align*}
\end{example}

\begin{example}
Roll a die, spell the number, take $\#$ of letters but with the possibility of the dice rolling off the table and scoring $0$
\begin{align*}
&\Omega = \{1,2,3,4,5,6,0\}\\
&\Sigma = 2^{\Omega} \quad \text{($\sigma$-algebra of all subsets)}\\
&P(\{1\})= \dots =P(\{6\}) = \frac{1}{6} \quad 
P(\{0\}) = 0
\end{align*}
\begin{align*}
X:\quad  &0\rightarrow 4\\
&1\rightarrow 3\\
&2\rightarrow 3\\
&3\rightarrow 5\\
&4\rightarrow 4\\
&5\rightarrow 4\\
&6\rightarrow 3
\end{align*}
\end{example}

\begin{example}
Tossing a fair coin infinitely many times
\begin{align*}
&\Omega = [0,1]\\
&\Sigma = ?
\end{align*}
we can then represnt each event as a real number in $[0,1]$ for example all events where first three results are $HTH$ rest unknown:
\[\{\omega = 0.101***\} = \left[\frac{5}{8} , \frac{3}{4}\right] \]
All binary intervals must be in $\Sigma$. The minimal $\sigma$-algebra with this property is $\mathcal{B}$ the borel $\sigma$-algebra. 
\[P(\omega : 0.101*****\dots ) = leb\left[\frac{5}{8}, \frac{3}{4}\right] = \frac{1}{8}\]
\[\Rightarrow \text{P is a lebesque measure}\]
some questions that could be asked:\\
if $\omega = \omega_1, \omega_2 , \dots$
\begin{enumerate}[(a)]
\item what is the number in the $n^{th}$ position\\
$[0,1] \rightarrow \RR$\\
$0.\omega_1 \omega_2 \dots \mapsto  \omega_n$
\item how many $1$'s out of the first $n$ tosses?\\
$[0,1] \rightarrow \RR$\\
$0.\omega_1 \omega_2 \dots \mapsto \omega_1 + \dots + \omega_n$
\end{enumerate}
\end{example}

\begin{definition}
An event is an element of $\Sigma$
\end{definition}
Suppose an event $E$ occurs = suppose $\omega \in E$. An event occurs with probability $p= P(E)$. We are interested in: $P(X \in B) \equiv P(\omega \;:\;X(\omega) \subseteq B)$
\begin{definition}
\[\mu_X(B) := P(X\in B), \quad B \in \mathcal{B}\]
This probability measure on $(\RR , \mathcal{B})$ is called the distribution of $X$ or the law of $X$.\\
\end{definition}
$\mathcal{B}$ is generated by $\{(-\infty, t], \; t \in \RR\}$

\subsection{Distribution function}
\begin{definition}
\[F_X(t):= \mu_X((-\infty, t]) = P(x \leq t)\quad t\in \RR\]
$F_X(t)$ is called the distribution function of $X$
\end{definition}

\begin{example}
Bernoullli:
\[\mu_X(B) := P(X\in B) = \begin{cases}
   1 & \text{if }0,1\in B \\
   \frac{1}{2}      & \text{if just one of } 0 ,1 \in B\\
   0    & \text{otherwise}
  \end{cases}
\]
\end{example}

\begin{definition}\label{propFX}
Properties of $F_X$:
\begin{enumerate}
\item $F_X$ is increasing
\item $F_X \rightarrow 1$ as $t \rightarrow \infty$ and $F_X \rightarrow 0$ as $t \rightarrow -\infty$
\item $F_X$ is right continuous
\end{enumerate}
\begin{proof}\quad \\
(1):\quad\\
 $t_1<t_2$
\begin{align*}
F_X(t_1) &=P(x \leq t_1)\\
F_X(t_2) &=P(x \leq t_2).
\end{align*}
\[t_1<t_2 \Rightarrow \{X \leq t_1\} \subset \{X\leq t_2\}\]
\[ \therefore P(x \leq t_1)\leq P(x \leq t_2)\]
(2):\quad\\
 Let $t_n \rightarrow \infty$ (need countability with $\sigma$-algebras).
\[F_X (t_n) = P(x \leq t_n) \rightarrow P\left(\cup_{n \in \mathbb{N}}\{X\leq t_n\}\right) = P(\Omega) = 1\]
Let $t_n \searrow \infty$
\[F_X (t_n) = P(x \leq t_n) \rightarrow P\left(\cap_{n \in \mathbb{N}}\{X\leq t_n\}\right) = P(\phi) = 0\]
(3):\quad\\
Let $t_n \searrow t$
\[F_X (t_n) = P(x \leq t_n) \rightarrow P\left(\cap_{n \in \mathbb{N}}\{X\leq t_n\}\right) = P(x \leq t) = F_X(t)\]
\end{proof}
\end{definition}

\begin{theorem}[Skorokhod Representation]\label{sko}
If $F:\RR \rightarrow [0,1]$ satisfies (1)-(3) from def \ref{PropFX} above then there is a randon variable $X$ on the probability space $([0,1], \mathcal{B}, leb)$ such that
\[F_X = F\]
\begin{idea}
If $F$ is invertable, then take $G=F^{-1}$ and $X(\omega) = G(\omega)$
\[F_X(t) = leb(\omega:X(\omega) \leq t)=F(t)\]
\end{idea}
\begin{proof}
Define $G:[0,1] \rightarrow \RR$
\[G(\omega)= \inf\{t:F(t)> \omega\}\]
Define $X(\omega)=G(\omega)$\\
need to prove: 
\[F_X(u) =leb\{\omega :  G(\omega) \leq u\} \overset{?}{=} F(u)\]
\[i.e \;F_X(u) =leb\{\omega : \inf\{t :F(t)>\omega\}  \leq u\} \overset{?}{=} F(u)\]
It suffices to show:
\[ [0,F(u)) \subset \{\omega :\inf\{t :F(t)>\omega\}  \leq u\} \subset [0,F(u)] \]
\begin{enumerate}[(a)]
\item Let $\omega \in [0,F(u))$ \begin{align*} &\Rightarrow \omega <F(u)\\
&\Rightarrow u \in \{ t :  F(t)> \omega \}\\
&\Rightarrow \inf\{t :  F(t)> \omega \} \leq u \\
&\Rightarrow \omega \in \;\text{"middle set"}\end{align*}
\item Let $\omega$ be such that $\inf\{t :  F(t)> \omega \} \leq u$\\
monotonicity of $F$: \[F(\inf\{t :  F(t)> \omega \})\leq F(u)\]
right continuity:  \[\inf\{F(t) :  F(t)> \omega \} < F(u)\]
\[\omega \leq \inf\{F(t) :  F(t)> \omega \} < F(u)\]
\end{enumerate}
\end{proof}
\end{theorem}

\begin{example}
Uniform Distribution
\[F(t) = \begin{cases}
0 &if \; t<0\\
t &if \; t\in [0,1]\\
1 &if \; t > 1 \end{cases} \]
$X \; ; \; ([0,1], \mathcal{b}, leb)$
\[X(\omega) = \omega \quad \text{(jumps to $\infty, \; -\infty$ outside of $[0,1]$)}\]
$0.\omega_1\omega_2 \omega_3 \dots$ - uniform  random variable on $[0,1]$
\end{example}

\begin{example}
Exponential random variable (with mean $\mu$). 
\[F(t) = \begin{cases}
1-e^{ - t/\mu} &if \;  t\geq 0\\
0 & otherwise \end{cases} \]
\end{example}

\begin{example} Normal, $N(\underset{mean}{\mu}, \underset{variance}{\sigma^2})$
\[F(t) = \frac{1}{\sigma \sqrt{2\pi}}\int_{-\infty}^{t}\exp(- \frac{(u-\mu)^2}{2 \sigma^2})du\]
\end{example}

\begin{example}
Poisson Distribution
\[P(X=k) = e^{-\lambda}\frac{\lambda^k}{k!}, \quad k=0,1,2,\dots\]
$([0,1], \mathcal{B}, leb)$ \begin{alignat*}{2}
&\Omega = \{0,1,2,\dots \} &\quad &\quad\\
&\Sigma = 2^{\Omega} &\quad &\quad\\
&P(\{k\}) = e^{-\lambda}\frac{\lambda^k}{k!} &\quad & \leftarrow \; \text{probability measure complicated}\\
&X: 0 \rightarrow 0 &\quad &\quad\\
& \; \; \: \, : 1 \rightarrow 1 &\quad &\quad\\
&\; \; \: \, : 2 \rightarrow 2 &\quad & \leftarrow \; \text{Simple}\\
& \; \; \: \, :3 \rightarrow 3 &\quad &\quad\\
\end{alignat*}
By using Skorohod Representation, thm \ref{sko}, we keep the probability measure simple and random variable function complicated. 
\end{example}

\begin{definition} If one can write $F_x = \int_{-\infty}^{t}f_X(u)du$ then the law/distribution is called continuous and $f_X$ is called the density.
\[\mu_X((-\infty, t])= F_X(t)du =  \int_{-\infty}^{t}f_X(u)du =  \int_{(-\infty, t]}f_X(u)d\, leb(u)\]
\end{definition}

\begin{remark} \quad \\
\begin{enumerate}[(1)]

\item \begin{alignat*}{2} X \text{ has a densty}  &\Leftrightarrow & &\text{ the law is continuous}\\
&\Leftrightarrow & & \; X \text{ is absolutely continuous w.r.t $leb$ and:} \\ & & & \; f_X = \frac{d\mu_x}{d\, leb}\; \; \text{     (Radon - Nikodym Density)} \end{alignat*}
\item if $F$ is differentiable then \[f_x = F'_x\]
\item Exponential: \begin{align*} F(t) &= \begin{cases}
1-e^{ - t/\mu} &if \;  t\geq 0\\
0 & otherwise \end{cases} \\
f_x(t) &= \begin{cases}
\frac{1}{\mu}e^{- t/\mu} &if \;  t\geq 0\\
0 & otherwise \end{cases} \end{align*} 
Normal:
\[f_X(t) = \frac{1}{\sigma \sqrt{2\pi}}\exp(- \frac{(t-\mu)^2}{2 \sigma^2})\]
\item \[\int_{-\infty}^{\infty}f_X(u)du = 1\]
\[ \lim_{x \rightarrow \infty}\int_{-\infty}^{x}f_X(t)dt = \lim_{x \rightarrow \infty}F(X)\]
\end{enumerate}
\end{remark}

\subsection{Expectation and varience}

\begin{remark}[Reminder from measure theory]\quad \\
\begin{enumerate}  
\item \begin{align*} f &= \sum_{n}^{i=1}a_i{\mathbb 1}_{B_i} \quad \text{(simple functions)}\\
\int f dP &= \sum_{n}^{i=1}a_iP(B_i) \end{align*}
\item $f \geq 0 $, take simple functions $f_n$ where $f_n \nearrow f$. Define
\[\int f dP = \lim_{n \rightarrow \infty}\int f_n dP \quad \in [0,1]\]
\item for arbitrary $f$
\[ f = f^{+}+ ( - f^{-}) \]
Define
\[\int f dP = \int f^{+} dP  - \int f^{-} dP\]
if both are finite.We say the function is non-lebesque measurable otherwise.
\end{enumerate}
\textbf{MCT} suppose $f_n: \Omega \rightarrow [0,\infty]$ and $f_n \nearrow f$ a.s. Then:
\[\int f_n dP \rightarrow \int f dP\]
\textbf{DCT} suppose $f_n \rightarrow f$ a.s. and $|f_n(\omega)| < g(\omega)$ a.s. where $\int g(\omega) < \infty$ Then:
\[\int f_n dP \rightarrow \int f dP\]
\end{remark}

\begin{definition}
Let $X$ be a random variable on $(\Omega , \Sigma , P)$. If $X$ is integrable then \[EX = \int X dP \]
this is called the expectation of $X$. If $X >0$, we allow the case $EX = \infty$
\end{definition}
\begin{definition}
 If $X$ is square integrable ($X^2$ is integrable), then \[VarX =E (X - EX)^2 \]
this is called the variance of $X$. 
\[E (X - EX)^2 = E (X^2 -2X\cdot EX + (EX)^2) = EX^2 -2( EX)^2 + (EX)^2 = EX^2 -( EX)^2\]
\end{definition}

\begin{lemma}If $EX^2< \infty$ then $E|X|<\infty$ and so $EX < \infty$
\begin{proof}
\[E|X| = E|X|\cdot 1 \leq \underbrace{\sqrt{EX^2}}_{finite} \cdot \underbrace{\sqrt{E1^2}}_{1} < \infty\]
\end{proof}
\end{lemma}

\begin{theorem}[Chebyshev inequality] \quad\\
Let $x$ be a non negitive r.v. then for any $c>0$:
\[P(x>c) \leq c^{-1}EX\]
\begin{proof}
Define \[y(\omega) = \begin{cases} c & if \; \omega \;  is \; st \;X(\omega)>c\\
0 & otherwise \end{cases} \]
\[Y\leq X \; a.s.\; \Rightarrow \underbrace{EY}_{ = c\cdot P(x>c)} \leq EX\]
\end{proof}
\end{theorem} 

\begin{theorem} Let $X$ be a random variable on $(\Omega , \Sigma , P)$ and $h:\RR \rightarrow \RR$ integrable on $(\RR , \mathcal{B} , leb)$. Then 
\[Eh(x) = \int h(x) d\mu_X(x) \]
\[
\begin{diagram}
\node{\Omega} \arrow{e,t}{X}  \arrow{se}
\node{\RR}  \arrow{s,r}{h} \\
 \node[2]{\RR}
\end{diagram}
\]
\begin{proof}\quad \\
\begin{enumerate}
\item $h = {\mathbb 1}_B, \; b \in \mathcal{B}$
\[Eh(x) = E{\mathbb 1}_B(x) = 1\cdot P(X \in B)\]
\[\int h(x)d\mu_X(x) = \int {\mathbb 1}_B(x)d\mu_X(x) = 1 \cdot \mu_X(B) = P(X \in B)\]
\item $h = \sum_{i=1}^{m}a_i{\mathbb 1}_{B_i}$\\
\[\text{The formula holds by linearity of the integral}\]
\item $h\geq 0$.
\[\underset{positive)}{\underset{(simple,}{h_n}} \nearrow h \overset{MCT}{\Rightarrow} \int h_nd\mu_x \rightarrow \int hd\mu_x\]
\[\underset{positive)}{\underset{(simple,}{h_n(x)}} \nearrow h(x) \overset{MCT}{\Rightarrow} E h_n(x) \rightarrow Eh(x)\]
\item $h$ arbitrary
\[ h = \underbrace{h^{+}}_{\geq 0} -\underbrace{h^{-}}_{\geq 0} \Rightarrow Eh(x) = \int h(x)d\mu_X(x)\] 
\end{enumerate}
\end{proof}
\end{theorem}

\begin{corollary}
if $X$ has density $h(x)$ then 
\[EX = \int h(x)f(x)dx \]
in particular if $h(x) = x$
\[EX = \int xf(x)dx  \quad \text{(old formula)} \]
and for $h(x) = x^2$
\[EX^2 = \int x^2f(x)dx \]
If $X$ has finitely, or countably, many values 
\[Eh(x) = \sum_{i=1}^{n}h(a_i)\cdot P(x=a_i)\]
in particular if $h(x) = x$
\[EX = \sum_{i=1}{n}a_i\cdot P(x=a_i) \quad \text{(old formula)} \]
\end{corollary}

\begin{example} $X$- Bernulli
\begin{align*} EX &= 1\cdot P(x=1) + 0\cdot P(x=0) = \frac{1}{2}\\
 EX^2 &= 1^2\cdot P(x=1) + 0\cdot P(x=0) = \frac{1}{2}\\
 VarX &= EX^2 - (EX)^2= \frac{1}{2} - \frac{1}{4} = \frac{1}{4} \end{align*}
\end{example}

\begin{example} N(0,1)
\[EX = \int_{-\infty}^{\infty} t \frac{1}{\sqrt{2\pi}}e^{-t^2/2}dt = 0\]
\[EX^2 = \int_{-\infty}^{\infty} t^2 \frac{1}{\sqrt{2\pi}}e^{-t^2/2}dt = 0\]
\end{example}
\begin{example}
$n$ people collecting their suitcases at random \[\begin{array}{cccc}
1, &2, &\dots, &n\\
\downarrow &\downarrow & &\downarrow \\
\sigma(1) &\sigma(2) &\dots &\sigma(n) \end{array}\]
Pick a random permutaton $\sigma$ uniformly with probability $\frac{1}{n!}$\\
Probability of everyone getting wrong suitcase?\\
Expected number of correct suitcases?
\[N=X_1 + X_2 +\dots + X_n \quad X_i = \begin{cases} 1 & \text{if the ith passenger collected correct suitcase}\\
0 & \text{otherwise} \end{cases} \]
\[EN = \sum_{i=1}^{n}EX_i = \sum_{i=1}^{n}1\cdot P(X_i = 1) = \sum_{i=1}^{n}\frac{(n-1)!}{n!} = \frac{1}{n}n = 1\]
\end{example}

\section{Independence}

\begin{definition} Let $(\Omega , \Sigma , P)$ be a probability space. Events $A,B \in \Sigma$ are independent if 
\[P(A \cap B) = P(A)\cdot P(B)\]
two $\sigma$-algebras $\Sigma_1 , \Sigma_2 \subset \Sigma$ are independent if 
\[P(A \cap B) = P(A)\cdot P(B) \quad \text{for any} \; A \in \Sigma_1 \; B \in \Sigma_2 \]
finitely many $\sigma$-algebras $\Sigma_1 , \Sigma_2 , \dots , \Sigma_n \subset \Sigma$ are independent if 
\[P\left(\cap-{i=1}^{n}A_i\right) = \prod_{i=1}^nP(A_i) \; whenever \; A_i \in \Sigma_i \; 1\geq i \leq n \]
a sequence of  $\sigma$-algebras $\Sigma_1 , \Sigma_2 , \dots \subset \Sigma$ are independent for any $n$\\
let $X,Y$ be random variables, they are independent if 
\[\sigma(x) \coprod \sigma(y) \]
Recall: 
\[\sigma(x) = \{ X^{-1}(B), \; B \in \mathcal{B}\} = \{X \in B, \; B \in \mathcal{B}\}\]
"information which we can get from $X$".
\end{definition} 

\begin{example} \quad \\
$X \sim$ coin toss: Bernulli\\
$Y \sim$ roll a die: $P(x=1) = \dots = P(x=6) = \frac{1}{2}$
\begin{align*}
&\Omega = \{0,1\} \times \{1,2,3,4,5,6\}\\
&\Sigma = 2^{\Omega}\\
&P(\text{each point}) = \frac{1}{2}
\end{align*} 

\[\begin{diagram}
\node{ \begin{array}{ccc} \; & 0 &1\\ 1 &. &.\\ 2 &. &. \\ 3&. &. \\ 4 &. &. \\ 5 &. &. \\ 6 &. &. \end{array}} \arrow{e,t}{X}  \arrow{ese,b}{Y}
\node{\{0,1\}}  \\
 \node[3]{\{1,2,3,4,5,6\}}
\end{diagram} \]

\[ \left.\begin{aligned}
        X(\omega_1, \omega_2) &= \omega_1\\
        Y(\omega_1, \omega_2) &= \omega_2
       \end{aligned}
 \right\}
 \qquad \text{are they independent?}\]

\begin{align*}
\sigma(x) &= \{\phi, \Sigma, \underbrace{\{(1,i), \; i = 1\dots 6\}}_{\{X=1\}} , \underbrace{\{(0,i), \; i = 1\dots 6\}}_{\{X=0\}}\}\\
\sigma(y) &= \{\phi, \Sigma,\{y=1\}, \dots \{y=6\}, \{y=3\, or \, 4 \, or \, 6\} \: etc \dots\}
\end{align*}
\[\underbrace{P\{X=1, \; Y=6\} = \frac{1}{12}, \quad P\{X=1\}= \frac{1}{2} \; P\{Y=6\} = \frac{1}{6}}_{\text{\large $\frac{1}{12}= \frac{1}{2}\cdot \frac{1}{6}$}}\]
similarly check this for all pairs of sets!
\end{example}

\begin{definition}
Let $\mathcal{I}$ be a collection of sets, it is called a $\pi$-system if $\forall A,B \in \mathcal{I}$ \[\quad A\cap B \in \mathcal{I}\]
\end{definition}
\begin{example}
\[
 \left.\begin{aligned}  &\left.\begin{aligned}
       &\{(-\infty, t), \; t \in \RR\} \\
       &\{(-\infty, t], \; t \in \RR\} \\
&\{\{(a, b): a<b\}, \phi\}\end{aligned} \right\}
 \: \text{generate $\mathcal{B}$} \\
& \; \: \{\{1\},\{2\},\{3\},\phi\}
       \end{aligned} \;
 \right\}
 \quad \text{$\pi$-system on $\mathcal{B}$}\]
\end{example}

\begin{example}
\[
 \left.\begin{aligned} 
       &\{ \{x<t\}, \; t \in \RR\} \\
        &\{ \{x\leq t\}, \; t \in \RR\}
       \end{aligned} \; \right\}
 \quad \text{$\pi$-system generating } \sigma(x)\]
because 
\[\{x<t\}\cap \{x<s\} = \{x<\min(t,s)\} \; etc\]
\end{example}
\begin{example}
If $X,\; \underbrace{\text{takes finitely or countably many values}}_{discrete} \; a_1, a_2 , \dots$
\[\{\{x=a_1\}, \{x=a_2\}, \dots \ \phi \} - \text{$\pi$-system generating } \sigma(x)\]
\end{example}

\begin{theorem}\label{ind1}
Let $(\Omega, \Sigma)$ be a set with $\sigma$-algebra and $mathcal{I}$ be a $\pi$-system generating $\Sigma$. Let $\mu_1 , \mu_2$ be measures such that \begin{enumerate}
\item $\mu_1(\Omega) = \mu_2(\Omega) < \infty$
\item $\mu_1(I) = \mu_2(I)$ for any $I \in \mathcal{I}$ \end{enumerate}
then \[\mu_1(A) = \mu_2(A)\quad \forall A \in \Sigma\]
\begin{proof} Stated without proof \end{proof}
\end{theorem}

\begin{theorem}
Let $X,Y$ be rv's and $\mathcal{I}, \mathcal{J}$ be $\pi$-systems generating $\sigma(x)$ and $\sigma(y)$.
\[ \mathcal{I}\; and  \mathcal{J} \text{ are independent} \Rightarrow X \;and\; Y \text{ are independent}\]
 \begin{proof} Stated without proof \end{proof}
\end{theorem}

\begin{corollary}\quad \\
\begin{enumerate}[(a)]
\item To check the independence of $X$ and $Y$ it suffices to check \[P(X<t, Y<s) = P(X<t)P(Y<s)\]
\item if $X$ and $Y$ are discrete, taking values $a_1, a_2, \dots$ and $b_1,b_2,\dots$ then
\[P(X=a_i,Y=b_i)=P(X=a_i)P(Y=b_i) \quad\forall i,j\]
\end{enumerate}

\begin{proof}[Proof of (a)] fix $I \in \mathcal{I}$
\begin{align*} \mu_1(B) &= P(I\cap B) \quad on\; \sigma(Y)\\
\mu_2(B) &= P(I)P(B) \end{align*}
 $\mu_1 , \mu_2$ are measures  such that  $\mu_1(\Omega) = \mu_2(\Omega) = P(I) < \infty$ and they agree on $\mathcal{J}$
\begin{align*}
&\overset{Thm}{\underset{\ref{ind1}}{\Rightarrow}} \mu_1(B) = \mu_2(B) \quad \forall B \in \sigma(Y)\\
&\;\Rightarrow P(X=a_i,Y=b_i)=P(X=a_i)P(Y=b_i) \quad\forall I \in \mathcal{I}, \; B \in \sigma(Y) \end{align*}
\end{proof}
\begin{proof}[Proof of (b)] fix $B \in \sigma(Y)$. Define \begin{align*}
v_1(A) &= P(A \cap B) \quad on\;\sigma(X)\\
v_2(A) &=P(A)P(B) \end{align*}
$v_1, v_2$ are measures such that $v_1(\Omega)=v_2(\omega)= P(B)<\infty$ and they agree on $\mathcal{I}$
\begin{align*}
&\overset{Thm}{\underset{\ref{ind1}}{\Rightarrow}} \text{they agree on }\sigma(X)\\
&\Rightarrow P(A\cap B) = P(A)P(B) \quad \text{whenever }A\in \sigma(X), \; B \in \sigma(Y) \end{align*}
\end{proof}
\end{corollary} 

\begin{example}
$X_1, X_2,\dots$ \begin{itemize}
\item independent
\item each $X_i$ has a prescribed distribution function $F_i$
\end{itemize}
do they always exist?\\\quad\\
\textbf{Trick Model}:\\
It suffices to construct $U_1, U_2, \dots$, which are independent and have uniform distribution, because $X_1 = G_1(U_1), X_2=G_2(U_2), \dots $ generalised as in the Skorokhod representation, \ref{sko}.
\[G=F^{-1} \qquad G(\omega) = \inf\{t:F(t)>\omega\} \]
\begin{itemize}
\item $X_i$ has distribution $F_i$
\item they are independent since $U_1, U_2, \dots$ are independent \end{itemize}
How do we construct $U_1, U_2, \dots$
\[([0,1] , \mathcal{B}, leb) \quad U(\omega)=\omega \; (Uniform!) = .\omega_1\omega_2\dots\]
$\omega_1,\omega_2,\dots$ are \begin{enumerate}[(a)]
\item bernulli
\item independent
\end{enumerate}
\[P(\omega_2 = 1) = \frac{1}{2}\quad \underset{\frac{1}{4}}{P(\omega_1 = 0, \omega_2= 1)} = \underset{\frac{1}{2}}{P(\omega_1 = 0)}\underset{\frac{1}{2}}{P( \omega_2= 1)}\]
$\omega= .\omega_1\omega_2\dots$
\begin{align*}
U_1(\omega) &= .\omega_1\omega_2\omega_{25}\omega_{100}\\
U_2(\omega) &= .\omega_2\omega_3\omega_{200}\\
U_3(\omega) &= .\omega_4\omega_{500}\omega_{1000}
\end{align*}
Uniform + Independent
\end{example}

\subsection{Finite and infinite occurance of Events}
\begin{example}
\[\begin{array}{cccccc}
X_1, &X_2 &X_3 &X_4 &\dots &Bernulli\\
1, &0, &1, &1, &\dots & \end{array}\]
"there will be infinitely many $1$'s in the sequence with probability $1$." \[E_n= \{X_n = 1\}\]
\[ E = \{\text{ infinitely many $1$'s in the sequence}\} = \{\forall N \in \NN \exists n \geq N \;X_n=1\}\]
\[ = \bigcap_{N=1}^{\infty}\bigcup_{n \geq N}\{X_n = 1\}\] \end{example}

\begin{notation}Let $(E_n)$ be a sequence of events
\begin{align*}
\{E_n \; i.o.\} &=\bigcap_{N=1}^{\infty}\bigcup_{n \geq N}E_n \quad \text{infinitely many of $E_n$ occur} \\
\{E_n\; i.o.\}^{c} &= \bigcup_{N=1}^{\infty}\bigcap_{n \geq N}E_n^c \quad \text{finitely many events occur}\end{align*}
\end{notation}
\begin{note} i.o. - infinitely often \end{note}

\begin{theorem}[Strong Law of Large Numbers]
Let $(X_n)$ be independent identically distributed random variables such that $E|X_1|<\infty$, then
\[\frac{X_1 +\dots + X_n}{n} \rightarrow \underset{a.s.}{EX_1} = EX_n \quad \forall n \]
Outline of proof:\\
$S_n = X_1 + \dots + X_n$ denote $\mu = EX_1$ we need to prove:
\[P\left(\omega \in \Omega: \frac{S_n}{n} \rightarrow \mu\right) = 1\]
\begin{align*}
P\left(\omega \in \Omega: \frac{S_n}{n} \rightarrow \mu\right) &= P\left( \forall k>0 \; \; \exists N \in \NN \; \; \forall n>N \; \; \left| \frac{S_n}{n} - \mu \right|<\frac{1}{k}\right) \\ 
&= P\left(\bigcap_{k=1}^{\infty}\bigcup_{N=1}^{\infty}\bigcap_{n \geq N}\left\{ \left|\frac{S_n}{n} - \mu \right|<\frac{1}{k} \right\} \right)\end{align*}
We need to show $\forall k \in \NN$
\[P\left(\bigcup_{N=1}^{\infty}\bigcap_{n \geq N}\left\{ \left|\frac{S_n}{n} - \mu \right|<\frac{1}{k} \right\} \right) = 1\]
that is equivalent to 
\[P\left(\bigcup_{N=1}^{\infty}\bigcap_{n \geq N}\left\{ \left|\frac{S_n}{n} - \mu \right|\geq \frac{1}{k} \right\}^c \right) = 1\]
ie
\begin{align*}
&P\left(\left\{ \left|\frac{S_n}{n} - \mu \right|\geq \frac{1}{k}\; \; i.o. \right\}^c \;\right) = 1\\
&P\left(\left\{ \left|\frac{S_n}{n} - \mu \right| \geq \frac{1}{k} \; \; i.o. \right\} \right) = 0
\end{align*}
Proven later. \end{theorem}

\subsection{The Borel-Cantelli Lemmas}
\begin{lemma}[Borel-Cantelli Lemma 1 - BC1]\label{bc1}
Let $(E_n)$ be events such that $\sum\limits_{i=1}^{\infty}P(E_n) < \infty$, then 
\[P(E_n \; i.o.) = 0\]
\begin{proof}
\[P(\bigcup_{n \geq N}E_n) \leq \sum_{n \geq N}P(E_n) \quad \forall n \in \NN\]
\[P(E_n \; i.o) =P(\bigcap_{N=1}^{\infty}\bigcup_{n \geq N}E_n) = \lim_{n\rightarrow \infty}P(\bigcup_{n \geq N}E_n) \leq \lim_{n\rightarrow \infty} \sum_{n \geq N}P(E_n) = 0\]
since $\sum\limits_{i=1}^{\infty}P(E_n) < \infty$.
\end{proof}
\end{lemma}

\begin{lemma}[Borel-Cantelli Lemma 2 - BC2]\label{bc2}
Let $(E_n)$ be events independent events such that $\sum\limits_{i=1}^{\infty}P(E_n) =\infty$, then 
\[P(E_n \; i.o.) = 1\]
\begin{proof}
\[P(E_n \; i.o.) = 1 \Rightarrow P(\{E_n \; i.o.\}^c) = P( \bigcup_{N=1}^{\infty}\bigcap_{n \geq N}E_n^c)=0\]
i.e\[P(\bigcap_{n \geq N}E_n^c) =0 \quad \forall n \in \NN\]
\begin{align*}
P(\bigcap_{n \geq N}E_n^c) &= \lim_{k\rightarrow \infty}P(\bigcap\limits_{n =N}^{k}E_n^c) = \lim_{k\rightarrow \infty}\prod_{n =N}^{k}\underbrace{P(E_n^c)}_{1-P(E_n)}\\
&\leq  \lim_{k\rightarrow \infty}\prod_{n =N}^{k}e^{-P(E_n)}= \lim_{k\rightarrow \infty}e^{-\sum_{n=N}^{k}P(E_n)} =0 \; \forall N \end{align*}
\end{proof}
\end{lemma}

\begin{remark}
$E_n = E$ with some $E$ such that $0<P(E)<1, \; \{E_n \;i.o.\} = E$ so $P(E_n \; i.o.) = p$ not $0,1$. BC2, \ref{bc2}, doesn't hold for dependent events. \end{remark}

To summarize if $(E_n)$ are independent then
\[P(E_n \; i.o.) = \begin{cases} 1 &if \;  \sum\limits_{i=1}^{\infty}P(E_n) =\infty\\
0 & if \; \sum\limits_{i=1}^{\infty}P(E_n) < \infty \end{cases} \]
\begin{ap}
$X_n$ iid assume all $X_n$ have exponential distribution. we wish to show:
\[\lim_{n \rightarrow \infty}\frac{X_n}{log(n)} = 1\]
$\lim\sup a_n =a$ means $a$ is the largest value such that there is a subsequence $a_{n_k}$ converging to $a$
\begin{example}\quad\\
\begin{enumerate}
\item $1,\;-1,\;1,\;-1,\dots$
\[\lim\sup = 1\]
\item $1,\;0,\;3,\;0, \; 5, \dots$
\[\lim\sup = \infty\]
\end{enumerate}

\end{example}

So if I want to prove $\lim\sup a_n=a$:
\begin{enumerate}
\item $\forall b>a,\quad a_n>b$ occurs for finitely many $n$
\item $\forall b<a,\quad a_n>b$ occurs for infinitely many $n$
\end{enumerate}
So for 
\[\lim_{n \rightarrow \infty}\frac{X_n}{log(n)} = 1\]
\begin{enumerate}
\item $b>1$
\[P\{\frac{X_n}{log(n)} >b \;\;i.o\} =0\]
\[\sum_{i=1}^{\infty}P(\frac{X_n}{log(n)}>b) = \sum_{i=1}^{\infty}P(X_n>b\,log(n))= \sum_{i=1}^{\infty}e^{-b\,log(n)} = \sum_{i=1}^{\infty}\frac{1}{n^b}<\infty\]
\item $b<1$
\[P\{\frac{X_n}{log(n)} <b \; \;i.o\} =1\]
\[\sum_{i=1}^{\infty}P(\frac{X_n}{log(n)}<b)= \sum_{i=1}^{\infty}\frac{1}{n^b}=\infty\]
\end{enumerate}
\end{ap}

\begin{example}
For the following two iid sequences $(X_n)$:\\
If $(X_n)$ is Exponentially distributed such that $F(x) = 1 - e^{-x}, \; x>0$
\[\lim_{n\rightarrow \infty}\sup \frac{X_n}{log(n)} =1\]
If $(X_n)$ is normally distributed $N(0,1)$
\[\lim_{n\rightarrow \infty}\sup \frac{X_n}{\sqrt{2log(n)}} =1\]
(this is an exersize in homework.)\\
Are the any distributions such that $(X_n)$ grows along a straight line?
\[\lim_{n\rightarrow \infty}\sup\frac{|X_n|}{n} = \alpha ?\]
answer
\[\lim_{n\rightarrow \infty}\sup\frac{|X_n|}{n} = \begin{cases} o & if \; E|X_1| < \infty \text{ $\leftarrow$ follows from SLLN}\\
\infty & if \; E|X_1|=\infty \end{cases}\]
\begin{proof}
\[\frac{X_n}{n} = \underset{EX_1 \quad}{\underset{\quad \downarrow \; \text{\tiny SLLN}}{\frac{X_1 + \dots +X_n}{n}}} - \underset{EX_1 \quad}{\underset{\quad \downarrow \;  \text{\tiny SLLN}}{\frac{X_1 + \dots +X_{n-1}}{n-1}}}\underset{1}{\underset{ \downarrow }{\cdot\frac{n-1}{n}}} \rightarrow 0 \]
We want to show the if $E|X_1|= \infty$
\[P\left(\lim_{n\rightarrow \infty}\sup\frac{|X_n|}{n} = \infty \right)=1\]
i.e.
\[P\left( \forall m \; \frac{X_n}{n} >m  \; i.o.\right) = P\left( \bigcap_{n \i \NN}\frac{X_n}{n} >m  \; i.o.\right) =1\]
i.e 
\[P\left(\frac{X_n}{n} >m  \; i.o.\right) =1 \quad \forall m\]
Use BC2, \ref{bc2}
\[\sum\limits_{n=1}^{\infty}P\left (\frac{|X_1|}{n} > m \right) =  \sum\limits_{n=1}^{\infty}P\left(\frac{|X_1|}{n} > m \right) = \sum\limits_{n=1}^{\infty}E{\mathbb 1}_{\{\frac{|X_1|}{n} > m\}}\]
Idea $m =1 $ 
\[P(|X_1|>1)\]\[P(|X_1|>2)\]\[\vdots \; \; \vdots\]
\[\sum\limits_{n=1}^{\infty}E{\mathbb 1}_{\{\frac{|X_1|}{n} > m\}} \overset{MCT}{=} E\sum\limits_{n=1}^{\infty}{\mathbb 1}_{\{ n <\frac{|X_1|}{m} \}} \geq E\left(\frac{|X_1|}{m} - 1\right)= \infty\]
\end{proof}
\end{example}

\begin{theorem}[Expectation and Variance for independent variables]
Let $X$ and $Y$ be independent random variables
\begin{enumerate}[(a)]
\item if $E|X| < \infty, E|Y| < \infty$ then $E|XY| < \infty$
\[E(XY)=E(X)\cdot E(Y)\]
\item if $E(X^2), E(Y^2) < \infty$ then 
\[Var(X+Y)= Var(X) + Var(Y) \]
\end{enumerate}
\begin{proof}[Proof of (a)]\quad \\
(i) Lets check this for simple r.v's 
\[X= \sum\limits_{i=1}^{n}a_i{\mathbb 1}_{A_i}, \quad Y= \sum\limits_{j=1}^{m}b_j{\mathbb 1}_{B_j}\quad \text{mutually different}\]
\begin{align*}
E(XY) &= E((\sum\limits_{i=1}^{n}a_i{\mathbb 1}_{A_i})(\sum\limits_{j=1}^{m}b_j{\mathbb 1}_{B_j})) = E(\sum\limits_{j=1}^{n}\sum\limits_{j=1}^{m}a_ib_j{\mathbb 1}_{A_i\cap B_j})\\  &= \sum\limits_{j=1}^{n}\sum\limits_{j=1}^{m}a_ib_j\underbrace{P(A_i\cap B_j)}_{= P(A_i)P(B_j)} = (\sum\limits_{i=1}^{n}a_iP(A_i))(\sum\limits_{i=1}^{m}b_jP(B_j)) \\ &= E(X)E(Y)\end{align*}
(ii) $X\geq 0, \; Y \geq 0 \; \leftarrow$ by approximation $X, \; Y$ by simple r.v's and using MCT.\\
(iii) $X, Y$ arbitrary, take
\[X= X^+ - X^- \quad Y=Y^+ - Y^- \]
and use linearity.
\end{proof}
\begin{proof}[Proof of (b)]
\begin{align*}
Var(X+Y) &= E(X+Y)^2 - (E(X+Y)^2  \\ &= E(X^2 +2XY +Y^2) - ((EX)^2 +2EXEY + (EY)^2) \\ &= EX^2 -(EX)^2 + EY^2 - (EY)^2 \\ &= VarX + VarY\end{align*}
\end{proof}
\end{theorem}

\begin{example}
$n \in \NN, \; X_1 , \dots , X_n \; - $ independent. 
\[P(X_i= 1) = p \quad P(X_i=0)= 1-p\]
\[Y=X_1 + \dots + X_n \quad \text{number of heads over $n$-tosses}\]
$Y$ has Binomial distribution
\[P(Y=k) = {n \choose k}p^k(1-p)^{n-k}\]
\[EY= \sum\limits_{k=1}^{n}kP(Y=k) \quad VarY=\sum\limits_{k=1}^{n}k^2P(Y=k) \]
\[\vdots \qquad \vdots\]
\[EY= E(X_1 + \dots + X_n) = EX_1 + \dots + EX_n = np\]
\[VarY = Var(X_1 + \dots + X_n) = VarX_1 + \dots + VarX_n = np(1-p)\]
\end{example}

\subsection{Bernstein’s inequality}
\begin{theorem}[Bernstein’s inequality]\label{bern}
Let $X_1 , \dots , X_n$ be independent and such that 
\[P(X_i=1)=\frac{1}{2}, \quad P(X_i=-1)=\frac{1}{2} \quad \forall 1\leq i \leq n \]
and $a_1, \dots , a_n \; \in \RR$ then
\[P\left(\left|\sum\limits_{i=1}^{n}a_iX_i\right|\right) \leq 2\exp\left(\frac{-t^2}{\sum\limits_{i=1}^{n}a_i^2}\right) \quad \forall t>0\]
\begin{proof}
Denote $c= \sum\limits_{i=1}^{n}a_i^2$ and let $\lambda >0$
\begin{align*} 
E\left(\exp{\left(\lambda\sum\limits_{i=1}^{n}a_iX_i\right)}\right)  &= E\left(\prod\limits_{i=1}^{n}e^{\lambda a_iX_i}\right) = \prod\limits_{i=1}^{n}E\left(e^{\lambda a_iX_i}\right) = \prod\limits_{i=1}^{n}\frac{e^{\lambda a_i} +e^{-\lambda a_i}}{2}\\  &=  \prod\limits_{i=1}^{n}\cosh{\lambda a_i} \leq \prod\limits_{i=1}^{n}\exp{\left(\frac{\lambda^2 a_i^2}{2}\right)} = \exp{\left(\frac{\lambda^2 c}{2}\right)}
\end{align*}
now
\begin{align*}
P\left(\lambda\sum\limits_{i=1}^{n}a_iX_i > \lambda t\right) &= P\left(\exp\left(\lambda\sum\limits_{i=1}^{n}a_iX_i\right) > \exp(\lambda t)\right) \\ &\overset{\mathclap{Chebyshev}}{\leq} E\left(\exp\left(\lambda\sum\limits_{i=1}^{n}a_iX_i\right)\cdot\exp(- \lambda t)\right) \\ &\leq \exp{\left(\frac{\lambda^2 c}{2} - \lambda t\right)}\end{align*}
Take $\lambda$ in such a way that it minimises $\frac{\lambda^2 c}{2} - \lambda t$
\begin{align*}
P\left(\sum\limits_{i=1}^{n}a_iX_i >  t\right) &\leq \exp{\left(\frac{t^2}{c^2}c\frac{1}{2} -\frac{t^2}{c}\right)} = \exp{\frac{-t^2}{2c}}\\
P\left(\sum\limits_{i=1}^{n}\underbrace{a_i}_{\mathclap{b_i = -a_i}}X_i < - t\right) &= P\left(\sum\limits_{i=1}^{n}b_iX_i >  t\right) \leq \exp{\frac{-t^2}{2c}}\\
P\left(\left|\sum\limits_{i=1}^{n}a_iX_i \right| >  t\right) &= P \left(\sum\limits_{i=1}^{n}a_iX_i >  t \cup \sum\limits_{i=1}^{n}a_iX_i <  -t\right) \\ &\leq P \left(\sum\limits_{i=1}^{n}a_iX_i >  t \right) + P\left( \sum\limits_{i=1}^{n}a_iX_i <  -t\right)\\ &\leq 2\exp{\frac{-t^2}{2c}}
\end{align*}
\end{proof}
\end{theorem}

\begin{theorem}[SLLN for r.v's taking values $\pm 1$]
Let $X_1 , \dots , X_n$ be a sequence of iid random variables such that 
\[P(X_i=1)=\frac{1}{2}, \quad P(X_i=-1)=\frac{1}{2} \quad \forall  i  \]
then
\[\frac{X_1 + \cdots + X_n}{n}\rightarrow 0 \;(=EX_1) \; a.s\]
\begin{proof}
we want to prove 
\[P\left(\forall k \; \exists N \; \forall n \geq N \; \left|\frac{X_1 + \cdots + X_n}{n}\right|<\frac{1}{k} \right) =1\]
i.e
\[P\left(\bigcap_{k=1}^{\infty}\bigcup_{N=1}^{\infty}\bigcap_{n \geq N} \left\{ \left|\frac{X_1 + \cdots + X_n}{n}\right|<\frac{1}{k}  \right\} \right) =1\]
i.e 
\[P\left(\bigcup_{N=1}^{\infty}\bigcap_{n \geq N} \left\{ \left|\frac{X_1 + \cdots + X_n}{n}\right|<\frac{1}{k}  \right\} \right) =1 \quad \forall k\]
\[P\left(\bigcap_{N=1}^{\infty}\bigcup_{n \geq N} \left\{ \left|\frac{X_1 + \cdots + X_n}{n}\right| \geq \frac{1}{k}  \right\} \right) =0 \]
this means 
\[P\left( \left|\frac{X_1 + \cdots + X_n}{n}\right| \geq \frac{1}{k} \; i.o.  \right) =0 \quad \forall k\]
Use BC1:
\[\sum\limits_{n=1}^{\infty}\left( \left|\frac{X_1 + \cdots + X_n}{n}\right| \geq \frac{1}{k}  \right) \overset{(*)}{\geq} \sum\limits_{n=1}^{\infty}2\exp{\frac{-n}{2k^2}} < \infty\]
\begin{note}[*] this is an application of bernsteins inequality with $a_1= \cdots= \_n = 1$ and $t=\frac{n}{k}$ since
\[\sum\limits_{n=1}^{\infty}e^{-n} = \sum\limits_{n=1}^{\infty}(\underset{<1}{e^{-1}})^{n} \]
\end{note}
\end{proof}
\end{theorem}

\subsection{Joint Laws}
\begin{recall}\[X \leadsto \mu_X \quad on \; (\RR, \mathcal{B}) \] \end{recall}
Let $X, \; Y$ be two r.v's
\begin{definition}[Joint Law]
The Joint Law of $X$ and $Y$ is a probability measure
\[\mu_{X,Y} \quad on \; (\RR^2, \mathcal{B}(\RR^2)) \]
Defined by 
\[\mu_{X,Y} = P((x,y) \in \mathcal{B})\]
\end{definition}
\begin{definition}[The Joint distribution function]
\[F_{X,Y}(x,y)=P(X\leq x , \; Y \leq y) = \mu_{X,Y}((-\infty,x]\times (- \infty, y])\]
\end{definition}

\begin{theorem}
Let $X$ and $Y$ be independent
\begin{enumerate}[(a)]
\item \[F_{X,Y}(x,y) = F_X(x)F_Y(y)\]
\item if $X$ and $Y$ have densities $f$ and $g$ then $\mu_{X,Y}(x,y)$ has density 
\[f(x)g(y)\]
with respect to lebesgue measure on $\RR^2$
\item  if $X$ and $Y$ have densities $f$ and $g$, then $X +Y$ has density $(f*g)(t)$ where
\[(f*g)(t)=\int_{-\infty}^{\infty}f(x)g(t-x)dx \]
which is called the convolution of $f$ and $g$.
\end{enumerate}
\begin{proof}[Proof of (a)]
\[F_{X,Y}(x,y) = P(X\leq x , \; Y \leq y) = P(X\leq x)P(Y \leq y) = F_X(x)F_Y(y)\]
\end{proof}
\begin{proof}[Proof of (b)]
we want to show 
\[\mu_{X, Y}(B) = f(x)g(y)dxdy \quad \forall B \subset \mathcal{B}(\RR^2)\]
it suffices to look at $B= (-\infty,x]\times (- \infty, y], \; \forall x,y$ since they form a $\pi$-system generating $\mathcal{B}(\RR^2)$.
\begin{align*} \mu_{X,Y}((-\infty,x]\times (- \infty, y]) &= F_{X,Y}(x,y) = F_X(x)F_Y(y) = \int\limits_{-\infty}^{x}f(u)du\int\limits_{-\infty}^{y}g(v)dv \\ &= \int\limits_{-\infty}^{x}\int\limits_{-\infty}^{y}f(u)g(v)dudv\end{align*}
\end{proof}
\begin{proof}[Proof of (c)]
\begin{align*} F_{X+Y}(t) &= P(X+Y \leq t) \overset{(b)}{=}\iint\limits_{U+V \leq t}f(u)g(v)dudv = \int\limits_{-\infty}^{\infty}\int\limits_{-\infty}^{t-x}f(u)g(v)dudv \\ &= \int\limits_{-\infty}^{\infty}f(u)\underbracket[0.5pt][7pt]{\left(\int\limits_{-\infty}^{t-x}g(v)dv\right)}_{v=z-u}du =  \int\limits_{-\infty}^{\infty}f(u)\int\limits_{-\infty}^{t}g(z-u)dzdu \\ &=\int\limits_{-\infty}^{t}\left(\int\limits_{-\infty}^{\infty}f(u)g(z-u)du\right)dz \end{align*}
\[ F_{X+Y}(t) = \int\limits_{-\infty}^{\infty}f(u)g(t-u)du\]
\end{proof}
\end{theorem}

\begin{example}
Density of $X\cdot Y$?
\[ F_{X\cdot Y}(t) = \int\limits_{uv\leq t}f(u)g(v)dudv\]
\end{example}

\subsection{Tail events and Kolmogorov $0-1$ law}
$(X_n)$ - r.v's
\begin{alignat*}{2}
&\{\lim X_n >0\} &\quad &\leftarrow \; \overset{\text{\small tail events}}{\boxed{\text{doesn't depend on any finite no. r.v's}}}\\
&\{\sup X_n >0\} &\quad &\leftarrow \; \text{is not like that}
\end{alignat*}

\begin{definition}Let $X_n$ be a sequence of r.v.'s 
\[\mathcal{T}_n =\sigma(X_{n+1}>X_{n+2}> \cdots ) \quad \text{$n^{th}$ tail $\sigma$-algebra} \]
this is the information contained in $X_{n+1}, X_{n+2}, \dots$ 
\[\mathcal{T} = \bigcap_{n=1}^{\infty}\mathcal{T}_n  \quad \text{the tail $\sigma$-algebra} \]
each $A_i \in \mathcal{T}$ is called a Tail event.
\end{definition}

\begin{example}\label{01}\quad \\
\begin{enumerate}
\item \[\{X_n \rightarrow a\} \; - \; \text{tail event}\]
since
\[\{X_n \rightarrow a\} = \{X_1, X_2, X_3,\dots \rightarrow a\} = \{X_{m+1}, X_{m+2}, \dots \rightarrow a\} \in \mathcal{T}_m\]
\[\Rightarrow \{X_n \rightarrow a\} \in \cap \mathcal{T}_m = \mathcal{T}\]
\item
\begin{alignat*}{2}
&\{\lim_{n\rightarrow \infty}X_n \; exists\} &\; &- \; \text{tail event}\\
&\left\{\sum\limits_{n=1}^{\infty}X_n < \infty\right\} &\; &-  \; \text{tail event}
\end{alignat*}
\item \[\{\sum\limits_{n=1}^{\infty}X_n < 10\} \; -  \; \text{not a tail event} \]
\item \[\left\{\frac{X_1 + \cdots +X_n }{n} \; converges\right\} \; -  \; \text{tail event}\]
since
\[\left\{\frac{X_1 + \cdots +X_n }{n} \; converges\right\} = \left\{\frac{X_1 }{1}, \; \frac{X_1 + X_2 }{2}, \;\frac{X_1+ X_2 +X_3 }{3}, \dots \; converges\right\}\]
\[\frac{X_1 + \cdots +X_n }{n} = \underbracket{\frac{X_1 + \cdots +X_m }{n}}_{\rightarrow \; 0} + \frac{X_{m+1} + \cdots +X_n }{n}\]
\[ \forall m \; \left\{\frac{X_{m+1} + \cdots +X_n }{n} \; converges\right\} \in \mathcal{T}_n \Rightarrow \text{Tail event}\]
\item \[\{\sum\limits_{n=1}^{\infty}X_n >0\} \; -  \; \text{not a tail event} \]
Consider 
\[X_1 = \left\{1,-1 \; \text{each with }P=\frac{1}{2}\right\} \quad X_i = 0, \; \forall i \geq 2\]
we want to show 
\[A \notin \mathcal{T} = \bigcap_{n=1}^{\infty}\mathcal{T}_n, \quad \mathcal{T}_n = \sigma(X_{n+1}, X_{n+2}, dots )\]
\[\begin{array}{ccc}
\sigma(X_2) &= &\{\phi , \Omega\}\\
\sigma(X_3) &= &\{\phi , \Omega\}\\
\; \vdots & & \; \vdots\\
\sigma(X_n) &= &\{\phi , \Omega\}
\end{array}\]
\[\Rightarrow \mathcal{T}_n = \{\phi , \Omega\} \Rightarrow  \mathcal{T}= \{\phi , \Omega\} \]
\[P(A)= \frac{1}{2} \Rightarrow A\neq \phi, \; A \neq \Omega \]
\end{enumerate}
\end{example}

\begin{theorem}[Kolmogorov $0-1$ law]\label{kol01}
If $(X_n)$ is a sequence of independent random variables, then each tail event has probability $0$ or $1$.
\begin{proof}
\[\sigma_n=\sigma(X_1, \dots , X_n)\]
\[\mathcal{T}_n = \sigma(X_{n+1},X_{n+2},  \dots )\]
since $(X_n)$ are independent 
\[\sigma_n \Perp  \mathcal{T}_n, \quad \forall n\]
\[\mathcal{T} = \bigcap_{i=1}^{\infty}\mathcal{T}_n \Rightarrow \mathcal{T} \subset \mathcal{T}_n \Rightarrow \sigma_n \Perp  \mathcal{T}, \quad \forall n\]
Denote 
\[\sigma_{\infty}(X_1, X_2 \dots ) \Rightarrow \; each\; \mathcal{T}_n \subset \sigma_{\infty} \Rightarrow \mathcal{T} \subset \sigma_{\infty}\]
On the other hand 
\[\sigma_{\infty} \Perp  \mathcal{T}\]
since $\sigma_{\infty}$ is generated by the $\pi$-system $\bigcup_{i=1}^{n}\sigma_i$ which is independent of $\mathcal{T}$.
\[\Rightarrow \mathcal{T} \Perp \mathcal{T}\]
\[\forall A \in \mathcal{T} \quad P(A)=P(\underset{\in \mathcal{T}}{A}\cap \underset{\in \mathcal{T}}{A})= P(A)P(A) = P(A)^2\]
\[\Rightarrow P(A) = 0 \quad or \quad P(A)=1\]
\end{proof}
\end{theorem}

\begin{example}\quad\\
\begin{enumerate}
\item for all tail events in example \ref{01} above \[P(\dots) = 0\;or \;1\quad \text{(if $(X_n)$ are independent)}\]
\item $\frac{|X_n|}{n}$ if $E|X_1| < \infty$ then by SLLN \[\frac{|X_n|}{n} \rightarrow 0\]
if  $E|X_1| = \infty$
\[\frac{|X_n|}{n} \; diverges.\]
\[P\left(\left\{\frac{|X_n|}{n} \quad converges.\right\}\right) = \begin{cases} 1 &if \; E|X_1| < \infty\\ 0 &if \; E|X_1| = \infty\end{cases}\]
\item Percolation. Have a lattice and flip a coin with probability p, if heads keep edge, if tails remove edge.
\[\{\text{there is no infinite cluster}\}  \; -  \; \text{tail event} \]
$X_n =$ edge n edges away from start point. Infinite cluster exists with either $P=1 \; or \; 0$ depending on value of $p$. 
\end{enumerate}
\end{example}

\section{Weak convergence}
$X,Y$ independent bernulli random variables 
\[X \neq Y\quad a.s.\]
but
\[X\overset{in}{\underset{Law}{=}} Y\quad since \quad \mu_X = \mu_Y\]
\[\frac{X_1+ \cdots X_n}{n} \overset{SLLN}{\rightarrow} \mu \quad a.s.\]
\[\frac{X_1+ \cdots X_n - n\mu}{n\sigma}\overset{in}{\underset{Law}{=}} N(1,0) \quad (CLT) \] 
where here $\sigma$ means varience of $(X_i)$

\begin{definition}
we say that $X_n \rightarrow X$ in law, in distribution or weakly if 
\[F_{X_n}(t) \Rightarrow F_X(t) \quad \forall t \in \RR\]
where $F_X$ is continuous. 
\[\Leftrightarrow \mu_{X_n} \rightarrow \mu_X \quad weakly\]
\end{definition} 

\begin{notation} to say that $X_n \rightarrow X$ in law, in distribution or weakly:
 \[\underbrace{\overset{d}{\rightarrow}}_{\text{\small we will use this}} \qquad \underbrace{\overset{w}{\rightarrow}, \Rightarrow}_{\text{\small some people use these}}\]
\end{notation}

\begin{remark}
why do we exclude discontinuity points?
\[X_n = \frac{1}{n} \; with \; P=1\]
Need to check someone elses notes cant read mine!
\end{remark}

\begin{theorem}[Relation between a.s. and weak convergence]\label{weak1}\quad \\
\begin{enumerate}
\item if $X_n\rightarrow X$ a.s. then $X_n\overset{d}{\rightarrow} X$
\item if $\mu_X \rightarrow \mu$ weakly then there are random variables $(X_n)$ and $X$ such that
\begin{align*} &X_n\text{ has law }\mu_n \quad \forall n\\ &X\text{ has law }\mu \end{align*}
and $X_n \rightarrow X$ a.s.
\end{enumerate}
\end{theorem}
\begin{theorem}\label{weak2}[Useful definition of weak convergence]
\[\mu_n \rightarrow \mu \;weakly \; \Rightarrow \int\limits_{\RR}hd\mu_n \rightarrow \int\limits_{\RR}hd\mu \]
\[\forall h:\RR \rightarrow \RR \qquad \text{continuous and bounded}\]
\end{theorem}

\begin{idea}
\begin{alignat*}{2}
X_n \overset{d}{\rightarrow} X \; \Rightarrow \; \; &EX_n \rightarrow EX & \quad &\int Xd\mu_n \rightarrow \int Xd\mu \\
&EX_n^2 \rightarrow EX^2 & \quad &\int X^2 d\mu_n \rightarrow \int X^2 d\mu \end{alignat*}
$\Leftarrow$ mabie we can get this if we check for a sufficently large class of test functions.
\[\boxed{\int\limits_{\RR}hd\mu_n \rightarrow \int\limits_{\RR}hd\mu} \quad \rightarrow \quad  Eh(X_n) \rightarrow Eh(X)\]
\end{idea}

\begin{proof}[Proof of theorems \ref{weak1} and \ref{weak2}]
Plan: $(2), \; \Rightarrow, \; \Leftarrow,\; (1)$
\begin{proof}[Proof of $(2)$]
we are given $\mu_n \rightarrow \mu$ weakly i.e.
\[F_n(t) \rightarrow F(t) \quad \forall t, \; \text{where $F$ is continuous}\]
we use Skorokhod representation, \ref{sko}, to construct all $X_n$ and $X$.
 \begin{alignat*}{3}
([0,1],\mathcal{B}, Leb), \quad &X(\omega)& &= \inf\{u:F(u)>\omega\}& &\leftarrow \text{ has law }\mu \\
&X_n(\omega)& &=\inf\{u:F_n(u)>\omega\} &  &\leftarrow \text{ has law }\mu_n \\
\end{alignat*}
\[B= \{\omega \in [0,1] : \exists x,y \in \RR\; s.t. \; F(x)=F(y)\}\]
we need to
\begin{enumerate}[(a)]
\item prove that $B$ is at most countable and $Leb(B) = 0$
\item prove that $X_n(\omega) \rightarrow X(\omega) \quad \forall \omega \in [0,1]\backslash B$
\end{enumerate}
proof of (a): each $\omega \in B$ generate an interval $(x,y)$, all these intevals don't intersect. Each interval contains a rational number. \begin{align*}
&\Rightarrow\; \text{at most countably many intervals}\\
&\Rightarrow\; \text{at most countably many }\omega
\end{align*}
proof of (b): lets prove that the set of discontinuity points of $F$ is at most countable, (same arguement as for (a) but considering intervals fromed on the $y$ axis). Now let $\omega \in [0,1]\backslash B, \; \epsilon >0$. Choose $0<\delta<\epsilon$ so that $X(\omega) \pm \delta$ are continuity points of $F$. 
\[F(X(\omega) - \delta) < \omega < F(X(\omega) + \delta) \]
\[\left.\begin{aligned}
       F_n(X(\omega) - \delta) &\rightarrow F(X(\omega) - \delta)\\
        F_n(X(\omega) + \delta) &\rightarrow F(X(\omega) + \delta)
       \end{aligned}
 \right\} \; \underset{\text{\normalsize and weak convergence}}{\text{Since  $X(\omega) \pm \delta$  are are continuity points of $F$}}\]
\[\Rightarrow \exists N\; \forall n\geq N \quad F_n(X(\omega) - \delta) < \omega < F_n(X(\omega) + \delta) \]
\[\Rightarrow X(\omega) - \delta < X_n(\omega) < X(\omega) + \delta\]



\end{proof}
\begin{proof}[Proof of $(\Rightarrow)$]
we are given 
\end{proof}
\begin{proof}[Proof of $(\Leftarrow)$]
we are given 
\end{proof}
\begin{proof}[Proof of $(1)$]
we are given 
\end{proof}
\end{proof}

\end{document}
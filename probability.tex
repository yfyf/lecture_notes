\documentclass[12pt]{article}
\usepackage{amsfonts, amsthm, amsmath}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{pb-diagram}
\usepackage{hyperref}
\usepackage{cancel}


\setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{0in}
\setlength{\textheight}{9.5in}
\setlength{\topmargin}{0in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\parskip}{0pt}
\setlength{\parindent}{0pt}

\def\CC{\mathbb{C}}
\def\MM{\mathbb{M}}
\def\FF{\mathbb{F}}
\def\PP{\mathbb{P}}
\def\QQ{\mathbb{Q}}
\def\RR{\mathbb{R}}
\def\ZZ{\mathbb{Z}}
\def\NN{\mathbb{N}}
\def\gotha{\mathfrak{a}}
\def\gothb{\mathfrak{b}}
\def\gothm{\mathfrak{m}}
\def\gotho{\mathfrak{o}}
\def\gothp{\mathfrak{p}}
\def\gothq{\mathfrak{q}}
\DeclareMathOperator{\disc}{Disc}
\DeclareMathOperator{\Gal}{Gal}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\Norm}{Norm}
\DeclareMathOperator{\Trace}{Trace}
\DeclareMathOperator{\Cl}{Cl}

\def\head#1{\medskip \noindent \textbf{#1}.}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem*{note}{Note}
\newtheorem*{remark}{Remark}
\newtheorem*{claim}{Claim}
\newtheorem*{idea}{Idea}
\newtheorem*{notation}{Notation}
\newtheorem*{ap}{Applications}
\begin{document}
\title{Probability \\ 3105}
\author{Based on lectures by\\Dr N Sidorova}
\date{Jan 2012}
\maketitle

\tableofcontents
\setcounter{tocdepth}{4}
\newpage

\section{Rigorous set up}

\subsection{Probability space, events and random variables}
\begin{definition}[$\sigma$-algebra of sets]
Let $\Omega$ be a set and $\Sigma$ be a collection of sets. Then $\Sigma$ is a $\sigma$ - algebra if
\begin{enumerate}
\item $\Omega, \; \phi \in \Sigma$
\item $A \in \Sigma$ then $\Omega/A \in \Sigma$
\item $A_1, A_2, \dots \in \Sigma$ then $\cup_{1}^{\infty}A_i \in \Sigma$
\end{enumerate}
\end{definition}

\begin{definition}[Measure]
$\mu:\Sigma \rightarrow [0,\infty]$ is called a measure if
\begin{enumerate}
\item $\mu(\phi) = 0$
\item $A_1, A_2, \dots$ are disjoint then $\mu(\cup_{1}^{\infty}A_i) = \sum_{1}^{\infty}\mu(A_i)$
\end{enumerate}
\end{definition}

\begin{definition}[Probability Measure] 
A measure $\mu$ is called a Probability Measure denoted by $P$ if \[P(\Omega) =1\]
\end{definition}

\begin{definition}[Probability Space]
A triple $(\Omega, \Sigma, P)$, where $\Omega$ is a set, $\Sigma$ is a $\sigma$-algebra and $P$ is a probability measure, is called a Probability Space.  
\end{definition}

\begin{definition}[Measurable Function]
a function $X$ is called a Measurable function if 
\[\forall B \in \mathcal{B} \quad X^{-1}(B) = \{w\;:\; X(w) \in \mathcal{B} \} \in \Sigma \]
\end{definition}

\begin{definition}[Random Variable] 
A random variable is called a measurable function
\begin{align*}
X:\Omega &\rightarrow \RR\\
(\Omega, \Sigma) &\rightarrow (\RR , \underbrace{\mathcal{B}}_{\mathclap{borel \; \sigma -algebra}})
\end{align*}
\end{definition}

The idea:\\
$\Omega$ - Random outcomes\\
$\Sigma$ - All possible events\\
$P(E)$ - Probability of the event $E$

\begin{example}
Bernulli = "tossing a coin" = "$0$ or $1$ with probability $\frac{1}{2}$
\begin{align*}
&\Omega = \{H,T\}\\
&\Sigma = \{ \{H\},\{T\}, \{H,T\}, \phi \} = 2^{\Omega}\\
&P(\{H\}) = P(\{T\}) = \frac{1}{2}\\
&P(\{H,T\}) = 1\\
&P(\phi) = 0
\end{align*}
\begin{align*}
X:&H\rightarrow 1\\
&T\rightarrow 0
\end{align*}
\[\text{"Probability that $X=1$"} = P(\omega :X(\omega)=1) = P(\{H\}) = \frac{1}{2}\]
\end{example}

\begin{example}
Roll a die, spell the number, take $\#$ of letters
\begin{align*}
&\Omega = \{1,2,3,4,5,6\}\\
&\Sigma = 2^{\Omega} \quad \text{($\sigma$-algebra of all subsets)}\\
&P(\{1\})= \dots =P(\{6\}) = \frac{1}{6}\\
&P(\{1,3,5\}) = \frac{1}{6}+\frac{1}{6}+\frac{1}{6}= \frac{1}{2} \; etc \dots
\end{align*}
\begin{align*}
X:\quad &1\rightarrow 3\\
&2\rightarrow 3\\
&3\rightarrow 5\\
&4\rightarrow 4\\
&5\rightarrow 4\\
&6\rightarrow 3
\end{align*}
\end{example}

\begin{example}
Roll a die, spell the number, take $\#$ of letters but with the possibility of the dice rolling off the table and scoring $0$
\begin{align*}
&\Omega = \{1,2,3,4,5,6,0\}\\
&\Sigma = 2^{\Omega} \quad \text{($\sigma$-algebra of all subsets)}\\
&P(\{1\})= \dots =P(\{6\}) = \frac{1}{6} \quad 
P(\{0\}) = 0
\end{align*}
\begin{align*}
X:\quad  &0\rightarrow 4\\
&1\rightarrow 3\\
&2\rightarrow 3\\
&3\rightarrow 5\\
&4\rightarrow 4\\
&5\rightarrow 4\\
&6\rightarrow 3
\end{align*}
\end{example}

\begin{example}
Tossing a fair coin infinitely many times
\begin{align*}
&\Omega = [0,1]\\
&\Sigma = ?
\end{align*}
we can then represnt each event as a real number in $[0,1]$ for example all events where first three results are $HTH$ rest unknown:
\[\{\omega = 0.101***\} = \left[\frac{5}{8} , \frac{3}{4}\right] \]
All binary intervals must be in $\Sigma$. The minimal $\sigma$-algebra with this property is $\mathcal{B}$ the borel $\sigma$-algebra. 
\[P(\omega : 0.101*****\dots ) = leb\left[\frac{5}{8}, \frac{3}{4}\right] = \frac{1}{8}\]
\[\Rightarrow \text{P is a lebesque measure}\]
some questions that could be asked:\\
if $\omega = \omega_1, \omega_2 , \dots$
\begin{enumerate}[(a)]
\item what is the number in the $n^{th}$ position\\
$[0,1] \rightarrow \RR$\\
$0.\omega_1 \omega_2 \dots \mapsto  \omega_n$
\item how many $1$'s out of the first $n$ tosses?\\
$[0,1] \rightarrow \RR$\\
$0.\omega_1 \omega_2 \dots \mapsto \omega_1 + \dots + \omega_n$
\end{enumerate}
\end{example}

\begin{definition}
An event is an element of $\Sigma$
\end{definition}
Suppose an event $E$ occurs = suppose $\omega \in E$. An event occurs with probability $p= P(E)$. We are interested in: $P(X \in B) \equiv P(\omega \;:\;X(\omega) \subseteq B)$
\begin{definition}
\[\mu_X(B) := P(X\in B), \quad B \in \mathcal{B}\]
This probability measure on $(\RR , \mathcal{B})$ is called the distribution of $X$ or the law of $X$.\\
\end{definition}
$\mathcal{B}$ is generated by $\{(-\infty, t], \; t \in \RR\}$

\subsection{Distribution function}
\begin{definition}
\[F_X(t):= \mu_X((-\infty, t]) = P(x \leq t)\quad t\in \RR\]
$F_X(t)$ is called the distribution function of $X$
\end{definition}

\begin{example}
Bernoullli:
\[\mu_X(B) := P(X\in B) = \begin{cases}
   1 & \text{if }0,1\in B \\
   \frac{1}{2}      & \text{if just one of } 0 ,1 \in B\\
   0    & \text{otherwise}
  \end{cases}
\]
\end{example}

\begin{definition}\label{propFX}
Properties of $F_X$:
\begin{enumerate}
\item $F_X$ is increasing
\item $F_X \rightarrow 1$ as $t \rightarrow \infty$ and $F_X \rightarrow 0$ as $t \rightarrow -\infty$
\item $F_X$ is right continuous
\end{enumerate}
\begin{proof}\quad \\
(1):\quad\\
 $t_1<t_2$
\begin{align*}
F_X(t_1) &=P(x \leq t_1)\\
F_X(t_2) &=P(x \leq t_2).
\end{align*}
\[t_1<t_2 \Rightarrow \{X \leq t_1\} \subset \{X\leq t_2\}\]
\[ \therefore P(x \leq t_1)\leq P(x \leq t_2)\]
(2):\quad\\
 Let $t_n \rightarrow \infty$ (need countability with $\sigma$-algebras).
\[F_X (t_n) = P(x \leq t_n) \rightarrow P\left(\cup_{n \in \mathbb{N}}\{X\leq t_n\}\right) = P(\Omega) = 1\]
Let $t_n \searrow \infty$
\[F_X (t_n) = P(x \leq t_n) \rightarrow P\left(\cap_{n \in \mathbb{N}}\{X\leq t_n\}\right) = P(\phi) = 0\]
(3):\quad\\
Let $t_n \searrow t$
\[F_X (t_n) = P(x \leq t_n) \rightarrow P\left(\cap_{n \in \mathbb{N}}\{X\leq t_n\}\right) = P(x \leq t) = F_X(t)\]
\end{proof}
\end{definition}

\begin{theorem}[Skorokhod Representation]\label{sko}
If $F:\RR \rightarrow [0,1]$ satisfies (1)-(3) from def \ref{PropFX} above then there is a randon variable $X$ on the probability space $([0,1], \mathcal{B}, leb)$ such that
\[F_X = F\]
\begin{idea}
If $F$ is invertable, then take $G=F^{-1}$ and $X(\omega) = G(\omega)$
\[F_X(t) = leb(\omega:X(\omega) \leq t)=F(t)\]
\end{idea}
\begin{proof}
Define $G:[0,1] \rightarrow \RR$
\[G(\omega)= \inf\{t:F(t)> \omega\}\]
Define $X(\omega)=G(\omega)$\\
need to prove: 
\[F_X(u) =leb\{\omega :  G(\omega) \leq u\} \overset{?}{=} F(u)\]
\[i.e \;F_X(u) =leb\{\omega : \inf\{t :F(t)>\omega\}  \leq u\} \overset{?}{=} F(u)\]
It suffices to show:
\[ [0,F(u)) \subset \{\omega :\inf\{t :F(t)>\omega\}  \leq u\} \subset [0,F(u)] \]
\begin{enumerate}[(a)]
\item Let $\omega \in [0,F(u))$ \begin{align*} &\Rightarrow \omega <F(u)\\
&\Rightarrow u \in \{ t :  F(t)> \omega \}\\
&\Rightarrow \inf\{t :  F(t)> \omega \} \leq u \\
&\Rightarrow \omega \in \;\text{"middle set"}\end{align*}
\item Let $\omega$ be such that $\inf\{t :  F(t)> \omega \} \leq u$\\
monotonicity of $F$: \[F(\inf\{t :  F(t)> \omega \})\leq F(u)\]
right continuity:  \[\inf\{F(t) :  F(t)> \omega \} < F(u)\]
\[\omega \leq \inf\{F(t) :  F(t)> \omega \} < F(u)\]
\end{enumerate}
\end{proof}
\end{theorem}

\begin{example}
Uniform Distribution
\[F(t) = \begin{cases}
0 &if \; t<0\\
t &if \; t\in [0,1]\\
1 &if \; t > 1 \end{cases} \]
$X \; ; \; ([0,1], \mathcal{b}, leb)$
\[X(\omega) = \omega \quad \text{(jumps to $\infty, \; -\infty$ outside of $[0,1]$)}\]
$0.\omega_1\omega_2 \omega_3 \dots$ - uniform  random variable on $[0,1]$
\end{example}

\begin{example}
Exponential random variable (with mean $\mu$). 
\[F(t) = \begin{cases}
1-e^{ - t/\mu} &if \;  t\geq 0\\
0 & otherwise \end{cases} \]
\end{example}

\begin{example} Normal, $N(\underset{mean}{\mu}, \underset{variance}{\sigma^2})$
\[F(t) = \frac{1}{\sigma \sqrt{2\pi}}\int_{-\infty}^{t}\exp(- \frac{(u-\mu)^2}{2 \sigma^2})du\]
\end{example}

\begin{example}
Poisson Distribution
\[P(X=k) = e^{-\lambda}\frac{\lambda^k}{k!}, \quad k=0,1,2,\dots\]
$([0,1], \mathcal{B}, leb)$ \begin{alignat*}{2}
&\Omega = \{0,1,2,\dots \} &\quad &\quad\\
&\Sigma = 2^{\Omega} &\quad &\quad\\
&P(\{k\}) = e^{-\lambda}\frac{\lambda^k}{k!} &\quad & \leftarrow \; \text{probability measure complicated}\\
&X: 0 \rightarrow 0 &\quad &\quad\\
& \; \; \: \, : 1 \rightarrow 1 &\quad &\quad\\
&\; \; \: \, : 2 \rightarrow 2 &\quad & \leftarrow \; \text{Simple}\\
& \; \; \: \, :3 \rightarrow 3 &\quad &\quad\\
\end{alignat*}
By using Skorohod Representation, thm \ref{sko}, we keep the probability measure simple and random variable function complicated. 
\end{example}

\begin{definition} If one can write $F_x = \int_{-\infty}^{t}f_X(u)du$ then the law/distribution is called continuous and $f_X$ is called the density.
\[\mu_X((-\infty, t])= F_X(t)du =  \int_{-\infty}^{t}f_X(u)du =  \int_{(-\infty, t]}f_X(u)d\, leb(u)\]
\end{definition}

\begin{remark} \quad \\
\begin{enumerate}[(1)]

\item \begin{alignat*}{2} X \text{ has a densty}  &\Leftrightarrow & &\text{ the law is continuous}\\
&\Leftrightarrow & & \; X \text{ is absolutely continuous w.r.t $leb$ and:} \\ & & & \; f_X = \frac{d\mu_x}{d\, leb}\; \; \text{     (Radon - Nikodym Density)} \end{alignat*}
\item if $F$ is differentiable then \[f_x = F'_x\]
\item Exponential: \begin{align*} F(t) &= \begin{cases}
1-e^{ - t/\mu} &if \;  t\geq 0\\
0 & otherwise \end{cases} \\
f_x(t) &= \begin{cases}
\frac{1}{\mu}e^{- t/\mu} &if \;  t\geq 0\\
0 & otherwise \end{cases} \end{align*} 
Normal:
\[f_X(t) = \frac{1}{\sigma \sqrt{2\pi}}\exp(- \frac{(t-\mu)^2}{2 \sigma^2})\]
\item \[\int_{-\infty}^{\infty}f_X(u)du = 1\]
\[ \lim_{x \rightarrow \infty}\int_{-\infty}^{x}f_X(t)dt = \lim_{x \rightarrow \infty}F(X)\]
\end{enumerate}
\end{remark}

\subsection{Expectation and varience}

\begin{remark}[Reminder from measure theory]\quad \\
\begin{enumerate}  
\item \begin{align*} f &= \sum_{n}^{i=1}a_i{\mathbf 1}_{B_i} \quad \text{(simple functions)}\\
\int f dP &= \sum_{n}^{i=1}a_iP(B_i) \end{align*}
\item $f \geq 0 $, take simple functions $f_n$ where $f_n \nearrow f$. Define
\[\int f dP = \lim_{n \rightarrow \infty}\int f_n dP \quad \in [0,1]\]
\item fpr arbitrary $f$
\[ f = f^{+}+ ( - f^{-}) \]
Define
\[\int f dP = \int f^{+} dP  - \int f^{-} dP\]
if both are finite.We say the function is non-lebesque measurable otherwise.
\end{enumerate}
\textbf{MCT} suppose $f_n: \Omega \rightarrow [0,\infty]$ and $f_n \nearrow f$ a.s. Then:
\[\int f_n dP \rightarrow \int f dP\]
\textbf{DCT} suppose $f_n \rightarrow f$ a.s. and $|f_n(\omega)| < g(\omega)$ a.s. where $\int g(\omega) < \infty$ Then:
\[\int f_n dP \rightarrow \int f dP\]
\end{remark}

\begin{definition}
Let $X$ be a random variable on $(\Omega , \Sigma , P)$. If $X$ is integrable then \[EX = \int X dP \]
this is called the expectation of $X$. If $X >0$, we allow the case $EX = \infty$
\end{definition}
\begin{definition}
 If $X$ is square integrable ($X^2$ is integrable), then \[VarX =E (X - EX)^2 \]
this is called the variance of $X$. 
\[E (X - EX)^2 = E (X^2 -2X\cdot EX + (EX)^2) = EX^2 -2( EX)^2 + (EX)^2 = EX^2 -( EX)^2\]
\end{definition}

\begin{lemma}If $EX^2< \infty$ then $E|X|<\infty$ and so $EX < \infty$
\begin{proof}
\[E|X| = E|X|\cdot 1 \leq \underbrace{\sqrt{EX^2}}_{finite} \cdot \underbrace{\sqrt{E1^2}}_{1} < \infty\]
\end{proof}
\end{lemma}

\begin{theorem}[Chebyshev inequality] \quad\\
Let $x$ be a non negitive r.v. then for any $c>0$:
\[P(x>c) \leq c^{-1}EX\]
\begin{proof}
Define \[y(\omega) = \begin{cases} c & if \; \omega \;  is \; st \;X(\omega)>c\\
0 & otherwise \end{cases} \]
\[Y\leq X \; a.s.\; \Rightarrow \underbrace{EY}_{ = c\cdot P(x>c)} \leq EX\]
\end{proof}
\end{theorem} 

\begin{theorem} Let $X$ be a random variable on $(\Omega , \Sigma , P)$ and $h:\RR \rightarrow \RR$ integrable on $(\RR , \mathcal{B} , leb)$. Then 
\[Eh(x) = \int h(x) d\mu_X(x) \]
\[
\begin{diagram}
\node{\Omega} \arrow{e,t}{X}  \arrow{se}
\node{\RR}  \arrow{s,r}{h} \\
 \node[2]{\RR}
\end{diagram}
\]
\begin{proof}\quad \\
\begin{enumerate}
\item $h = {\mathbf 1}_B, \; b \in \mathcal{B}$
\[Eh(x) = E{\mathbf 1}_B(x) = 1\cdot P(X \in B)\]
\[\int h(x)d\mu_X(x) = \int {\mathbf 1}_B(x)d\mu_X(x) = 1 \cdot \mu_X(B) = P(X \in B)\]
\item $h = \sum_{i=1}^{m}a_i{\mathbf 1}_{B_i}$\\
\[\text{The formula holds by linearity of the integral}\]
\item $h\geq 0$.
\[\underset{positive)}{\underset{(simple,}{h_n}} \nearrow h \overset{MCT}{\Rightarrow} \int h_nd\mu_x \rightarrow \int hd\mu_x\]
\[\underset{positive)}{\underset{(simple,}{h_n(x)}} \nearrow h(x) \overset{MCT}{\Rightarrow} E h_n(x) \rightarrow Eh(x)\]
\item $h$ arbitrary
\[ h = \underbrace{h^{+}}_{\geq 0} -\underbrace{h^{-}}_{\geq 0} \Rightarrow Eh(x) = \int h(x)d\mu_X(x)\] 
\end{enumerate}
\end{proof}
\end{theorem}

\begin{corollary}
if $X$ has density $h(x)$ then 
\[EX = \int h(x)f(x)dx \]
in particular if $h(x) = x$
\[EX = \int xf(x)dx  \quad \text{(old formula)} \]
and for $h(x) = x^2$
\[EX^2 = \int x^2f(x)dx \]
If $X$ has finitely, or countably, many values 
\[Eh(x) = \sum_{i=1}^{n}h(a_i)\cdot P(x=a_i)\]
in particular if $h(x) = x$
\[EX = \sum_{i=1}{n}a_i\cdot P(x=a_i) \quad \text{(old formula)} \]
\end{corollary}

\begin{example} $X$- Bernulli
\begin{align*} EX &= 1\cdot P(x=1) + 0\cdot P(x=0) = \frac{1}{2}\\
 EX^2 &= 1^2\cdot P(x=1) + 0\cdot P(x=0) = \frac{1}{2}\\
 VarX &= EX^2 - (EX)^2= \frac{1}{2} - \frac{1}{4} = \frac{1}{4} \end{align*}
\end{example}

\begin{example} N(0,1)
\[EX = \int_{-\infty}^{\infty} t \frac{1}{\sqrt{2\pi}}e^{-t^2/2}dt = 0\]
\[EX^2 = \int_{-\infty}^{\infty} t^2 \frac{1}{\sqrt{2\pi}}e^{-t^2/2}dt = 0\]
\end{example}
\begin{example}
$n$ people collecting their suitcases at random \[\begin{array}{cccc}
1, &2, &\dots, &n\\
\downarrow &\downarrow & &\downarrow \\
\sigma(1) &\sigma(2) &\dots &\sigma(n) \end{array}\]
Pick a random permutaton $\sigma$ uniformly with probability $\frac{1}{n!}$\\
Probability of everyone getting wrong suitcase?\\
Expected number of correct suitcases?
\[N=X_1 + X_2 +\dots + X_n \quad X_i = \begin{cases} 1 & \text{if the ith passenger collected correct suitcase}\\
0 & \text{otherwise} \end{cases} \]
\[EN = \sum_{i=1}^{n}EX_i = \sum_{i=1}^{n}1\cdot P(X_i = 1) = \sum_{i=1}^{n}\frac{(n-1)!}{n!} = \frac{1}{n}n = 1\]
\end{example}

\section{Independence}

\begin{definition} Let $(\Omega , \Sigma , P)$ be a probability space. Events $A,B \in \Sigma$ are independent if 
\[P(A \cap B) = P(A)\cdot P(B)\]
two $\sigma$-algebras $\Sigma_1 , \Sigma_2 \subset \Sigma$ are independent if 
\[P(A \cap B) = P(A)\cdot P(B) \quad \text{for any} \; A \in \Sigma_1 \; B \in \Sigma_2 \]
finitely many $\sigma$-algebras $\Sigma_1 , \Sigma_2 , \dots , \Sigma_n \subset \Sigma$ are independent if 
\[P\left(\cap-{i=1}^{n}A_i\right) = \prod_{i=1}^nP(A_i) \; whenever \; A_i \in \Sigma_i \; 1\geq i \leq n \]
a sequence of  $\sigma$-algebras $\Sigma_1 , \Sigma_2 , \dots \subset \Sigma$ are independent for any $n$\\
let $X,Y$ be random variables, they are independent if 
\[\sigma(x) \coprod \sigma(y) \]
Recall: 
\[\sigma(x) = \{ X^{-1}(B), \; B \in \mathcal{B}\} = \{X \in B, \; B \in \mathcal{B}\}\]
"information which we can get from $X$".
\end{definition} 

\begin{example} \quad \\
$X \sim$ coin toss: Bernulli\\
$Y \sim$ roll a die: $P(x=1) = \dots = P(x=6) = \frac{1}{2}$
\begin{align*}
&\Omega = \{0,1\} \times \{1,2,3,4,5,6\}\\
&\Sigma = 2^{\Omega}\\
&P(\text{each point}) = \frac{1}{2}
\end{align*} 

\[\begin{diagram}
\node{ \begin{array}{ccc} \; & 0 &1\\ 1 &. &.\\ 2 &. &. \\ 3&. &. \\ 4 &. &. \\ 5 &. &. \\ 6 &. &. \end{array}} \arrow{e,t}{X}  \arrow{ese,b}{Y}
\node{\{0,1\}}  \\
 \node[3]{\{1,2,3,4,5,6\}}
\end{diagram} \]

\[ \left.\begin{aligned}
        X(\omega_1, \omega_2) &= \omega_1\\
        Y(\omega_1, \omega_2) &= \omega_2
       \end{aligned}
 \right\}
 \qquad \text{are they independent?}\]

\begin{align*}
\sigma(x) &= \{\phi, \Sigma, \underbrace{\{(1,i), \; i = 1\dots 6\}}_{\{X=1\}} , \underbrace{\{(0,i), \; i = 1\dots 6\}}_{\{X=0\}}\}\\
\sigma(y) &= \{\phi, \Sigma,\{y=1\}, \dots \{y=6\}, \{y=3\, or \, 4 \, or \, 6\} \: etc \dots\}
\end{align*}
\[\underbrace{P\{X=1, \; Y=6\} = \frac{1}{12}, \quad P\{X=1\}= \frac{1}{2} \; P\{Y=6\} = \frac{1}{6}}_{\text{\large $\frac{1}{12}= \frac{1}{2}\cdot \frac{1}{6}$}}\]
similarly check this for all pairs of sets!
\end{example}

\begin{definition}
Let $\mathcal{I}$ be a collection of sets, it is called a $\pi$-system if $\forall A,B \in \mathcal{I}$ \[\quad A\cap B \in \mathcal{I}\]
\end{definition}
\begin{example}
\[
 \left.\begin{aligned}  &\left.\begin{aligned}
       &\{(-\infty, t), \; t \in \RR\} \\
       &\{(-\infty, t], \; t \in \RR\} \\
&\{\{(a, b): a<b\}, \phi\}\end{aligned} \right\}
 \: \text{generate $\mathcal{B}$} \\
& \; \: \{\{1\},\{2\},\{3\},\phi\}
       \end{aligned} \;
 \right\}
 \quad \text{$\pi$-system on $\mathcal{B}$}\]
\end{example}

\begin{example}
\[
 \left.\begin{aligned} 
       &\{ \{x<t\}, \; t \in \RR\} \\
        &\{ \{x\leq t\}, \; t \in \RR\}
       \end{aligned} \; \right\}
 \quad \text{$\pi$-system generating } \sigma(x)\]
because 
\[\{x<t\}\cap \{x<s\} = \{x<\min(t,s)\} \; etc\]
\end{example}
\begin{example}
If $X,\; \underbrace{\text{takes finitely or countably many values}}_{discrete} \; a_1, a_2 , \dots$
\[\{\{x=a_1\}, \{x=a_2\}, \dots \ \phi \} - \text{$\pi$-system generating } \sigma(x)\]
\end{example}

\begin{theorem}\label{ind1}
Let $(\Omega, \Sigma)$ be a set with $\sigma$-algebra and $mathcal{I}$ be a $\pi$-system generating $\Sigma$. Let $\mu_1 , \mu_2$ be measures such that \begin{enumerate}
\item $\mu_1(\Omega) = \mu_2(\Omega) < \infty$
\item $\mu_1(I) = \mu_2(I)$ for any $I \in \mathcal{I}$ \end{enumerate}
then \[\mu_1(A) = \mu_2(A)\quad \forall A \in \Sigma\]
\begin{proof} Stated without proof \end{proof}
\end{theorem}

\begin{theorem}
Let $X,Y$ be rv's and $\mathcal{I}, \mathcal{J}$ be $\pi$-systems generating $\sigma(x)$ and $\sigma(y)$.
\[ \mathcal{I}\; and  \mathcal{J} \text{ are independent} \Rightarrow X \;and\; Y \text{ are independent}\]
 \begin{proof} Stated without proof \end{proof}
\end{theorem}

\begin{corollary}\quad \\
\begin{enumerate}[(a)]
\item To check the independence of $X$ and $Y$ it suffices to check \[P(X<t, Y<s) = P(X<t)P(Y<s)\]
\item if $X$ and $Y$ are discrete, taking values $a_1, a_2, \dots$ and $b_1,b_2,\dots$ then
\[P(X=a_i,Y=b_i)=P(X=a_i)P(Y=b_i) \quad\forall i,j\]
\end{enumerate}

\begin{proof}[Proof of (a)] fix $I \in \mathcal{I}$
\begin{align*} \mu_1(B) &= P(I\cap B) \quad on\; \sigma(Y)\\
\mu_2(B) &= P(I)P(B) \end{align*}
 $\mu_1 , \mu_2$ are measures  such that  $\mu_1(\Omega) = \mu_2(\Omega) = P(I) < \infty$ and they agree on $\mathcal{J}$
\begin{align*}
&\overset{Thm}{\underset{\ref{ind1}}{\Rightarrow}} \mu_1(B) = \mu_2(B) \quad \forall B \in \sigma(Y)\\
&\;\Rightarrow P(X=a_i,Y=b_i)=P(X=a_i)P(Y=b_i) \quad\forall I \in \mathcal{I}, \; B \in \sigma(Y) \end{align*}
\end{proof}
\begin{proof}[Proof of (b)] fix $B \in \sigma(Y)$. Define \begin{align*}
v_1(A) &= P(A \cap B) \quad on\;\sigma(X)\\
v_2(A) &=P(A)P(B) \end{align*}
$v_1, v_2$ are measures such that $v_1(\Omega)=v_2(\omega)= P(B)<\infty$ and they agree on $\mathcal{I}$
\begin{align*}
&\overset{Thm}{\underset{\ref{ind1}}{\Rightarrow}} \text{they agree on }\sigma(X)\\
&\Rightarrow P(A\cap B) = P(A)P(B) \quad \text{whenever }A\in \sigma(X), \; B \in \sigma(Y) \end{align*}
\end{proof}
\end{corollary} 

\begin{example}
$X_1, X_2,\dots$ \begin{itemize}
\item independent
\item each $X_i$ has a prescribed distribution function $F_i$
\end{itemize}
do they always exist?\\\quad\\
\textbf{Trick Model}:\\
It suffices to construct $U_1, U_2, \dots$, which are independent and have uniform distribution, because $X_1 = G_1(U_1), X_2=G_2(U_2), \dots $ generalised as in the Skorokhod representation, \ref{sko}.
\[G=F^{-1} \qquad G(\omega) = \inf\{t:F(t)>\omega\} \]
\begin{itemize}
\item $X_i$ has distribution $F_i$
\item they are independent since $U_1, U_2, \dots$ are independent \end{itemize}
How do we construct $U_1, U_2, \dots$
\[([0,1] , \mathcal{B}, leb) \quad U(\omega)=\omega \; (Uniform!) = .\omega_1\omega_2\dots\]
$\omega_1,\omega_2,\dots$ are \begin{enumerate}[(a)]
\item bernulli
\item independent
\end{enumerate}
\[P(\omega_2 = 1) = \frac{1}{2}\quad \underset{\frac{1}{4}}{P(\omega_1 = 0, \omega_2= 1)} = \underset{\frac{1}{2}}{P(\omega_1 = 0)}\underset{\frac{1}{2}}{P( \omega_2= 1)}\]
$\omega= .\omega_1\omega_2\dots$
\begin{align*}
U_1(\omega) &= .\omega_1\omega_2\omega_{25}\omega_{100}\\
U_2(\omega) &= .\omega_2\omega_3\omega_{200}\\
U_3(\omega) &= .\omega_4\omega_{500}\omega_{1000}
\end{align*}
Uniform + Independent
\end{example}

\subsection{Finite and infinite occurance of Events}
\begin{example}
\[\begin{array}{cccccc}
X_1, &X_2 &X_3 &X_4 &\dots &Bernulli\\
1, &0, &1, &1, &\dots & \end{array}\]
"there will be infinitely many $1$'s in the sequence with probability $1$." \[E_n= \{X_n = 1\}\]
\[ E = \{\text{ infinitely many $1$'s in the sequence}\} = \{\forall N \in \NN \exists n \geq N \;X_n=1\}\]
\[ = \bigcap_{N=1}^{\infty}\bigcup_{n \geq N}\{X_n = 1\}\] \end{example}

\begin{notation}Let $(E_n)$ be a sequence of events
\begin{align*}
\{E_n \; i.o.\} &=\bigcap_{N=1}^{\infty}\bigcup_{n \geq N}E_n \quad \text{infinitely many of $E_n$ occur} \\
\{E_n\; i.o.\}^{c} &= \bigcup_{N=1}^{\infty}\bigcap_{n \geq N}E_n^c \quad \text{finitely many events occur}\end{align*}
\end{notation}
\begin{note} i.o. - infinitely often \end{note}

\subsection{The Borel-Cantelli Lemmas}
\begin{lemma}[Borel-Cantelli Lemma 1 - BC1]\label{bc1}
Let $(E_n)$ be events such that $\sum\limits_{i=1}^{\infty}P(E_n) < \infty$, then 
\[P(E_n \; i.o.) = 0\]
\begin{proof}
\[P(\bigcup_{n \geq N}E_n) \leq \sum_{n \geq N}P(E_n) \quad \forall n \in \NN\]
\[P(E_n \; i.o) =P(\bigcap_{N=1}^{\infty}\bigcup_{n \geq N}E_n) = \lim_{n\rightarrow \infty}P(\bigcup_{n \geq N}E_n) \leq \lim_{n\rightarrow \infty} \sum_{n \geq N}P(E_n) = 0\]
since $\sum\limits_{i=1}^{\infty}P(E_n) < \infty$.
\end{proof}
\end{lemma}

\begin{lemma}[Borel-Cantelli Lemma 2 - BC2]\label{bc2}
Let $(E_n)$ be events independent events such that $\sum\limits_{i=1}^{\infty}P(E_n) =\infty$, then 
\[P(E_n \; i.o.) = 1\]
\begin{proof}
\[P(E_n \; i.o.) = 1 \Rightarrow P(\{E_n \; i.o.\}^c) = P( \bigcup_{N=1}^{\infty}\bigcap_{n \geq N}E_n^c)=0\]
i.e\[P(\bigcap_{n \geq N}E_n^c) =0 \quad \forall n \in \NN\]
\begin{align*}
P(\bigcap_{n \geq N}E_n^c) &= \lim_{k\rightarrow \infty}P(\bigcap\limits_{n =N}^{k}E_n^c) = \lim_{k\rightarrow \infty}\prod_{n =N}^{k}\underbrace{P(E_n^c)}_{1-P(E_n)}\\
&\leq  \lim_{k\rightarrow \infty}\prod_{n =N}^{k}e^{-P(E_n)}= \lim_{k\rightarrow \infty}e^{-\sum_{n=N}^{k}P(E_n)} =0 \; \forall N \end{align*}
\end{proof}
\end{lemma}

\begin{remark}
$E_n = E$ with some $E$ such that $0<P(E)<1, \; \{E_n \;i.o.\} = E$ so $P(E_n \l i.o.) = p$ not $0,1$. BC2, \ref{bc2}, doesn't hold for dependent events. \end{remark}

To summarize if $(E_n)$ are independent then
\[P(E_n \; i.o.) = \begin{cases} 1 &if \;  \sum\limits_{i=1}^{\infty}P(E_n) =\infty\\
0 & if \; \sum\limits_{i=1}^{\infty}P(E_n) < \infty \end{cases} \]
\begin{ap}
$X_n$ iid assume all $X_n$ have exponential distribution. we wish to show:
\[\lim_{n \rightarrow \infty}\frac{X_n}{log(n)} = 1\]
$\lim\sup a_n =a$ means $a$ is the largest value such that there is a subsequence $a_{n_k}$ converging to $a$
\begin{example}\quad\\
\begin{enumerate}
\item $1,\;-1,\;1,\;-1,\dots$
\[\lim\sup = 1\]
\item $1,\;0,\;3,\;0, \; 5, \dots$
\[\lim\sup = \infty\]
\end{enumerate}

\end{example}

So if I want to prove $\lim\sup a_n=a$:
\begin{enumerate}
\item $\forall b>a,\quad a_n>b$ occurs for finitely many $n$
\item $\forall b<a,\quad a_n>b$ occurs for infinitely many $n$
\end{enumerate}
So for 
\[\lim_{n \rightarrow \infty}\frac{X_n}{log(n)} = 1\]
\begin{enumerate}
\item $b>1$
\[P\{\frac{X_n}{log(n)} >b \;\;i.o\} =0\]
\[\sum_{i=1}^{\infty}P(\frac{X_n}{log(n)}>b) = \sum_{i=1}^{\infty}P(X_n>b\,log(n))= \sum_{i=1}^{\infty}e^{-b\,log(n)} = \sum_{i=1}^{\infty}\frac{1}{n^b}<\infty\]
\item $b<1$
\[P\{\frac{X_n}{log(n)} <b \; \;i.o\} =1\]
\[\sum_{i=1}^{\infty}P(\frac{X_n}{log(n)}<b)= \sum_{i=1}^{\infty}\frac{1}{n^b}=\infty\]
\end{enumerate}
\end{ap}



\end{document}
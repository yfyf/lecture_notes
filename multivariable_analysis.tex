\documentclass[11pt]{article}
\usepackage{amsfonts, amsthm, amsmath}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{pb-diagram}
\usepackage{hyperref}
\usepackage{cancel}
\usepackage[all]{xy}
\usepackage{framed}

\setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{0in}
\setlength{\textheight}{9in}
\setlength{\topmargin}{0in}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\parskip}{0pt}
\setlength{\parindent}{0pt}


\def\maj{\mathcal{J}}
\def\maw{\mathcal{W}}
\def\mat{\mathcal{T}}
\def\mas{\mathcal{S}}
\def\CC{\mathbb{C}}
\def\MM{\mathbb{M}}
\def\FF{\mathbb{F}}
\def\PP{\mathbb{P}}
\def\QQ{\mathbb{Q}}
\def\RR{\mathbb{R}}
\def\ZZ{\mathbb{Z}}
\def\NN{\mathbb{N}}
\DeclareMathOperator{\alt}{Alt}
\DeclareMathOperator{\dive}{div}
\DeclareMathOperator{\curl}{curl}
\def\head#1{\medskip \noindent \textbf{#1}.}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem*{note}{Note}
\newtheorem*{remark}{Remark}
\newtheorem*{recall}{Recall}
\newtheorem*{claim}{Claim}
\newtheorem*{properties}{Properties}
\newtheorem*{notation}{Notation}
\newtheorem*{aim}{Aim}






\begin{document}
\title{Multivariate Analysis\\ MATH 3109\\UCL}
\author{Based on lectures by Dr. Iannis Petridis\\Typed by John Sylvester}
\date{Autum 2011}
\maketitle

\tableofcontents
\setcounter{tocdepth}{4}
\newpage

\section{Mulivarible  Calculus}
\subsection{Notation}
$X\in\RR^{n}$, $ X=\{x_{1},x_{2},\dots,x_{n}\}$ where $x_{i}\in\RR$
$\RR^{n}$ is a vector space\\
length norm:\[|x|=\sqrt{x_1^2 + x_2^2 + \dots +x_n^2 }\]
If $Y,X\in\RR^{n}$ and $ Y=\{y_{1},y_{2},\dots,y_{n}\}$ then \[X \cdot Y=x_{1}y_{1}+ x_{2}y_{2} +\dots + x_{n}y_{n}\]
Standard Basis:
\begin{align*}
  e_j=(0, \dots &,  0, 1, 0, \dots)\\
& \textrm{ \scriptsize j-1,\: j, \:j+1} 
\end{align*}
Properties of norm
\[|x|\geq0\]
\[|x|=0  \Leftrightarrow x=\vec{0}\]
\[| \lambda x| = | \lambda| \cdot |x| , \quad x \in \RR^n, \quad \lambda \in \RR \]

\textbf{linear Transformation}
\[T:\RR^{n}\rightarrow\RR^{n}\]
\begin{enumerate}[(i)]
\item $ T(x+y)=T(x) + T(y)$
\item $T(\lambda x) =\lambda T(x) $
\end{enumerate}
Matrix Representation of T with respect to the standard basis:\\
\[T(e_i)=\sum_{j=1}^{m}a_{i,j}e_j \quad\textrm{ where }\quad [T]_{\epsilon}^{\epsilon}=A=(a_i,j)_{\substack{i=1,\dots ,m \\ j=1,\dots ,n}}\]

Given: $T:\RR^{n}\rightarrow\RR^{m}, S:\RR^{n}\rightarrow\RR^{m} \textrm{ and } U:\RR^{m}\rightarrow\RR^{k}$
\begin{enumerate}[(i)]
\item $ [UT]_{kxm}=[U]_{kxm}[T]_{mxn}$
\item $[T+S]=[T]+[S]$
\item $\lambda[T]=[\lambda T]$
\end{enumerate}
$T:\RR^{n}\rightarrow\RR^{m}, X\in \RR^{n}, Y\in \RR^{m}, X=(x^{1},\dots ,x^{n}), Y=(y^{1},\dots ,y^{m})$\\
\[  \left(\! \begin{array}{c} y^{1} \\ y^{2}\\ \vdots\\ y^{m} \end{array}\! \right) = [T] \left(\! \begin{array}{c} x^{1} \\ x^{2}\\ \vdots\\ x^{n} \end{array}\! \right) \]

\subsection{Functions \& Continuity}
$f:\RR^{n} \rightarrow\RR^{m}$ vector valued function\\
$f:A \rightarrow\RR^{m}$ where $A \subset \RR^{n}$\\
then $f$ has components which are scalar fields.\\
$ f^{i}:A \rightarrow\RR$\\
\[f(x)=(f^{1} (x),\dots ,f^{m}(x))\] 

$\Pi^{i}:\RR^{m}\rightarrow\RR $
\[\Pi^{i}((x)^{1},\dots ,(x)^{m})\]
$\Pi^{i}$ is a linear transformation for i=1,$\dots$,m\\

\[
\begin{diagram}
\node{\RR^{n}} \arrow{e,t}{f}  \arrow{se,b}{f^{i}}
\node{\RR^m}  \arrow{s,r}{\Pi^{i}} \\
 \node[2]{\RR}
\end{diagram}
\]

\begin{definition}
$f:\RR^{n} \rightarrow \RR^{m}$ then 
$\lim_{x\to a} (f(x))=b$ means:
\[
\forall \epsilon > 0, \exists \delta > 0 \; st,\; 
0<|x-a|<\delta \Rightarrow |f(x)-b|<\epsilon\] 
\end{definition}

\begin{definition}
$f$ is called continuious at a if:
\[\lim_{x\to a} (f(x))=f(a)\]
$f$ is called continuious on the set A if it is continuious at a, $\forall a \in A$
\end{definition}

\begin{theorem}[Combination Theorm]\label{T:Combination Theorm}
Assume \[\lim_{x\to a} (f(x))=b, \quad lim_{x\to a} (g(x))=c\]
then:
\begin{enumerate}[(i)]
\item$\lim_{x\to a} (f(x) + g(x))=b+c$
\item$\lim_{x\to a} (\lambda f(x))=\lambda b$
\item$\lim_{x\to a} (f(x)\cdot g(x))=b\cdot c$
\item$\lim_{x\to a} |f(x)|=|b|$
\end{enumerate}
\begin{proof}
of (iii)
\begin{align*}
f(x)\cdot g(x)-b\cdot c &= f(x)\cdot g(x) -b\cdot g(x) + b\cdot g(x) -b\cdot c \\
&= g(x)\dot (f(x)-b) + b\cdot (g(x) - c) \\
|f(x)\cdot g(x)-b\cdot c|&= |g(x)\dot (f(x)-b) + b\cdot (g(x) - c)|\\
 &\leq |g(x)\dot (f(x)-b)| +| b\cdot (g(x) - c)|\\
\end{align*}
Cauchy-Schwartz: $|x^{1}y^{1} + \dots +x^{n}y^{n}| \leq \sqrt{(x^{1})^{2} + \dots +(x^{n})^{2}} \cdot \sqrt{(y^{1})^{2} + \dots +(y^{n})^{2}}$
\[|f(x)\cdot g(x)-b\cdot c| \leq |g(x)\dot (f(x)-b)| +| b\cdot (g(x) - c)|  \leq |g(x)|\cdot |f(x)-b| +|b|\cdot |g(x) - c|\]
Since $ lim_{x\to a} (g(x))=c$, $g$ is a bounded neighbourhood of a, i.e: 
\[
\exists M  \geq 0, \; \exists \delta > 0 \; st,\; 
|g(x)| \leq M \; for \; |x-a|<\delta\] 
\end{proof}
\end{theorem}

\begin{remark} We have:\\
\begin{enumerate}[(i)]
\item $f:\RR^{n} \rightarrow \RR^{m}$ is continuous iff: $f^{i}:\RR^{n} \rightarrow \RR$ is continuous for $i=1,\dots , m$
\item Polynomial functions in n-variables, $f(x^{1}, \dots ,x^{n})$, are continuous
\item Rational functions, $R(x)= \frac{P(x)}{Q(x)}$, are continuous where defined, ie: $Q(x) \neq 0$ and P, Q are polynomials in n-variables.
\end{enumerate}
\end{remark}

\begin{theorem}\label{T:Lin trans cont}
Linear transformations are continuous.
\begin{proof}
$T:\RR^{n} \rightarrow \RR^{m}$ let $a \in \RR^{n}$ to show: \[\lim_{h\to 0}T(a+h) = T(a)\] where $h = (h^1 , \dots , h^n )$
\begin{align*}
|T(a+h) - T(a)| & = |T(h)|=|T(h^{1}e_{1}+ \dots +h^{n}e_{n})| =|h^{1}T(e_{1}) + \dots +h^{n}T(e_{n})|\\ 
&\leq  |h^{1}||T(e_{1})|+ \dots |h^{n}||T(e_{n})| \leq |h|(T(e_{1})+ \dots T(e_{n})) \\
So: \quad |T(a+h) - T(a)| &\leq  M|h| \quad where \quad M= \sum_{i=1}^{n}|T(e_i)| \\
So \: given \quad \epsilon > 0,\quad choose \quad \delta &= \frac{\epsilon}{M} \quad such \: that \quad |h|< \delta \Rightarrow |T(a+h) - T(a)|< \epsilon \\
\end{align*}
\end{proof}
\end{theorem}

\begin{example}
$f(x,y)= \frac{x^{2} - y^{2}}{x^{2} +y^{2}}, \quad (x,y)=(0,0)$ assume $\quad \lim_{(x,y) \to (0,0)} f(x,y) = L$

\[\forall \epsilon > 0, \quad  \exists \delta >0  \quad such\: that \quad 0<|(x,y)|<\delta \Rightarrow |f(x,y)-L|<\epsilon \]
\text{Plug $(x,0)$ into $f$:} 
\[f(x,0) \;= \frac{x^{2}-0}{x^{2}-0} = 1\]
\text{Plug $(0,y)$ into $f$:} 
\[ f(0,y)\;= \frac{0-y^{2}}{0 +y^{2}} = -1\]
\begin{align*}
If \: |x|< \delta \quad |f(x,0)| < \delta & \Rightarrow |f(x,0) - L|< \epsilon \quad ie \quad |1-L|< \epsilon\\
If \: |y|< \delta \quad |f(0,y)| < \delta& \Rightarrow |f(0,y) - L|< \epsilon \quad ie \quad |-1-L|< \epsilon\\
&\Rightarrow \epsilon = \frac{1}{2} \quad contradiction! \\
\shortintertext{Now consider $ y=mx, m \in \RR$}
&f(x,mx)=\frac{x^{2} - (mx)^{2}}{x^{2} + (mx)^{2}} = \frac{1-m^{2}}{1+m^{2}}\\
&\lim_{x \to 0}(\lim_{y \to 0} f(x,y)) = \lim_{x \to 0}1 = 1\\
&\lim_{y \to 0}(\lim_{x \to 0} f(x,y)) = \lim_{y \to 0}-1 = -1\\
\end{align*}
\text{However checking along straight lines is  not enough to prove continuity.}\\

\end{example}

\begin{example}

\[
 f(x,y) =
  \begin{cases}
   \frac{xy}{\sqrt{x^{2} + y^{2}}} & \text{if } (x,y) \neq (0,0) \\
   0       & \text{if } (x,y) = (0,0)
  \end{cases}
\]
Show f is continuous at (0,0)
\[\forall \epsilon > 0, \quad \exists \delta>0\]
\[\left| \frac{xy}{\sqrt{x^{2} + y^{2}}}\right| \leq \frac{|x| \cdot |y|}{\sqrt{x^{2} + y^{2}}} \leq \frac{\sqrt{x^{2} + y^{2}} \cdot \sqrt{x^{2} + y^{2}}}{\sqrt{x^{2} + y^{2}}}= \sqrt{x^{2} + y^{2}} = |(x,y)|\]
Since: \[\begin{diagram}
\node[3]{.} \arrow{s,r,-}{y} \arrow{wsw,t,-}{ \sqrt{x^{2} + y^{2}}} \\
\node{.} \arrow[2]{e,b,-}{x} \node[2]{.}
\end{diagram}\]
\end{example}
\begin{note}
if the total degree of the neumerator is higher than the denominator in a rational function, then the limit should be 0.
\end{note}

\begin{theorem}\label{T:composition}
If $f$ is continuous at a and $g$ is continuous at $f(a)$ then $g \circ f$ is continuous at a.
\end{theorem}

\subsection{Partial Derivitives}

\begin{definition}
Let $f:\RR^{n} \rightarrow \RR, \: a\in \RR$
Define: \[ D_{i}f(a) = \lim_{h \to 0}\frac{f(a^{1}, \dots , a^{i-1}, a^{i}+h, a^{i+1}, \dots , a^{n})}{h}\]
\end{definition}

\begin{example}
if $f:\RR^{2} \rightarrow \RR$
\[\left.\frac{df}{dx}\right| _{(a,b)} = D_{1}f(a,b)\]
\[\left.\frac{df}{dy}\right| _{(a,b)} = D_{2}f(a,b)\]
\[and \: in \: \RR^{3} \: we\: use \: \frac{df}{dx}, \frac{df}{dy} \: and \: \frac{df}{dz} \: etc.\]
\end{example}

\begin{example}
\begin{align*}
 f(x,y) &=
  \begin{cases}
   \frac{x^{2} - y^{2}}{x^{2} + y^{2}} & \text{if } (x,y) \neq (0,0) \\
   1       & \text{if } (x,y) = (0,0)
  \end{cases}\\
 D_{1}f(0,0) =\left.\frac{df}{dx}\right| _{(0,0)} &=  \lim_{x \to 0}\frac{f(x,0) - f(0,0)}{x}= \lim_{x \to 0}\frac{\frac{x^{2}-0}{x^{2}-0} -1}{x} = 0\\
D_{2}f(0,0) =\left.\frac{df}{dy}\right| _{(0,0)} &=  \lim_{y \to 0}\frac{f(0,y) - f(0,0)}{y}= \lim_{y \to 0}\frac{\frac{0-y^{2}}{0+y^{2}} -1}{y} = \lim_{y \to 0} \frac{-2}{y}= \pm \infty\\
\end{align*}
\end{example}

\subsection{Total Derivitive}


In 1 dimention we write the following for the derivitive of $f:\RR \rightarrow \RR$
\[\quad f^{'}(a)=\lim_{h \to 0}\frac{f(a+h) - f(a)}{h}\]
we try to write it in higher dimentions $f:\RR^{n} \rightarrow \RR^{m}$ in this form
\begin{align*}
\lim_{h \to 0}\left[\frac{f(a+h) - f(a)}{h} - f^{'}(a)\right] &=\lim_{h \to 0}\left[\frac{f(a+h) - f(a) - h \cdot f^{'}(a)}{h}\right]\\
&=\lim_{h \to 0}\frac{|f(a+h) - f(a) - h \cdot f^{'}(a)|}{|h|} =0 \\
\end{align*}
For $f:\RR^{n} \rightarrow \RR^{m}$ consider the tangent line at a: $y=f(a) +f^{'}(a)(x-a)$\\
call $x-a = h$ then we have: \[y=f(a) +f^{'}(a)(h)\]
this is an Affine transformation, not a linear map.\\
Look at the map:
\[\lambda:h \rightarrow hf^{'}(a), \quad h \in \RR\]
This is a linear map.
\begin{align*}
\lambda(h_{1} + h_{2})&=(h_{1} + h_{2})f^{'}(a)= h_{1}f^{'}(a) + h_{2}f^{'}(a) =\lambda(h_{1}) + \lambda(h_{2})\\
\lambda(\alpha \cdot h)&=(\alpha h)f^{'}(a)=\alpha(hf^{'}(a))=\alpha \cdot\lambda( h)\\
&\lim_{h \to 0}\frac{|f(a+h) - f(a) - \lambda(h)|}{|h|} =0 \\
\end{align*}

\begin{definition}[Total Derivitive]\label{D:Total Derivitive}
$f:\RR^{n} \rightarrow \RR^{m}\: or \:(f:A \rightarrow \RR^{m}, \: A \subset \RR^{n}, \:A\;is\;open)$
is differentiable at a $(a \in A)$ if we can find a linear transformation $ \lambda:\RR^{n} \rightarrow \RR^{m}$ st:
\[\lim_{h \to 0}\frac{|f(a+h) - f(a) - \lambda(h)|}{|h|} =0 \]
The linear transformation $\lambda$ is called the total derivitive of f at a and denoted Df(a) st
\[Df(a)=\lambda(h)\] 
\end{definition}

\begin{example} 
$f:\RR^{n} \rightarrow \RR^{m}, \: f(x)=k, \: k \in\RR^{m} $ is differentiable at $a\in\RR^{n}$ with the 0 linear transformation $0:\RR^{n} \rightarrow \RR^{m}, \: 0(h)=0$
\[\lim_{h \to 0}\frac{|f(a+h) - f(a) - 0(h)|}{|h|} = \lim_{h \to 0}\frac{|k - k - 0|}{|h|}= 0 \]
\end{example}

\begin{example}
If $f:\RR^{n} \rightarrow \RR^{m}$ is a linear transformaton, it is differentiable at $a\in\RR^{n}$ with linear transformation $Df(a) = f$
\[\lim_{h \to 0}\frac{|f(a+h) - f(a) - f(h)|}{|h|}= \lim_{h \to 0}\frac{|f(a+h -a -h)|}{|h|} = 0\]
\end{example}

\begin{theorem}[Uniqueness of Total Derivitive]\label{T:Uniqueness of Total Derivitive}
If f is differentiable at a then there exists a unique linear transformation, $ \lambda:\RR^{n} \rightarrow \RR^{m}$, such that
\[\lim_{h \to 0}\frac{|f(a+h) - f(a) - \lambda(h)|}{|h|} =0 \]
\begin{proof}
suppose $ \mu:\RR^{n} \rightarrow \RR^{m}$ is another linear transformation such that:
\[\lim_{h \to 0}\frac{|f(a+h) - f(a) - \mu(h)|}{|h|} =0 \]
deduce that $\lambda= \mu \;\;  i.e \; \;\forall h\in\RR^{n}$
 \[ \lambda(h)= \mu(h)\]
\begin{align*}
\frac{|\lambda(h)- \mu(h)|}{|h|}&= \frac{|\lambda(h) +f(a) -f(a+h) +f(a+h) -f(a)- \mu(h)|}{|h|}\\
&\leq \frac{|f(a+h) -f(a) -\lambda(h)|}{|h|} +  \frac{|f(a+h) -f(a) -\mu(h)|}{|h|}\\
\shortintertext{Conclude that:} \qquad \qquad \qquad &\\
\lim_{h \to 0}\frac{|\lambda(h)- \mu(h)|}{|h|} &\leq 0 + 0 =0 \quad (*)
\end{align*}
Let h=0 $\lambda= 0=\mu$ since $\lambda, \mu$ are linear. Now fix $h \in \RR^{n}$, $h\neq0$ and let $t\in\RR$ such that $th\in\RR^{n} $ then replace $h$ with $th$ in (*):
\begin{align*}
\lim_{t \to 0}\frac{|\lambda(th)- \mu(th)|}{|th|}&= \lim_{t \to 0}\frac{|t\lambda(h)- t\mu(h)|}{|t||h|}\\
&= \lim_{t \to 0}\frac{|t|}{|t|}\frac{|\lambda(h)- \mu(h)|}{|h|}= \frac{|\lambda(h)- \mu(h)|}{|h|} = 0\\
\end{align*}
\[\Rightarrow |\lambda(h)- \mu(h)|=0 \Rightarrow \lambda(h)= \mu(h)\]
deduce that $\lambda= \mu \;\;  i.e \; \;\forall h\in\RR^{n}$
 \[ \lambda(h)= \mu(h)\]

\end{proof}
\end{theorem}

\begin{definition}[Jacobian Matrix]\label{D:Jacobian Matrix}
$f:\RR^{n} \rightarrow \RR^{m}$ is differentiable at $a \in \RR^{n}$ and it is derivitive at a $Df(a):\RR^{n} \rightarrow \RR^{m}$ is a linear map. Then the matrix representation of $Df(a)$ is $f^{'}(a) \in \MM_{mxn}$ and is called the Jacobian Matrix of f at a.
\end{definition}

\begin{example}\label{Df example}
$f:\RR^{2} \rightarrow \RR^{2}, \quad f(x,y)=(x^{2}y,x+5) \quad x,y \in \RR$\\
Show that $Df(1,2)(h^{1}, h^{2})=(4h^{1} +  h^{2}, h^{1})$:
\begin{align*}
&f((1,2) +(h^{1}, h^{2})) - f(1,2) -Df(1,2)(h^{1}, h^{2})\\
&= f(1+h^{1}, 2+ h^{2}) - f(1,2) - (4h^{1}+ h^{2}, h^{1})\\
&=((1+h^{1})^{2}(2+ h^{2}), (1+h^{1} +5)) - (2,6)  - (4h^{1}+ h^{2}, h^{1})\\
&=(2+ h^{2} + 2(h^{1})^{2} +(h^{1})^{2}h^{2} + 2h^{1}h^{2} + 4h^{1} -2 -4h^{1} -h^{2}, 6+h^{1} - 6 -h^{1})
\end{align*}
\textrm{Take length:}
\[|(2(h^{1})^{2} +(h^{1})^{2}h^{2} + 2h^{1}h^{2}, 0)| \leq 2|h|^{2} +|h|^{2}|h| + 2|h||h| = 4|h|^{2} + |h|^{3}\]
\textrm{So:}
\begin{align*}
&\lim_{h \to 0}\frac{|f((1,2) +(h^{1}, h^{2})) - f(1,2) -Df(1,2)(h^{1}, h^{2})|}{|h|}\\
& \leq
\lim_{h \to 0}\frac{4|h|^{2} + |h|^{3}}{|h|} =  \lim_{h \to 0}4|h| + |h|^{2}=0
\end{align*}
\end{example}

\begin{definition}
$f^{'}(a)$ is the matrix representation of $Df(a)$
\[Df(a)(h)^{t}= \left(\! \begin{array}{c} y^{1} \\ y^{2}\\ \vdots\\ y^{m} \end{array}\! \right) = f^{'}(a) \left(\! \begin{array}{c} h^{1} \\ h^{2}\\ \vdots\\ h^{n} \end{array}\! \right) \]
\[ f^{'}(a) = \begin{pmatrix}
  D_{1}f^{1}(a) & D_{2}f^{1}(a) & \cdots &D_{n}f^{1}(a) \\
  D_{1}f^{2}(a) & D_{2}f^{2}(a) & \cdots & D_{n}f^{2}(a) \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  D_{1}f^{m}(a) & D_{2}f^{m}(a) & \cdots & D_{n}f^{m}(a)
 \end{pmatrix}\]
\end{definition}

\begin{example} With this new information we can tackle example \ref{Df example}:\\
$f:\RR^{2} \rightarrow \RR^{2}, \quad f(x,y)=(x^{2}y,x+5) \quad x,y \in \RR$\\
Show that $Df(1,2)(h^{1}, h^{2})=(4h^{1} +  h^{2}, h^{1})$:
\[\frac{df^{1}}{dx} = 2xy, \quad \frac{df^{1}}{dy} = x^{2}, \quad \frac{df^{2}}{dx} = 1, \quad \frac{df^{2}}{dy} = 0\]
\[f^{'}(1,2)=\begin{pmatrix}
 4&1 \\
  1&0 \\
 \end{pmatrix}\]
\[f^{'}(1,2) \left(\!\! \begin{array}{c} h^{1} \\  h^{2} \end{array}\!\! \right)= \begin{pmatrix}
 4&1 \\
  1&0 \\
 \end{pmatrix}\!\!\! \left(\!\! \begin{array}{c} h^{1} \\  h^{2} \end{array}\!\! \right)=  \left(\!\!\! \begin{array}{c} 4h^{1} + h^{2}\\  h^{2} \end{array}\!\! \right)\]
\end{example}

\begin{remark}
Having  directional derivitives in all directions $u\neq 0$ is not enough to guarantee $df(a)$ exists.
\end{remark}

\begin{theorem}
If $f$ is differentiable at a then $f$ is continuous at a.
\begin{proof}
\begin{align*}
\lim_{h \to 0}|f(a+h)-f(a)| &= \lim_{h \to 0}|f(a+h)-f(a) -Df(a) + Df(a)|\\
&\leq \lim_{h \to 0}\frac{|f(a+h)-f(a) -Df(a)(h)|}{|h|}\cdot|h| + \lim_{h \to 0}| Df(a)(h)|\\ 
&= 0 
\end{align*}
since $Df(a)$ is a linear transformation $Df(a)$ is continuous so: \[\lim_{h \to 0}| Df(a)(h)| = | Df(a)(0)| = 0\].
\end{proof}
\end{theorem}

\subsection{The Chain Rule}
\begin{theorem}[Chain Rule]\label{T:Chain Rule}
if $f:\RR^{n} \rightarrow \RR^{m}$ is differentiable at a and $f:\RR^{m} \rightarrow \RR^{k}$ is differentiable at $f(a)$ then $g \circ f:\RR^{n} \rightarrow \RR^{k}$ is differentiable at a and 
\[D(g \circ f)(a) = Dg(f(a))\circ Df(a)\]

\[\begin{diagram}
\node{\RR^{n}} \arrow{e,t}{Df(a)}  \arrow{se,b}{D(g\circ f)(a)}
\node{\RR^m}  \arrow{s,r}{Dg(f(a))} \\
 \node[2]{\RR^{k}}
\end{diagram}\]
\[(g\circ f)^{'}(a)= g^{'}(f(a))\cdot f^{'}(a), \quad \textrm{where $\cdot$ represents matrix multiplication}\]

\begin{proof}
if $b=f(a)$ and we let $Df(a) = \lambda$ and $Dg(f(a))=\mu$ then if we define:
\begin{align}
\varphi(x) &=f(x)-f(a) -\lambda(x-a)\\
\psi(y) &= g(y) - g(b) - \mu(y-b)\\
\rho(x) &= g\circ f(x) - g\circ f(a) - \mu \circ \lambda (x -a)
\end{align}
\text{Then:}
\begin{align}
\lim_{h \to 0}\frac{|f(a+h)-f(a) -Df(a)(h)|}{|h|} &= \lim_{x \to a}\frac{|\varphi(x)|}{|x-a|} = 0\\
\lim_{h \to 0}\frac{|g(b+h)-g(b) -Dg(b)(h)|}{|h|} &= \lim_{y \to b}\frac{|\psi(y)|}{|y-b|} = 0
\end{align}
We must show:
\[\lim_{h \to 0}\frac{|g\circ f(x) - g\circ f(a) - \mu \circ \lambda (x -a)|}{|h|} = \lim_{x \to b}\frac{|\rho(x)|}{|x-b|} = 0\]
Now:
\begin{alignat*}{2}
\rho(x) &=g(f(x)) - g(b) - \mu(\lambda(x-a)) &\qquad & \\
&= g(f(x)) - g(b) - \mu(f(x) - f(a) - \varphi(x)) &\qquad & \textmd{by (1)}\\
&= [g(f(x)) - g(b) - \mu(f(x)-f(a))] + \mu(\varphi(x)) &\qquad &\\
&= \psi(f(x)) + \mu(\varphi(x)) &\qquad & \textmd{by (2)}
\end{alignat*}
Thus we must Prove
\begin{align}
&\lim_{x \to a}\frac{|\psi(f(x))|}{|x-a|} = 0\\
&\lim_{x \to a}\frac{|\mu(\varphi(x))|}{|x-a|} = 0
\end{align}
It follows from (4) that for some $\delta > 0 $  we have
\[|\psi(f(x))|<\epsilon|f(x) - b| \quad if\; |f(x) - b|<\delta\]
which is true if $|x-a|< \delta_{1} $ for a suitable $\delta_{1}$. We also have that if T is a linear transformation then $\exists M \geq 0\; such \; that\;  |T(x)|<M|x|$. So then:
\begin{align*}
|\psi(f(x))| &<\epsilon|f(x) - b| \\
&= \epsilon|\varphi(x) + \lambda(x-a)|\\
& \leq \epsilon|\varphi(x)| + \epsilon M|x-a|
\end{align*}
So
\[\lim_{x \to a}\frac{|\psi(f(x))|}{|x-a|} \leq \lim_{x \to a}\frac{\epsilon|\varphi(x)|}{|x-a|}  + \lim_{x \to a}\frac{\epsilon M|x-a|}{|x-a|} = \epsilon M \rightarrow 0\]
Also
\[\lim_{x \to a}\frac{|\mu\varphi(x)|}{|x-a|} \leq \lim_{x \to a}\frac{M|\varphi(x)|}{|x-a|} = 0\]
\end{proof}
\end{theorem}

\begin{theorem}\label{s}
Define $s:\RR^2 \rightarrow \RR \quad s(x,y)=x + y$ then $s$ is differentiable  and $Ds = s$
\begin{proof}
S is linear i.e
\begin{align*}
s((x,y) + (x^{'},y^{'})) &=s(x+x^{'},y+y^{'})= s(x,y) + s(x^{'},y^{'})\\
s(\lambda(x,y)) &= \lambda s(x,y)\\
\intertext{So}
\lim_{h \to 0}\frac{|s(a+h) - s(a) -s(h)|}{|h|} = 0
\end{align*}
\end{proof}
\end{theorem}

\begin{theorem}\label{p}
Define $p:\RR^2 \rightarrow \RR, \quad p(x,y)=xy$, then $p$ is differentiable  and:\\
$Dp(a,b):\RR^2 \rightarrow \RR$ is linear with $Dp(a,b)(h,k) = ak + bh$ and $p^{'} = (b,a)$
\begin{proof}
use of derivitive
\begin{align*}
p((a,b)+(h,k)) - p(a,b) - Dp(a,b)(h,k) &= p(a+h,b+k) - p(a,b) - (ak + bh)\\
&=(a+h)(b+k) - ab - (ak + bh) = hk\\
\frac{|p((a,b)+(h,k)) - p(a,b) - Dp(a,b)(h,k)|}{|(h,k)|} &= \frac{|hk|}{\sqrt{h^2 + k^2}} \leq  \frac{\sqrt{h^2 + k^2}\sqrt{h^2 + k^2}}{\sqrt{h^2 + k^2}} = \sqrt{h^2 + k^2} \rightarrow 0
\end{align*}
\end{proof}
\end{theorem}

\begin{remark}
To check some $T:\RR^n \rightarrow \RR^m$ is linear we listed two properties:
\begin{align*}
T(x+y) &= T(x) +T(y)\\
T(\lambda x) &= \lambda T(x)\\
\shortintertext{we can instead just check:}
T( \lambda x + y) &= \lambda T(x) + T(y)
\end{align*}
\end{remark}

\begin{corollary}
$f,g:\RR^{n} \rightarrow \RR$ differentiable at $a \in \RR^{n}$
\begin{enumerate}[(i)]
\item $D(f+g)(a) = Df(a) + Dg(a)$
\item Product rule: $D(f \cdot g)(a) = g(a)\cdot Df(a) + f(a) \cdot Dg(a)$
\item Quotient rule: if $g(a) \neq 0, \; D(\frac{f}{g})(a) = \frac{1}{g(a)^2}\cdot (g(a)\cdot Df(a) - f(a) \cdot Dg(a))$
\end{enumerate}
\end{corollary}
\begin{proof}
For (i):\\
We can consder the function $s$ from theorem~\ref{s}, $s:\RR^2 \rightarrow \RR \quad s(x,y)=x + y$, but acting on $f$ and $g$ ie $s(f,g) = f+g$ and $Ds =s $
\[D(f+g)(a)= Ds(f(a),g(a)) \circ D(f,g)(a) = s \circ(Df(a),Dg(a)) = Df(a) + Dg(a)\]
For (ii):\\
We can consder the function $p$ from theorem~\ref{p}, $p:\RR^2 \rightarrow \RR \quad p(x,y)=xy$, but acting on $f$ and $g$ ie $p(f,g) = fg$ with  $Dp(f,g)(h,k) = fk + gh$
\[D(f \cdot g)(a) = Dp(f,g)(a)\cdot D(f,g)(a)= Dp(f(a),g(a)) \cdot (Df(a),Dg(a)) = f(a)\cdot Dg(a) + g(a)\cdot Df(a)\]
(iii) follows from (ii)
\end{proof}

\subsection{Mixed Derivitives}

$f:\RR^n \rightarrow \RR  , a \in \RR$
\[D_{i} = lim_{h \rightarrow 0}\frac{f(a^1 , \dots ,a^{i-1},a^{i} + h , a^{i+1}, \dots , a^n ) - f(a)}{h}\]
if $D_{i}f(x)$ exists for al a in some open set $U$ then we get a function $ U \xrightarrow {D_{i}} \RR , \; x \rightarrow D_{i}f(x)$ then we can talk about partial derivitives of $D_{i}f$ eg $D_{j}(D_{i}f(x)) = D_{ij}f(x)$\\
If $D_{i}f(x)$ exists $ \forall x \in U$ this is a function of $x$ and we can consider  $D_{j}(D_{i}f(x)) = D_{ji}f(x)$\\
In general $i \neq j$ eg $f(x,y)=x^{3}y^{5}:$
\begin{alignat*}{2}
D_{1}f(x,y) &= 3x^{2}y^{5} &\qquad  D_{2}f(x,y) &= 5x^{3}y^{4}\\
D_{2,1}f(x,y) &= 15x^{2}y^{4} &D_{1,2}f(x,y) &= 15x^{2}y^{4}
\end{alignat*}

\begin{theorem}
If $D_{i,j}$ and $D_{j,i}$ are continuous on an open set containing $a$ then
 \[D_{i,j} = D_{j,i}\]
\begin{proof} from homework 5:\\ 
First we repeat the well-known proof that, if $g : U \rightarrow \RR$ is continuous and $g(p) > 0$,
then there exists a neighborhood $V$ of $ p (p \in V \subset U,\; V \; open)$ with
\[q \in V \Rightarrow g(q) > 0\]
Take $\epsilon = g(p)$ in the defnition of continuity of $g$. There there exists a $V$ open with
$p \in V$ and
\[q \in V \Rightarrow |g(q) - g(p)| < g(p)\]
Since 
\[g(p) - g(q) \leq |g(q) - g(p)| < g(p) \Rightarrow  -g(q) < 0 \Leftrightarrow g(q) > 0\]
we get the result. The set V can be taken to contain a closed rectangle $[a, b] \! \times\! [c, d]$.

We apply the result to $g = D_{1,2}f - D_{2,1}f$. Assume (by contradiction) that $g(p)$ is not
always 0. Then there exists a point $p$ with $g(p) \neq 0$. We can assume that $g(p) > 0$,
otherwise consider $-g$. The function $g$ is given to be continuous. We have (using
Fubini twice)
\[0<\int_{[a, b] \! \times\! [c, d]}(D_{1,2}f(x,y) - D_{2,1}f(x,y))dA\]
\[=\int_{a}^{b}\left(\int_{c}^{d}D_{1,2}f(x,y)dy\right)dx - \int_{a}^{b}\left(\int_{c}^{d}D_{2,1}f(x,y)dx\right)dy\]
\[= \int_{a}^{b}\left(D_{1}f(x,d) - D_{1}f(x,c)\right)dx - \int_{c}^{d}\left(D_{2}f(b,y) - D_{2}f(a,y)\right)dy\]
\[= (f(b, d)-  f(a, d) - f(b, c) + f(a, c)) - (f(b, d)-  f(b, c) - f(a, d) + f(a, c)) = 0\]
using the fundamental theorem of calculus 6 times. This is a contradiction, so the
mixed partial derivatives are equal on the rectangle.
\end{proof}
\end{theorem}

\begin{theorem}\label{maxmin}
$A \subset \RR$ If the max or min of $f:A \rightarrow \RR$ occur at a point $a$ in the interior of $A$ and $D_{i}f(x)$ exists then $Df(a) = 0$
\begin{proof}
Consider $h(x) = f(a^{1}, \dots , a^{i-1} , x^{i}, a^{i+1}, \dots a^{n})$ $x$ in an open interval arround $a^{i}$.\\ Since $f$ has a max or min at $a$, $h$ has a max or min at $a^{i}$
\[\frac{dh}{dx}(a^{i}) = D_{i}f(a)\]
By analysis 2:
\[\frac{dh}{dx}(a^{i}) =0 \Rightarrow Df(a) = 0\]
\end{proof}
\end{theorem}

\begin{note}
The converse of Theorem \ref{maxmin} is not true, even in one dimension.
\end{note}



\subsection{Jacobian}
For $f:\RR^{n} \rightarrow \RR^{m}$ with total derivitive $Df(a):\RR^{n} \rightarrow \RR^{m}$ a linear map. Then  the Jacobian $f^{'}(a) \in \MM_{mxn}$ is the  unique representation of $Df(a)$ in the standard basis.

\begin{theorem}\label{jacob}
If  $f:\RR^{n} \rightarrow \RR^{m}$ is differentiable at a then $D_{j}f^{i}(a)$ exists $\forall i = 1, \dots , m\; \forall j = 1, \dots , n$ and the jacobian matrix is \[f^{'}(a) = (D_{j}f^{i}(a))_{j = 1, \dots , n}^{i=1,\dots , m}\]
\[ f^{'}(a) = \begin{pmatrix}
  D_{1}f^{1}(a) & D_{2}f^{1}(a) & \cdots &D_{n}f^{1}(a) \\
  D_{1}f^{2}(a) & D_{2}f^{2}(a) & \cdots & D_{n}f^{2}(a) \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  D_{1}f^{m}(a) & D_{2}f^{m}(a) & \cdots & D_{n}f^{m}(a)
 \end{pmatrix}\]
Where $f(x)= (f^{1}(x) , \dots , f^{m}(x)), \; f^{i}:\RR^{n} \rightarrow \RR$
\begin{proof}
Case $m=1$
\[
\begin{diagram}
\node{\RR} \arrow{e,t}{h}  \arrow{se,b}{f \circ h}
\node{\RR^n}  \arrow{s,r}{f} \\
 \node[2]{\RR}
\end{diagram}\]
\[ h(t) = (a^1 , \dots , a^{i-1}, t , a^{i+1}, \dots , a^{n}) \qquad \left.\frac{d(f \circ h)}{dt}\right| _{t=a^{i}} = D_{i}f(a)\]
\[\lim_{t \rightarrow a^{i}}\frac{(f\circ h)(t) - (f\circ h)(a^i)}{t-a^i} = \lim_{t \rightarrow a^{i}}\frac{f(a^1 , \dots , a^{i-1}, t, a^{i+1}, \dots , a^n) - f(a^1 , \dots , a^n)}{t-a^i} \]
$h$ is differentiable because its components are differentiable ie component $h^i$ is either constant $a^j$ where $j \neq i$ or $t$ when $j = i$
\begin{align*}
Dh(t)&=(Dh^{1}(t), \dots , Dh^{n}(t))\\
&=(0, \dots , 1, \dots , 0)
\end{align*}
\[ h^{'}(a^i) = \begin{pmatrix}
  0 \\
 0\\
  \vdots  \\
 1\\
\vdots  \\
0
 \end{pmatrix}_{(m \times 1)}\]

Case $m>1 $\\
$f:\RR^n \rightarrow \RR^m $
\[f(x) = (f^{1} (x), \dots f^{m}(x))\]
\[Df(a) = (Df^{1} (a), \dots Df^{a})\]
\[ f^{'}(a) = \begin{pmatrix}
  (f^1)^{'}(a) \\
  \vdots\\
(f^m)^{'}(a)\\
 \end{pmatrix}_{(m \times n)}\]
\[ f^{'}(a) = \begin{pmatrix}
  D_{1}f^{1}(a) & D_{2}f^{1}(a) & \cdots &D_{n}f^{1}(a) \\
  D_{1}f^{2}(a) & D_{2}f^{2}(a) & \cdots & D_{n}f^{2}(a) \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  D_{1}f^{m}(a) & D_{2}f^{m}(a) & \cdots & D_{n}f^{m}(a)
 \end{pmatrix}\]
\end{proof}
\end{theorem}

\begin{remark}
Abuse of notation since if $f:\RR \rightarrow \RR$
\[\text{this is a number} \rightarrow  \frac{dg(t_{0})}{dt}| = g'(t_{0}) \leftarrow \text{this is the $1 \times 1$ jacobian matrix}\]
\end{remark}

\begin{example}
\[ G(x,y) =
  \begin{cases}
   \frac{x^{2}y}{x^{4} + y^{2}} & \text{if } (x,y) \neq (0,0) \\
   0       & \text{if } (x,y) = (0,0)
  \end{cases}\]

Fix a vector $u \in \RR^2$ , $u=(u^1 , u^2 ) \neq (0,0), \quad u^2 \neq 0$ then the directional derivitive $D_{u}$ with $h \in \RR$ is:

\begin{align*}
D_{u}G(0,0) &= \lim_{h \rightarrow 0}\frac{G((0,0) + hu) - G(0,0)}{h} =  \lim_{h \rightarrow 0}\frac{G(hu^1 , hu^2 ) - 0}{h}\\
&= \lim_{h \rightarrow 0}\frac{(hu^1)^2(hu^2)}{(hu^1)^4 + (hu^2)^2} \cdot \frac{1}{h} =   \lim_{h \rightarrow 0}\frac{h^3 (u^1)^2 u^2}{h(h^4(u^1)^4 + h^2(u^2)^2)}\\
&=  \lim_{h \rightarrow 0}\frac{ (u^1)^2 u^2}{h^2(u^1)^4 + (u^2)^2} = \frac{(u^1)^2}{u^2}\\
\shortintertext{$u^2=0$}
D_{u}G(0,0) &=  \lim_{h \rightarrow 0}\frac{G(hu^1 , h \! \cdot \!0)}{h}=  \lim_{h \rightarrow 0}\frac{(\frac{(hu^1)^2 \! \cdot \! 0}{(hu^1)^4 + 0^2})}{h} = 0
\end{align*}




\end{example}

\begin{theorem}
 $f:\RR^n \rightarrow \RR^m $ if $D_{j}f^{i}(x)$ exist $ \forall x \in u, U\; open,  a \in U,  \forall i = 1,\dots, m,$ and $ j = 1, \dots, n$ and if $D_{j}f^{i}(x)$ continuous at $a$ ie 
\[\lim_{x \rightarrow a}(D_{j}f^{i}(x)) = D_{j}f^{i}(a)\]
then $Df(a)$ exists and $f$ is differentiable at $a$
\begin{proof}
As in the proof of theorem \ref{jacob} It suffices to consider the case $ m=1$, so that $f:\RR^n \rightarrow \RR$. Then
\begin{align*}
f(a+ h) - f(a) &= f(a^{1} + h^{1}, a^{2}, \dots , a^{n}) &&- f(a^{1}, \dots , a^{n})\\
&+ f(a^1 + h^1 , a^2 + h^2, a^3 , \dots , a^n ) &&-  f(a^{1} + h^1 , a^2 ,  \dots , a^{n})\\
&+ \dots  &&- \dots\\
&+ f(a^1 + h^1 ,\dots ,  a^n + h^n ) &&-  f(a^{1} + h^1 ,  \dots, a^{n-1} + h^{n-1} , a^{n})
\end{align*}
Recal from theorem \ref{jacob} that $D_{1}f$ is the derivitive of the function $h$ defined by\\ $h(x) = (x, a^2 , \dots , a^n )$. Applying the mean-value theorem to $h$ we obtain
\[ f(a^{1} + h^{1}, a^{2}, \dots , a^{n}) - f(a^{1}, \dots , a^{n}) = h^{1} \cdot D_{1}f(b_{1}, a^{2} , \dots, a^{n}) \]
for some $b_{1}$ between $a^1 $ and $a^1 + h^1 $. Similarly the $ith$ term in the sum equals 
\[h^{i} \cdot D_{i}f(a^1 + h^1 ,\dots ,  a^{i-1} + h^{i-1},  b_{i}, a^{i+1}, \dots , a^n ) = h^{i}D_{i}f(c_{i}) \quad \text{for some $c_{i}$} \]
Then
\begin{align*}
\lim_{h \rightarrow 0}\frac{|f(a+h) - f(a) - \sum_{i=1}^{n}D_{i}f(a) \cdot h^{i}|}{|h|} &= \lim_{h \rightarrow 0}\frac{| \sum_{i=1}^{n}[D_{i}f(c_{i}) - D_{i}f(a) ]\cdot h^{i}|}{|h|}\\
 & \leq \lim_{h \rightarrow 0}| \sum_{i=1}^{n}[D_{i}f(c_{i}) - D_{i}f(a)]|\cdot \frac{| h^{i}|}{|h|}\\
 & \leq \lim_{h \rightarrow 0}| \sum_{i=1}^{n}[D_{i}f(c_{i}) - D_{i}f(a)]|\\
&=0
\end{align*}
Since $D_{i}f$ is continuous at $a$ and as $h \rightarrow 0, c^{i} \rightarrow  a^{i}$.
\end{proof}
\end{theorem}

\begin{definition} If $f:\RR^n \rightarrow \RR^m$ has partial derivitives $ D_{j}f^{i} \quad \forall x \in U , \; U \; open, \;  a \in U$ and $D_{j}f^{i}$ is continuous at $a$ then we say $f$ is continuously differentiable at $a$. 
\end{definition}

\begin{example}
$f:\RR^2 \rightarrow \RR$, with $x:\RR \rightarrow \RR, y:\RR \rightarrow \RR$. 
\[\text{Define} \quad g:\RR \rightarrow \RR \quad g(t) = f(x(t),y(t))\]
\begin{align*}
\frac{dg(t_0)}{dt} &= (g^{'}(t_0)) = f^{'}(x(t_0), y(t_0)) \cdot \left(
    \begin{array}{c}
      x^{'}(t_0) \\
     y^{'}(t_0)
    \end{array}
  \right)\\
&= \frac{df}{dx}(x(t_0),y(t_0)) \cdot  \frac{dx}{dt}(t_0) + \frac{df}{dy}(x(t_0),y(t_0)) \cdot  \frac{dy}{dt}(t_0) \\
&= \frac{df}{dx}\cdot \frac{dx}{dt} + \frac{df}{dy}\cdot \frac{dy}{dt}
\end{align*}
\end{example}


\subsection{Inverse Function Theorem}

\begin{lemma}
Let $A \subset \RR^n$ be a rectangle with interior $A^{0}$ and let $g : A \rightarrow\RR^n$ be continuously differentiable. If there exist a constant $M > 0$ such that 
\[ |D_{j}g^{i}(x)| \leq M, \quad x \in A^{0}, \quad i,j=1, \dots , n. \]
then
\[ |g(x) - g(y)| \leq n^{2}M|x-y|, \quad x,y \in A.\]
\begin{proof}
 Fix $ i = 1, \dots , n.$ Then
\begin{align*}
g^{i}(y) - g^{i}(x) &= g^{i}(y^1 , y^2 , \dots , y^n) - g^{i}(x^1 , x^ 2 , \dots , x^n )\\
&= g^{i}(y^1 , y^2 , \dots , y^n) - g^{i}(x^1 , y^2 , \dots , y^n)  +  g^{i}(x^1 , y^2 , \dots , y^n)  - g^{i}(x^1 , x^ 2 , \dots , y^n )\\ 
& \quad +  g^{i}(x^1 , x^ 2 , \dots , y^n ) - \dots +  g^{i}(x^1 , x^ 2 , \dots , y^n ) -  g^{i}(x^1 , x^ 2 , \dots , x^n )\\ &= \sum^{n}_{j=1}(g^{i}(x^1 , x^ 2 , \dots , x^{j-1} , y^{j}, \dots , y^n ) - g^{i}(x^1 , x^ 2 , \dots , x^{j-1} , x^{j}, y ^{j+1}, \dots , y^n ) \\
&= \sum^{n}_{j=1}(y^{j} \! -\! x^{j})D_{j}g^{i}(z^{i}_{j})
\end{align*}
where $z^{i}_{j}$ is between $y^j$ and $x^j$, and we used the mean-value theorem in the interval between
$y_j$ and $x_j$ and in the $j$ variable. Using the triangle inequality and $|z^j| \leq |z|, $ we get
\[ |g^{i}(y) - g^{i}(x)| \leq \sum^{n}_{j=1}|y^{i} - x^{i}|M \leq \sum^{n}_{j=1}|y - x|M = nM|y-x|.\]
Since $|z| \leq \sum_{i}|z^i |, $ finally we get
 \[ |g(x) - g(y)| \leq \sum^{n}_{i=1}|g^{i}(y) - g^{i}(x)| \leq  \sum^{n}_{i=1} nM|y-x| =  n^{2}M|y-x|. \]
\end{proof}
\end{lemma}
\begin{remark}
 It is clear that the dimension of the target space enters only in the last line of
the calculation. If $g : \RR^n \rightarrow \RR^m$, then we get as upper bound $nmM|x- y|$. The inequality
is actually not optimal: one can use the Cauchy-Schwarz inequality twice to get a bound
$n^{1/2}m^{1/2}M|x-y|$ for $g:\RR^n \rightarrow \RR^m$.
\end{remark}

\setcounter{equation}{0}

\begin{theorem}[Inverse Function Theorem]
Theorem Let $f : \RR^n \rightarrow \RR^n$ be continuously differentiable on an open set containing $a$ and assume $detf^{'}(a) \neq 0$. Then there exists an open set $V$ containing $a$ and an open set $W$ containing $f(a)$ such that $f : V \rightarrow W$ is bijective with $f^{-1}: W \rightarrow V$ continuously differentiable and which satisfies:
\[ (f^{-1})^{'}(y) = [f^{'}(f^{-1}(y))]^{-1}, \quad y \in W.\]

\begin{proof}
\textit{Step 1:}\\ We reduce proving the theorem to the case where actually $f^{'}(a) = I_{nn}$. Call $ \lambda = Df(a)$. This is a linear transformation with nonsingular matrix representation $f^{'}(a)$, as det$f^{'}(a) \neq 0$. Therefore, $\lambda$ is invertible. The inverse $\lambda^{-1}$ is also a linear transformation, so $D(\lambda^{-1})(y) = \lambda^{-1}$ for $y \in \RR^n$. Both $\lambda$ and its inverse are continuous as linear transformations. Consider the function $ h = \lambda^{-1} \circ f$ defined on an open set comtaining a. \\ Then:
\[Dh(a) = D \lambda^{-1} (f(a)) \circ Df(a) = \lambda ^{-1} \circ Df(a) = \lambda^{-1} \circ \lambda = Id, \]

by using the chain rule. Here $ Id$ is the identity transformation. This gives $ h^{'}(a) = I_{n \times n}$, which has determinant $1\neq0$. Let $A$ be the matrix representation of $ \lambda^{-1}$. (which gives that $A^{-1}$ is the matrix representation of $\lambda= D\lambda$). This is an $ n \times n$ matrix with constant entries, i.e. not depending on $y$. Moreover, h is continuously differentiable, as
\[ (D_{j}h^{i}(x)) = h^{'}(x) = [ \lambda^{-1} \circ Df(x)] = A \cdot f^{'}(x) = A(D_{j}f^{i}(x)), \]


with entries depending continuously on x. Therefore, $h$ satisfies the conditions of the
inverse function theorem. Suppose that we can prove the conclusion of it for $h$, i.e. that
there exists an open set $V$ containing $a$ and $\tilde{W}$ open containing $h(a) = \lambda^{-1}(f(a))$ such that
$h : V \rightarrow \tilde{W}$ is bijective with continuously differentiable inverse $h^{-1}$. Even more, assume
that we have prove the formula for the derivative of the inverse of $h$:
\[ (h^{-1})' (z) = [h'(h^{-1}(z))]^{-1}. \]

Define $W =\lambda(\tilde{W}) =(\lambda^{-1})^{-1}(\tilde{W})$. This is the inverse image of $\tilde{W}$ by $\lambda^{-1}$, which is continuous, so it is an open set. Since $\lambda$ is bijective, $f= \lambda \circ h$ is bijective on $V$ with image $\lambda(\tilde{W}) = W$. Moreover, 
\[f^{-1} = h^{-1} \circ \lambda^{-1}, \]
which is continuously differentiable as the composition of two such maps. By the chain
rule for Jacobian
\[(f^{-1})'(y) = (h^{-1})'(\lambda^{-1}(y)) \cdot (\lambda^{-1})'(y) =  [h'(h^{-1}(\lambda^{-1}(y)))]^{-1} A = [h'((\lambda \circ h)^{-1}(y))]^{-1}  A = [h'(f^{-1}(y))]^{-1}A \]
\[ = [A^{-1}h'(f^{-1}(y))]^{-1} = [\lambda'h'(f^{-1}(y))]^{-1} = [(\lambda \circ h)'(f^{-1}(y))]^{-1} = [f'(f^{-1}(y))]^{-1}. \]

All these imply that it is enough to work with $h = \lambda ^{-1} \circ f$. The main property we will
use is that $h'(a)=I_{n \times n}$. For simplicity in our notation we call this function f so we can
assume that
\[f'(a) = I_{n \times n}. \]
This also means that $\lambda = Df(a) = Id$.\\
\textit{Step 2:}
The function f cannot take the value $f(a)$ arbitrarily close to $a$. Suppose that there is a sequence $h_{n} \in \RR^n$ such that $ h_{n} \rightarrow 0$ and $f(a + h_{n}) = f(a)$. We plug the sequence into the definition of the derivative at $a$ and use that $Df(a) = Id$ to get
\[ 0 = \lim_{h_{n} \rightarrow 0}\frac{|f(a + h_{n}) - f(a) - Df(a)(h_{n})|}{|h_{n}|} = \lim_{h_{n} \rightarrow 0}\frac{|-h_{n}|}{|h_{n}|} = 1\]
So this is a contradiction. Therefore, we can find a closed rectangle $U$ containing a such
that
\[f(x) \neq f(a), \quad \forall x \in U\backslash \{a\}. \]


\textit{Step 3:} The determinant is a polynomial expression in the entries of a matrix. If the matrix entries depend continuously on $x$, the same is true for the determinant of the matrix. So det$f'(x)$ is a continuous function on an open set containing $a$. Since det$f'(x) \neq 0$, by the inertia principle, there exists a small enough (rectangular) neighbourhood of $a$, which
we call $U$ again, such that
\begin{equation}
 \text{det}f'(x) \neq 0, \quad x \in U \end{equation} 
Moreover the partial derivatives $D_{j}f^{i}(x)$ are continuous and $D_{j}f^{i}(a) = \delta_{ij},$ as $Df(a) = Id$. So, for $x$ close enough to $a$ we have
\begin{equation}
|D_{j}f^{i}(x) - \delta_{ij}| < \frac{1}{2n^{2}}, \quad i,j = 1, \dots , n, \quad x \in U
\end{equation}
We assumed again that the neighbourhood is $U$\\
\textit{Step 4:}  Constructing a contraction map and showing that $f$ is injective in appropriate small meighbourhood. Now we define the function
\[ g(x) = f(x) - x\]
and apply the Lemma to this function for the closed rectangle $U$. We notice that $D_{j}g^{i}(x) =D_{j}f^{i}(x) - \delta_{ij}$, as we know the partial derivatives of the identity function $x$. We deduce that
\begin{equation}
|g(x_{1}) - g(x_{2})| \leq n^{2}\frac{1}{2n^2}|x_{1} - x_{2}| = \frac{1}{2}|x_{1} - x_{2}| \end{equation}

The choice of the neighbourhood in (2) so that the constant $1/(2n^{2})$ appears on the right is motivated with the desire to get g as a contraction map (with constant $1/2$) as we see in (3). Now the triangle inequality in the form $|a|-|b|\leq  |a-  b|$ gives
\[|x_{1} - x_{2}| - |f(x_{1}) - f(x_{2})| \leq |(x_{1} - x_{2}) - (f(x_{1}) - f(x_{2}))| = |-g(x_{1}) + g(x_2)| < \frac{1}{2}|x_{1} - x_{2}| \]
\begin{equation}
\Rightarrow |x_{1} - x_{2}| - \frac{1}{2}|x_{1} - x_{2}| < |f(x_{1}) - f(x_{2})| \Rightarrow  \frac{1}{2}|x_{1} - x_{2}|  <|f(x_{1}) - f(x_{2})|
\end{equation}

Here $x_1, x_2$ are in $U$. We immediately see that on $U$ the function $f$ is injective:
\[f(x_1)=f(x_2) \Rightarrow |x_1 - x_2| =0 \Rightarrow x_1 = x_2.\]

We still have not determined the neighbourhoods $W$ of $f(a)$ and $V$ of $a$.\\
\textit{Step 5:}  Determination of the minimum distance of $f(a)$ to the image of the boundary of $U$ and definition of $W$.\\ We have assumed that on the closed rectangle $ U$ we have $f(x) \neq f(a)$ for $x \neq a$. This is definitely true on the boundary of $U$, denoted $\partial U$, which is a closed and bounded set, i.e. compact. The function $ m(x) = |f(x)-  f(a)|$ is continuous on a neighbourhood of $\partial U$ and nonzero on it. It achieves a minimum value on $\partial U$ (an advanced argument from Real Analysis is that the image of a compact set is compact, so that $m(\partial U)$ is compact, which
means closed and bounded. Such a set has a maximum and minimum). The minimum value cannot be zero, say
\[ \min_{x \in \partial U} m(x) = \min_{x \in \partial U} |f(x) - f(a)| > 0 .\]
Now define 
\[W = \{y \in \RR^n , |y-f(a)| < \delta / 2\}.\]
\textit{Step 6:}  Comparison of $|y - f(x)|$ with $|y-  f(a)|$ for $x \in \partial U$, and $y \in W$. We have 
\[ |f(x)- f(a)| \geq \delta, \quad |y - f(a)| \leq \delta /2 \Rightarrow -|y-f(x)| + \delta \leq -|y-f(x)|+|f(x) - f(a)| \leq |y-f(a)| < \delta /2 \]
\[ \Rightarrow \delta /2 = \delta - \delta /2 < |y-f(x)| \Rightarrow |y-f(a)|< \delta / 2 < |y-f(x)|. \]
\textit{Step 7:} Show that for $y_0 \in W$ there exists a unique $x_0 \in U^{0}$ such that $f(x_0) = y_0$. The
uniqueness is obvious from the fact that $f$ is injective on $U$. The construction of such an $x_0$ is tricky. We define another function on $U$ by
\[g(x) = |f(x)-y_0 |^2 = \sum^{n}_{i=1}(f^{i}(x) - y^{i}_{0})^2.\]

This function in continuously differentiable, as it is a sum of the squares of the components.
On the compact set $U$ the function $g$ achieves its minimum, say at $x_0$, i.e. $g(x_0) \leq g(x)$ for
$x \in U$. We claim that $x_0$ is the desired point with $f(x_0) = y_0$. First we see that $x_0$ is in the
interior of the set $U$. On the boundary of $U$ the function $g(x)$ has values $> \delta /2$, by Step 6,
while $g(a) < \delta /2$. So the minimum is not achieved on the boundary of U. Therefore, it is
achieved in an interior point. This point has to be a critical point of $g$, i.e. $D_j g(x_0) = 0,
\; j = 1, \dots , n$. We calculate them to be
\[ 2 \sum^{n}_{i=1}(f^i (x_0) - y^{i}_{0})D_{j}f^{i}(x_0) = 0, \quad j=1, \dots , n.\]

This is a homogeneous system of linear equations with unknowns $f^{i}(x_0) - y^{i}_{0}$ and coefficients $D_j f^{i}(x_0)$. The determinant of the coefficients of the system is nonzero, as $x_0 \in U$. The system has a unique solution, and this solution is the zero vector, i.e
\[ 0 = f^{i}(x_0) - y^{i}_{0}, \quad i=1, \dots, n \Rightarrow f(x_0) = y_{0}. \]

\textit{Step 8:} We define $V$ and Show that $f:V \rightarrow W$ is bijective and continuous. We define $V = U^0 \cap f^{-1} (W)$. Clearly $f : V \rightarrow W$ is bijective. Moreover, $V$ is open as the intersection of the open set $U^0$
and the open set $f^{-1} (W)$, which is open as the inverse image of an open set W by the continuous function $f$. We now rewrite (4) as
\begin{equation}
|x_1 - x_2 | < 2| f(x_1) - f(x_2)| \Leftrightarrow |f^{-1}(y_1)- f^{-1}(y_2)| < 2|y_1 - y_2 |
\end{equation}
with $y_1 = f(x_1)$ and $y_2 = f(x_2),\; y_i \in W$. This shows that $f^{-1}$ is a Lipschitz function with constant 2, so that it is continuous. Alternatively, choose $\delta = \epsilon /2$ in the definition of continuity.\\

\textit{Step 9:}  Show that $f^{-1}$ is differentiable. Let $\mu = Df(x_1)$. Since $f^{-1} \circ f = Id$, the chain
rule gives the only possible choice for $Df^{-1}(y_1)= \mu^{-1}$. Here $f(x_1) = y_1$ and later $f(x) = y$.
By the definition of the derivative we have
\[ f(x) - f(x_1) = \mu (x-x_1) + \phi(x-x_1), \quad \lim_{x \rightarrow x_1}\frac{|\phi(x - x_1)|}{|x - x_1|} = 0.\]

We apply to the equation the linear transformation $\mu^{-1}$ to get
\[ \mu^{-1}(y-y_1) = x - x_1 + \mu^{-1}(\phi(x - x_1)) \Rightarrow x - x_1 -\mu^{-1}(y-y_1) = \mu^{-1}(\phi(x - x_1)) \]
\[ \Rightarrow f^{-1}(y) - f^{-1}(y_1) - \mu^{-1}(y - y_1) = -\mu^{-1}(\phi(x-x_1)). \]

By the definition of the derivative of $f^{-1}$ at $y_1$ we need to show that
\begin{equation}
\lim_{y \rightarrow Y_1}\frac{|-\mu^{-1}(\phi(x-x_1))|}{|y-y_1|}=0
\end{equation}
Since $\mu^{-1}$ is a linear transformation, we have seen that it is a bounded linear operator, i.e.
there exists a constant $\tilde{M}$ with
\[|\mu^{-1}(y)| \leq \tilde{M}|y|, \quad \forall y \in \RR^n . \]
Since
\[ \frac{|-\mu^{-1}(\phi(x-x_1))|}{|y-y_1|} \leq \frac{\tilde{M}|\phi(x-x_1)|}{|y-y_1|} \]
by the sandwich theorem it is enough to prove that
\[\lim_{y \rightarrow Y_1}\frac{|\phi(x-x_1)|}{|y-y_1|} =0\]
We have 
\[\frac{|\phi(x-x_1)|}{|y-y_1|} = \frac{|\phi(x-x_1)|}{|x-x_1|}\frac{|x-x_1|}{|y-y_1|} \leq \frac{|\phi(x-x_1)|}{|x-x_1|}\cdot 2,\]
by (5). Moreover, $y \rightarrow y_1$ iff $ x \rightarrow x_1$ as $ f$ is continuous at $x_1$ and $f^{-1}$ is continuous at $y_1$.We know that
\[\lim_{x \rightarrow x_1} \frac{|(\phi(x-x_1))|}{|x-x_1|} = 0 \]
This suffices to prove (6)\\
\textit{Step 10:} The partial derivatives $D_j (f^{-1})^i(y)$ are continuous. We know that the Jacobian of $f^{-1}(y)$ is
\[(f^{-1})'(y) = (D_j (f^{-1})^i(y)) = [f'(f^{-1}(y))]^{-1} = (D_j f^i (x))^{-1}. \] 
The inverse of the matrix $(D_jf^i(x))$ can be calculated as a quotient of two $n \times n$ determinants with entries among $D_jf^i(x)$. The denominator is the determinant of the Jacobian at $x$, which is nonzero for $x \in U$. The whole expression depends continuously on $x \in V$. As $f^{-1}$ is continuous, the inverse matrix depends continuously on $y \in W$. The individual entries are the partial derivatives of $f^{-1}$.
\end{proof}
\end{theorem}

\begin{example}
$f:\RR^2 \rightarrow \RR^2$
\[(z,w) = f(x,y) = (xy, x^2 + y^2), \quad z=xy, \; w=x^2 +y^2\]
\[f'(x,y) = \begin{pmatrix} y &x \\ 2x & 2y \end{pmatrix}\]
\[det\,f'(x,y) = 2y^2 - 2x^2 = 2(y+x)(y-x)\]
\[det\,f'(x,y) \neq 0 \Leftrightarrow x \neq\pm y \]
Solving:
\[y=\frac{z}{x}\]
\[\therefore \; w = x^2 + \frac{z^2}{x^2}\]
\[\therefore \; wx^2 = x^4 + z^2\]
\[\Rightarrow   x^4 - wx^2 + z^2 = 0\]
Let $t=x^2$ $\therefore \; t^2 - wt +z^2$
So
\[t = \frac{w \pm \sqrt{w^2 - 4z^2}}{2}\]
\[x= \pm \sqrt{\frac{w \pm \sqrt{w^2 - 4z^2}}{2}}\]
And \[y= \frac{z}{\pm \sqrt{\frac{w \pm \sqrt{w^2 - 4z^2}}{2}}}\]
You should be able to differentiate if $w^2 - 4z^2 \neq 0 \; \Leftrightarrow if \; y \neq \pm x$
\[\begin{bmatrix} \tfrac{dx}{dz} &\tfrac{dx}{dw} \\ \tfrac{dy}{dz} & \tfrac{dy}{dw} \end{bmatrix} = (f^{-1})'(z,w) = \begin{pmatrix} y &x \\ 2x & 2y \end{pmatrix}^{-1} = \frac{1}{2(y^2 -x^2)}\begin{pmatrix} 2y &-x \\ -2x & y \end{pmatrix}\]
\[\Rightarrow \frac{dx}{dz} = \frac{2y}{2(y^2 -x^2)}\]
\[\Rightarrow \frac{dx}{dw} = \frac{-x}{2(y^2 -x^2)}\]
When we have $\quad z=xy, \; w=x^2 +y^2$ along the lines $ y = \pm x$ the circle meets the hyperbola tangentially so we cannot invert.
\end{example}

\subsection{Implicit Function Theorem}
\dgARROWLENGTH=1em
\begin{example}
\[x^2 + y^2 = 1, \quad y = g(x)\] 
\[ 2x + 2y\frac{dy}{dx} = 0, \quad \frac{dy}{dx} = \frac{dg}{dx} = \frac{-x}{y}, \quad y \neq 0 \]
\end{example}
\begin{example}
\[y^2 + xz + z^2 - e^z - 4 = 0 \quad \text{(impossible to solve for z)}\]
\[set \; F(x,y,z) = y^2 + xz + z^2 - e^z - 4, \quad F(x,y, g(x,y)) = 0\]
\[
\begin{diagram}
\node[2]{F}\arrow{sw,-}\arrow{se,-}\arrow{s,-}\\
\node{X} \arrow{s,-} 
\node{Y} \arrow{s,-}
\node{Z}  \arrow{wsw,-}\arrow{sw,-} \\
\node{X} \arrow{e,-} \node{Y}
\end{diagram}
\]
Differentiate in $x$:
\begin{align*}
\frac{d}{dx}F(x,y,g(x,y)) &= \frac{dF}{dx}\frac{dx}{dx} + \frac{dF}{dy}\frac{dy}{dx} + \frac{dF}{dz}\frac{dz}{dx}\\
&=  \frac{dF}{dx} + \frac{dF}{dz}\frac{dg}{dx}  = 0\\
\frac{dg}{dx} &= -\frac{\frac{dF}{dx}}{\frac{dF}{dz}} = - \frac{z}{x^2 +2z - e^z }\\
\frac{dF}{dy}=0 \overset{chain}{\underset{rule}{\Rightarrow}} \frac{dF}{dy} + \frac{dF}{dz}\frac{dg}{dy} \Rightarrow 
\frac{dg}{dy} &= -\frac{\frac{dF}{dy}}{\frac{dF}{dz}} = - \frac{2y}{x^2 +2z - e^z }\\
\end{align*}
the point $(0,e,2)$ satisfies $F(x,y,z)=0$
\[e^2 + 0\cdot 2 + 2^2 - e^2 -4 =0\]
\begin{align*}
\left.\frac{dg}{dx}\right|_{(0,e)} &=- \frac{z}{x^2 +2z - e^z } = - \frac{2}{0+2\cdot 2- e^2 }\\
\left.\frac{dg}{dy}\right|_{(0,e)} &=- \frac{2y}{x^2 +2z - e^z }  = - \frac{2e}{0+2\cdot 2- e^2 }
\end{align*}
valid for $\frac{dF}{dz} \neq 0$
\end{example}

\textbf{General situation:} $m$ equations with $ m$ unknowns $y^1, \dots , y^m$
\begin{alignat*}{2}
f^1 &(x^1 , \dots , x^n , y^1 , \dots , y^m) = 0 &\qquad & \text{depends on n parameters:}\; x^1 , \dots , x^n\\
f^2 &(x^1 , \dots , x^n , y^1 , \dots , y^m)= 0 &\qquad & \text{Try to solve for:} \;  y^1 , \dots , y^m\\
&\vdots \qquad \qquad \vdots \qquad \qquad \vdots 			&\quad & \\
f^m &(x^1 , \dots , x^n , y^1 , \dots , y^m)= 0 &\quad &\\
\end{alignat*}
\[ x=(x^1 , \dots , x^n) , \qquad y=(y^1 , \dots , y^m)\]
So we have:
\begin{align*}
f^1(x,y) &=0\\
f^2(x,y) &=0\\
\vdots\\
f^m(x,y) &=0\\
\end{align*}
Define $f(x,y) = (f^1(x,y), \dots , f^m (x,y))=\underbracket[0.5pt]{0}_{vector} =\underbrace{(0,\dots,0)}_{
          \mathclap{m}}$\\
Let $a \in \RR^n , \; b \in \RR^m$ such that $f(a,b) = 0$ when we can find for each $(x^1,  \dots, x^n) $  near $a=(a^1 , \dots , a^n )$ a unique $y=(y^1 , \dots , y^m )$ near $b= (b^1 , \dots , b^m)$ such that: $f(x,y)=0$, \\$f(x^1 , \dots , x^n , y^1 , \dots , y^m) = 0$

\begin{theorem}[Implicit Function Theorem]\label{implicitFunction}
$f:\RR^n \times \RR^m \rightarrow \RR^m$ continuously differentiable on an open set containing $(a,b), \; a \in \RR^n , \; b \in \RR^m.$ moreover $f(a,b)=0$\\
consider the matrix \[M=(D_{j+n}f^{i}(a,b))^{i=1,\dots,m}_{j=1,\dot,m}\] assume det$M \neq 0$. Then there exist two open sets $A \subset \RR^n, \; b \subset \RR^m, \; a \in A, \; b \in B$. such that $\forall x \in A, \exists$ unique $g(x) \in B$ such that $f(x,g(x))=0$ Moreover $g:A \rightarrow B$ is differentiable.
\begin{proof} 
Increase the dimension of the target. Define $F:\underbrace{U}_{\mathclap{\in \;  \RR^n \times \RR^m}} \rightarrow \RR^n \times \RR^m$
\[F(x^1,\dots , x^n,y^1, \dots,y^m) = (x^1, \dots , x^n, f^1(x,y), \dots , f^m(x,y))\]
\[F(x,y) = (x,f(x,y))\]
F is continuously differentiable because $x^1 , \dots , x^n$ are continuously differentiable and $f^1(x,y), \dots , f^m(x,y)$ are continuously differentiable (because $f(x,y)$ is continuously differentiable)
\[F(a,b) = (a,f(a,b)) = (a,0)\]
\[
 F'(a,b) =
 \left( \begin{array}{cccc|ccc}
 1 & 0 & \cdots & 0 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 & 0 & \cdots & 0 \\
  \vdots  & \vdots  & \ddots & \vdots & \vdots & & \vdots \\
0 & 0 & \cdots & 1 & 0 & \cdots & 0 \\
\hline
  \frac{df^1}{dx^1} & \frac{df^1}{dx^2}& \cdots\frac{df^1}{dx^n} & \frac{df^1}{dy^1} & \cdots & \frac{df^1}{dy^m}\\
\vdots  & \vdots  & \ddots & \vdots & \vdots & & \vdots \\
  \frac{df^m}{dx^1} & \frac{df^m}{dx^2}& \cdots\frac{df^m}{dx^n} & \frac{df^m}{dy^1} & \cdots & \frac{df^m}{dy^m}
 \end{array} \right)
\]
\[ F'(a,b) =
 \left( \begin{array}{c|c}
 I_{n \times n} & 0_{n \times m}\\
\hline
\text{\scriptsize * Some $m \times n$} & M_{m \times m}\\
\text{\scriptsize matrix} & 
 \end{array} \right) \]

det$M \neq 0$ (reducing from top left entry).\\
By the inverse function theorem, $\exists$ an open set $W$ containing $F(a,b) = (a,0)$ and an open set containing $(a,0)$ which I can take to be a rectangle $A \times B, \;  a \in A , \; b \in B, \; A$ open in $\RR^n, \; B$ open in $\RR^m$.\\
\[F:A \times B \rightarrow W \text{ is bijective}\]
\[\exists h = F^{-1}:W \rightarrow A \times B \; \text{such that} \; F \cdot h = id\]
$h$ is continuously differentiable.
\begin{equation*} 
  \addtolength{\fboxsep}{5pt} 
   \boxed{ 
   \begin{gathered} 
      F(x^1,\dots , x^n,y^1, \dots,y^m) = (x^1, \dots , x^n, f^1(x,y), \dots , f^m(x,y))  \\ 
      F(x,y) = (x,f(x,y)) 
      \\ F \;\text{is continuously differentiable because} \; x^1 , \dots x^n \; \text{are continuously differentiable}
   \end{gathered} 
   } 
\end{equation*}
$h$ must have the form: $h(x,y) =  (x,k(x,y))$ for some function $k:W \rightarrow B, \; B \subset \RR^m$, $k$ continuously differentiable.
\[F(h(x,y) = (x,f(x,k(x,y))) = (x,y)\]
\[f(x,k(x,y)) = y\]
Set $y=0$
\[f(x,k(x,0))=0\]
The solution is $g(x)=k(x,0)$ (solution to $f(x,y)= 0$).
\end{proof}
\end{theorem}

\begin{theorem}\label{imp2}
Let $g : \RR^n \rightarrow \RR^p$ be a continuously differentiable function in an open set containing $a$ and assume that $p \leq n$. If $g(a) = 0$ and the rank of the $p \times n$ matrix
\[(D_jg^i(a))_{i=1,\dots,p \; j=1,\dots, n} \]
be equal to $p$. Then there exists an open set $A \subset \RR^n$ and a differentiable function $h : A \rightarrow \RR^n$ which is bijective onto an open set $V$ and $h^{-1}$ is differentiable and
\[(g \circ h)(x^1, x^2, \dots,x^n) = (x^{n-p+1}, x^{n - p+2}, \dots,x^n) \]
\begin{proof}
We can consider the function $g$ as $g : \RR^{n-p} \times \RR^P \rightarrow \RR^p$. The 'easy' case is as follows:\\
If the $p\times n$ matrix above is such that the last $p$ columns give a matrix $M$ with $det(M) \neq
0$, then we are exactly in the situation of the Implicit Function Theorem as worked out
above. The notation has only slightly changed: $x^{n-p+1} = y^1,\; x^{n-p+2} = y^2, \dots,x^n = y^p, \;
p = m,\; g = f$. We have found $h$ with $h(x, y) = (x, k(x, y))$ and 
\[(f \circ h)(x, y) = f(h(x, y)) = f(x, k(x, y)) = y,\]
and in our notation
\[(g \circ h)(x^1, x^2, \dots,  x^n) = (x^{n-p+1}, x^{n-p+2}, \dots, x^n).\]
In general we cannot assume that the last columns of the matrix give nonzero determinant. We know from Linear Algebra that there will be some $p$ columns with this property. Let these columns be $j^1, j^2, \dots, j^p$ with
\[M =(D_{j_k}g^i(a))_{i=1,\dots,p \; k=1,\dots ,p}, \qquad det(M) \neq 0.\]
We rearrange the variables as follows: Let $m : \RR^n \rightarrow \RR^n$ be defined by (put the variables
with superscript $j_k,\; k = 1, 2,\dots, p$ in the last entries and order in whatever way you want
the other variables)
\[m(x^1, x^2, \dots ,x^n) = (\dots, x^{j_1} ; x^{j_2},\dots, x^{j_p}).\]
Then $g \circ m$ is a function of the type discussed theorem \ref{implicitFunction}, so we can find a function $s : A \rightarrow \RR^n$
which is bijective onto an open set $V$ and $s^{-1}$ is differentiable and
\[ ((g \circ m) \circ s)(x^1, x^2, \dots, x^n) = (x^{n-p+1}; x^{n-p+2} , \dots,  x^n).\]
Then use $h = m \circ s$.
\end{proof}
\end{theorem}

\begin{example}
\[f: \RR^2 \rightarrow \RR^2, \quad f(x,y) = (xy, x^2 + y^2) = (z,w)\]
\[
\begin{pmatrix}
 \frac{dx}{dz} & \frac{dx}{dw}\\
\frac{dy}{dz} & \frac{dy}{dw}
 \end{pmatrix} =   \begin{pmatrix}
 \frac{dz}{dx} & \frac{dz}{dy}\\
\frac{dw}{dx} & \frac{dw}{dy}
 \end{pmatrix}^{-1} \]

\begin{alignat*}{2}
&z=xy, &\quad &y= \frac{z}{w} \\
&w = x^2 + y^2 = x^2 + \frac{z^2}{x^2} &\quad & \\
&wx^2 = x^4 + z^2 &\quad & \\
& x^4 - wx^2 + z^2 &\quad & (*)
\end{alignat*}
\[x=g(z,w)\]
Use implicit differentiation on $(*)$ with respect to $z$:
\[4x^3\frac{dx}{dz} - w \cdot 2x\frac{dx}{dz} + 2z = 0\]
\[\frac{dx}{dz}(4x^3 - 2xw) = -2z\]
\[\frac{dx}{dz} = \frac{-2z}{4x^3 - 2xw}=  \frac{-z}{x(2x^2 - w)}=  \frac{-y}{2x^2 - w}\]
Valid for
\[2x^2 - w \neq 0 \]
\[2x^2 - (x^2 + y^2) \neq 0 \]
\[x^2 - y^2 \neq 0 \]
\[ \Leftrightarrow f'(x,y) \neq 0\]

\[\left.\begin{aligned}
       & f(x,y) = 0 & \; &f(a,b)= 0 \\
        & f(x,g(x)) = 0 & \; &\text{solve implicitly for $y$} \\
	& x \in \RR^n, \; y \in \RR^m & \; & g:\RR^n \rightarrow \RR^m\\
	&f:\RR^n \times \RR^m \rightarrow \RR^m &\; & 
       \end{aligned}
 \right\}
 \qquad \text{Set up of implicit function theorem }\]
\[i=1,\dots,m \quad f^{i}(x^1 , \dots , x^n , g^1(x^1, \dots, x^n), \dots, g^m(x^1, \dots, x^n)) =0\]
how to compute $D_{j}g^{i}?$
\[D_{j}g^{i}(\dots) = 0\]
\[D_{1}f^i\cancel{\frac{dx^1}{dx^j}} + \cancel{\dots} +  D_{j}f^i\underbracket{\frac{dx^j}{dx^j}}_{= 1} + \cancel{\dots} + D_{n}f^i\cancel{\frac{dx^n}{dx^j}}+D_{n+1}f^i\frac{dg^1}{dx^j} + \dots + D_{n+m}f^i\frac{dg^m}{dx^j}=0\]
\[\underbrace{D_{n+1}f^i\frac{dg^1}{dx^j} + \dots + D_{n+m}f^i\frac{dg^m}{dx^j}}_{m \; \text{unknowns}}= - D_{j}f^i\frac{dx^j}{dx^j} \]
Check det of coefficents is $\neq 0$ 
\[\left[\begin{array}{ccc}
D_{n+1}f^1 & \dots & D_{n+m}f^1\\
\vdots & & \vdots\\
D_{n+1}f^m & \dots & D_{n+m}f^m\\
\end{array} \right] = M\]
\end{example}

\section{Integration}
\subsection{Multiple integrals}

$f:A \rightarrow \RR$, $A$ is a rectangle in $\RR^n$ $A= [a_1,b_1]\times \dots \times[a_n , b_n]$\\
Recall a partition $\mathcal{P}$ of $[a,b]$ is a collection of of points: $t_{0}, \dots , t_k$ with $a=t_0 < t_1 < \dots <t_k = b$\\
A Partition of a rectangle $ [a_1 , b_1]\times\dots \times [a_k , b_k]$ is a collection $\mathcal{P} = (P_1 , \dots , P_n)$ where $P_i$ is a partition of $[a_i, b_i], \; i=1,\dots ,n$ Subrectangles $[s_{j-1},s_j]\times[t_{m-1},t_m]$ Let $f$ be bounded on the rectangle  $ [a_1 , b_1]\times\dots \times [a_k , b_k]$

\begin{definition}
 Let $f$ be bounded on the rectangle  $ [a_1 , b_1]\times\dots \times [a_k , b_k]$ and let $S$ be subrectangle of the partition $\mathcal{P}$  
\[m_S(f) = \inf_{x \in S}f(x), \qquad M_S (f) = \sup_{x \in S}f(x) \]
Lower Riemann sum:
\[\mathcal{L}(f,\mathcal{P}) = \sum_{S}m_S(f).v(S)\]
where $v(s)$ is the volume of the subrectangle
\[S=[s_{l-1}, s_l] \times [t_{j-1},t_j] \times \dots \times[r_{k-1},r_k]\]
\[v(S)= (s_{l-1}- s_l)\cdot(t_{j-1},t_j)\cdots (r_{k-1},r_k)\]
Upper Riemann sum:
\[\mathcal{U}(f,\mathcal{P})= \sum_{S}M_S(f).v(S)\]
\[ \mathcal{L}(f,\mathcal{P}) \leq \mathcal{U}(f,\mathcal{P})\]
Refinement: A refinement $\mathcal{P}'$ of the partition $\mathcal{P}$ is as follows. Given $S$ a subrectangle of $\mathcal{P}'$, I can find a subrectangle T of $\mathcal{P}$ such that $S \subset T$ and $T= \cup_{S \subset T} S$, $S$ for $\mathcal{P}'$
\end{definition}

\setcounter{equation}{0}

\begin{lemma}
if $\mathcal{P}'$ is a refinement of $\mathcal{P}$, then:
\begin{align}
\mathcal{L}(f,\mathcal{P}) &\leq \mathcal{L}(f,\mathcal{P}')\\
\mathcal{U}(f,\mathcal{P}) &\geq \mathcal{U}(f,\mathcal{P}') 
\end{align}

\begin{proof} of (1)\\
Let $S$ be a subrectangle of $\mathcal{P}'$ and $T$ a subrectangle of $\mathcal{P}$ such that $S\subset T$ and
\begin{align*}
m_S(f) & \geq m_T(f) \\
m_S(f)v(S) &\geq m_T(f)v(S) \qquad \\
\intertext{now sum over all $S \subset T, \; S \; for \; \mathcal{P}'$}
\sum_{S \subset T}m_S(f)v(S) &\geq \sum_{S \subset T}m_T(f)v(S) = m_T(f)v(T)\\
\mathcal{L}(f,\mathcal{P}') = \sum_{T}\sum_{S \subset T}m_S(f)v(S) &\geq \sum_{T} m_T(f)v(T) = \mathcal{L}(f,\mathcal{P})\\
\end{align*}
\end{proof}
\end{lemma}

\begin{lemma}
For any two partitions $\mathcal{P}$ and $\mathcal{P}'$ we have:
 \[\mathcal{L}(f,\mathcal{P}) \leq \mathcal{U}(f,\mathcal{P}')\]
\begin{proof}
Take $\mathcal{P}''$ a refinement of $\mathcal{P}$ and $\mathcal{P}'$:
\[\mathcal{L}(f,\mathcal{P}) \leq\mathcal{L}(f,\mathcal{P}'') \leq \mathcal{U}(f,\mathcal{P}'') \leq  \mathcal{U}(f,\mathcal{P}')\]
\end{proof}
\end{lemma}

\begin{definition} \quad \\
The lower Riemann integral \[\int\limits_{A \;-}f = \sup_{\mathcal{P}} \;\mathcal{L}(f, \mathcal{P}) , \quad (\mathcal{P}\; \text{partition of rectangle $A$})\]
The upper Riemann integral \[\int\limits_{A}^{-}f = \inf_{\mathcal{P}} \;\mathcal{U}(f, \mathcal{P})\]
$f$ is called integrable if 
\[ \int\limits_{A\;-}f = \int\limits_{A}^{-}f \quad and \quad \int\limits_{A} f = \int\limits_{A \;-}f = \int\limits_{A}^{-}f\] 
\end{definition}

\begin{theorem}[Riemann's Integrability Criterion]\label{reimann}
$f$ is integrable over the rectangle $A \Leftrightarrow \forall  \epsilon > 0, \; \exists$ a partition $\mathcal{P}$ of $A$ such that 
\[\mathcal{U}(f, \mathcal{P}) - \mathcal{L}(f, \mathcal{P}) < \epsilon \]
\begin{proof}
($ \Rightarrow $)\\
\begin{align*}
&\inf_{\mathcal{P}}(\mathcal{U}(f, \mathcal{P}) - \mathcal{L}(f, \mathcal{P})) = 0\\
\Leftrightarrow &\inf_{\mathcal{P}}\;\mathcal{U}(f, \mathcal{P}) - \sup_{\mathcal{P}}\; \mathcal{L}(f, \mathcal{P}) = 0\\
\Leftrightarrow  &\int\limits_{A \;-}f = \int\limits_{A}^{-}f\\
\end{align*}
($\Leftarrow$)\\
Assume $\int\limits_{A \;-}f = \int\limits_{A}^{-}f$, fix $\epsilon > 0$
\[\text{Since} \; \int\limits_{A \;-}\! f = \sup_{\mathcal{P}} \;\mathcal{L}(f, \mathcal{P}), \quad \text{so} \quad \exists \mathcal{P}' \; s.t \; \int\limits_{A \;-}f - \frac{\epsilon}{2} < \mathcal{L}(f, \mathcal{P}')\]
\[\text{Since} \; \int\limits_{A}^{-}f = \inf_{\mathcal{P}} \;\mathcal{U}(f, \mathcal{P}), \quad \text{so} \quad \exists \mathcal{P}' \; s.t \; \int\limits_{A}^{-}f + \frac{\epsilon}{2} > \mathcal{U}(f, \mathcal{P}')\]
Take $\mathcal{P}''$ a common refinement of $\mathcal{P}$ and $\mathcal{P}'$
\[\int\limits_{A}^{-}f + \frac{\epsilon}{2} \;>\; \mathcal{U}(f, \mathcal{P}'') \geq \mathcal{L}(f, \mathcal{P}'') \;> \int\limits_{A \;-}f - \frac{\epsilon}{2}\]
So
\[\mathcal{U}(f, \mathcal{P}'') - \mathcal{L}(f, \mathcal{P}'') < \left(\cancel{\int\limits_{A}^{-}\!f} + \frac{\epsilon}{2}\right) - \left(\cancel{\int\limits_{A \;-}\!\!f} - \frac{\epsilon}{2}\right) = \epsilon\]
\end{proof}
\end{theorem}

\begin{example} Non-Riemann integrable function $f:\RR^2 \rightarrow \RR^2$
\[f(x,y) =
  \begin{cases}
  1 & \text{if } x \in \QQ \\
   0      & \text{if } x \notin \QQ
  \end{cases} \]
\begin{alignat*}{2}
m_S(f) &= 0 &\qquad M_S(f)&=1\\
\mathcal{L}(f, \mathcal{P}) &= 0 &\qquad \mathcal{U}(f, \mathcal{P}) &=1
\end{alignat*}
\end{example}

\begin{definition}
If $C \subset \RR^n$, define the characteristic function of $C$ to be 
\[X_{C}(x) =\begin{cases}
  1 & \text{if } x \in C \\
   0      & \text{if } x \notin C
  \end{cases} \]
If $f$ is bounded on $\bar{C}$ and $C$ is contained in a rectangle $A$, we define 
\[\int\limits_{C}f = \int\limits_{A}fX_{C}\]
\end{definition}

\setcounter{equation}{0}
$f:[a,b]\times[c,d] \rightarrow \RR$\\
Fix $x$ and consider $g_{x}:[c,d]\rightarrow\RR$ 
\[g_{x}(y) = f(x,y)\]
\[I(x)=\int_{c}^{d}g_{x}dy = \int_c^df(x,y)dy\]
\begin{equation}
\int_a^bI(x)dx = \int_a^b\left(\int_c^df(x,y)dy\right)dx
\end{equation}
Fix $y$ and define $h_{y}:[a,b]\rightarrow\RR$
\[h_y(x)=f(x,y)\]
\[J(y)=\int_{a}^{b}h_{y}dx = \int_a^bf(x,y)dx\]
\begin{equation}
\int_c^dJ(y)dy = \int_c^d\left(\int_a^bf(x,y)dx\right)dy
\end{equation}
$(1) = (2)$

\subsection{Fubini's theorem}

\begin{theorem}[Fubini]
Let $A$ be a rectangle in $\RR^n$ and let $B$ be a rectangle in $\RR^m$. $f:A \times B \rightarrow \RR$ is intergrable. define: 
\[ g_x: B \rightarrow \RR \quad  \text{ by } \quad g_x = f(x,y) , \qquad  \forall y \in B,\;  \forall x \in A \]
and let: 
\[ \left.\begin{aligned}
       \mathfrak{L}(x) &=  \int\limits_{B \;-}g_x =\int\limits_{B \;-}f(x,y)dy\\
       \mathfrak{U}(x) &=  \int\limits_{B}^{-}g_x =\int\limits_{B}^{-}f(x,y)dy
       \end{aligned}
 \; \right\}
 \qquad \text{exists} \; \; \forall x \in A\]
Then $\mathfrak{L}(x)$ and $\mathfrak{U}$ are intergrable over $A$, and:
\[\int\limits_{A}\mathfrak{L}(x)dx =\int\limits_{A} \left(\;\int\limits_{B \;-}f(x,y)dy \right)\! dx=\int\limits_{A} \left(\int\limits_{B}^{-}f(x,y)dy \right)\! dx = \int\limits_{A}\mathfrak{U}(x)dx = \int\limits_{A \times B}f\]
\begin{proof}
Let $\mathcal{P}_A$ be a partition of $A$, $\mathcal{P}_B$ be a partition of $B$. Let $S_A$ a subrectangle of $A$,  $S_B$ a subrectangle of $B$. Then the rectangles $S_A \times S_B$ give a partition $\mathcal{P}$ of $A\times B.$ \\
We will prove:
\[\mathcal{L}(f,\mathcal{P}) \underset{(1)}{\leq} \mathcal{L}(\mathfrak{L},\mathcal{P}_A) \underset{(2)}{\leq} \mathcal{U}(\mathfrak{L},\mathcal{P}_A) \underset{(3)}{\leq} \mathcal{U}(\mathfrak{U},\mathcal{P}_A) \underset{(4)}{\leq} \mathcal{U}(f,\mathcal{P})\]
Since $f$ is integrable over $A \times B$, given $\epsilon > 0$ Riemann's integrability criterion given a partition $\mathcal{P}$ of $A \times B$, such that: $\mathcal{U}(f,\mathcal{P}) - \mathcal{L}(f,\mathcal{P}) < \epsilon $. Then $\mathcal{P}$ defines $\mathcal{P}_A ,\; \mathcal{P}_B$ partitions of $A, \; B$ respectively. By the inequality above: $\mathcal{U}(\mathfrak{L},\mathcal{P}_A) - \mathcal{L}(\mathfrak{L},\mathcal{P}_A) < \epsilon $. By reimann's integrability criterion, $\mathcal{L}$ is integrable over $A$, since: 
\[\sup_{\mathcal{P}}\mathcal{L}(f,\mathcal{P})= \inf_{\mathcal{P}}\mathcal{U}(f,\mathcal{P}) = \int\limits_{A\times B}\! \! f \quad \Rightarrow \quad  \int\limits_{A}\mathfrak{L}(x)dx = \sup_{\mathcal{P}_A}\mathcal{L}(\mathfrak{L},\mathcal{P}_A)= \inf_{\mathcal{P}_A}\,\mathcal{U}(\mathfrak{L},\mathcal{P}_A) = \int\limits_{A\times B}\! \!f\]
Works simularly with $\mathfrak{U}(x)$.\\
Side remark:
\begin{align*}
\mathcal{L}(f, \mathcal{P}) &\leq \mathcal{L}(\mathfrak{L}, \mathcal{P}_A)\\
\sup_{\mathcal{P}}\mathcal{L}(f, \mathcal{P}) &\leq \sup_{\mathcal{P}_A}\mathcal{L}(\mathfrak{L}, \mathcal{P}_A)\\
\mathcal{U}(\mathfrak{L}, \mathcal{P}_A) &\leq \mathcal{U}(f, \mathcal{P})\\
\inf_{\mathcal{P}_A}\mathcal{U}(\mathfrak{L}, \mathcal{P}_A) &\leq \inf_{\mathcal{P}}\mathcal{L}(f, \mathcal{P})
\end{align*}

\begin{align*}
(2) \quad \mathcal{L}(\mathfrak{L},\mathcal{P}_A) &\leq \mathcal{U}(\mathfrak{L},\mathcal{P}_A) \\ &\text{always true for  a function $\mathfrak{L}$, partition $\mathcal{P}_A$} \\ \;& \; \\
(3) \quad \mathcal{U}(\mathfrak{L},\mathcal{P}_A) &\leq \mathcal{U}(\mathfrak{U},\mathcal{P}_A)\\
&\mathfrak{L}(x)  =\int\limits_{B \;-}f(x,y)dy, \; \mathfrak{U}(x)  =\int\limits_{B}^{-}f(x,y)dy \Rightarrow \mathfrak{L}(x) \leq \mathfrak{U}(x) \\
 &\Rightarrow \mathcal{U}(\mathfrak{L}(x), \mathcal{P}_A) \leq \mathcal{U}(\mathfrak{U}(x), \mathcal{P}_A) \\
\end{align*}
$(4)$ is proved simularly to $(1)$ so we only prove $(1)$. 

\[\mathcal{L}(f,\mathcal{P})= \sum_{S}m_{s}(f) v(S) = \sum_{S_A, \; S_B}\! m_{S_A \times S_B}(f)  v(S_A \times S_B) = \sum_{S_A}\left(\sum_{S_B} m_{S_A \times S_B}(f)  v(S_B) \right)\! v(S_A)\]
Now, if $x \in S_A$, then clearly $m_{S_A \times S_B}(f) \leq m_{S_B}(g_x).$ Consequently, for $x \in S_A$ we have
\[\sum_{S_B} m_{S_A \times S_B}(f) \cdot v(S_B) \leq \sum_{S_B}m_{S_B}(g_x)\cdot v(S_B) \leq \int\limits_{B \;-}g_x = \mathfrak{L}(x).\]
Therefore
\[\sum_{S_A}\left(\sum_{S_B} m_{S_A \times S_B}(f)  v(S_B) \right)\! v(S_A) \leq \mathcal{L}(\mathfrak{L}, \mathcal{P}_A).\]
When the function is reimenn integral
\end{proof}
\end{theorem}

\subsection{Change of variables}
\begin{theorem}
Let $A \in \RR^n$ be open, $g:A \rightarrow \RR^n$ be injective and continuously differentiable with $det \;g'(x) \neq 0, \; \forall x \in A$. Let $f:g(A) \rightarrow \RR$ be integrable.Then we have change of variables formula:
\[\int\limits_{g(A)}\!f=\int\limits_{A}(f \circ g)\cdot |det \:g'(x)|dx\]
\end{theorem}

\section{Calculus on Manifolds}








\subsection{Manifolds}

\begin{definition}[$C^{\infty}$]
A function $f:\RR^n \rightarrow \RR^m$ is called a $C^{\infty}$ function if all partial derivitives of all orders of all components exists and are continuous
\end{definition}

\begin{definition}[Diffeomorphism]
Let $U, \; V$ be open sets in $\RR^n$. A $C^{\infty}$ function $h:U\rightarrow V$ bijective and $h^{-1}:V \rightarrow U$, also a $C^{\infty}$ function, is called a diffeomorphism from $U$ to $V$
\end{definition}

\begin{definition}
A set $M$ is a K-dim manifold in $\RR^n$ if the following condition (M) holds. For every $x \in M$:\\
(M): There exisits two open sets $U, \; V$ of $\RR^n, \; x \in U$ and a diffeomorphsim $h:U \rightarrow V$ such that:
\[ h(U \cap M) = \{ y \in V \; s.t. \;y^{k+1} = y^{k+2} = \dots = y^{n} = 0\} \]
\end{definition}  

\begin{remark} \textmd{Reminder of linear algebra}\\
$T:\RR^n \rightarrow \RR^p$ Linear transformation, $rank(T) = dim(T(\RR^n)) \leq p$\\
\[[T]: \quad \text{rank = max number of linearly independant rows or columns.}\]
$rank(T) \leq min(n,p)$
\[[T] = \begin{pmatrix}
a_{11} & \dots & a_{1n}\\
\vdots & & \vdots\\
a_{p1} & & a_{pn} \end{pmatrix}, \quad [T] \in M_{p \times n} \]
determinant of minors: If $r$ is the max size of an $r \times r$ minor with non-zero determinant, then rank(T) = r.
\end{remark}


\begin{theorem}\label{manifold1}
 Let $A \rightarrow \RR^n$ be open and let $g : A \rightarrow \RR^p$ be a differentiable function such
that $g'(x)$ has rank $p$ on the set $g^{-1}(0)$. Then $g^{-1}(0)$ is an $n\! -\! p$ dimensional manifold in $\RR^n$.
\begin{proof}
 It follows directly from theorem \ref{imp2}. Let $x \in g^{-1}(0) = M$. We take $V = A$ in theorem\ref{imp2} so that we can find a diffeomorphism $H : V \rightarrow U$ , where $U$ is open in $\RR^n$ and 
\[g \circ H(x^1, x^2, \dots, x^n) = (x^{n-p+1}, x^{n-p+2}, \dots , x^n) \]
Let $h = H^{-1} : U \rightarrow V$ . We need to show that 
\[h(U \cap M) = \{y \in V,\; y^{n-p+1} = y^{n-p+2} = \dots = y^n = 0\}.\]
Let $y \in U \cap M$. Then $y \in g^{-1}(0)$, i.e. $g(y) = 0$. Since
\[h(g^{_1}(0)) = H^{-1}(g^{-1}(0)) = (g \circ H)^{-1}(0) = \{y \in V,\; y^{n-p+1} = y^{n-p+2} = \dots = y^n = 0\},\]
clearly we have for $ y \in g^{-1}(0)$ that $ h(y)$ has its last $p$ coordinates zero. The converse is
also obvious: if $z \in \{y \in V,\; y^{n-p+1} = y^{n-p+2} = \dots = y^n = 0\}$ then set $y = H(z) \in U$
and $g(y) = g(H(z)) = (z^{n-p+1},\dots, z^n) = (0,\dots ,0) \Rightarrow y \in g^{-1}(0) = M$ and $z = h(y) \in
h(U \cap M).$
\end{proof}
\end{theorem}

\begin{example}
\[S^2 = \{(x,y,z) \in \RR^3; \; \;  x^2 + y^2 +z^2 = 1\}\quad \text{$2$-dim manifold in $\RR^3$}\]
$g:\RR^3 \rightarrow \RR$
\[g(x,y,z) = x^2 + y^2 + z^2 - 1 \qquad S^2 = g^{-1}(0)\]
\[g'(x,y,z) = (\frac{dg}{dx}, \frac{dg}{dy}, \frac{dg}{dz}) = (2x, 2y, 2z)\]
Rank can be $0$ or $1$
\[\underbrace{2}_{(n-p)} = \underbrace{3}_{(n)}  - \underbrace{1}_{(p)} \qquad \text{Aim to show } rank(g')=1\; on\; M=g^{-1}(0) \]
\begin{align*}
rank\;g' = 0 &\Leftrightarrow 2x=2y=2z=0\\
&\Leftrightarrow (x,y,z)=(0,0,0)
\end{align*}
but $(0,0,0) \notin g^{-1}(0)$ because $g(0,0,0) = 0^2 + 0^2 +0^2 -1= -1$
\end{example}


\begin{example}The Sphere 
\[S^n = \{(x^1,\dots,x^{n+1}); \; \; ( x^1)^2 + \dots +(x^{n+1})^2  = 1\}\quad \text{is an $n$-dim manifold in $\RR^{n+1}$}\]
$g:\RR^{n+1} \rightarrow \RR^p$
\[g(x^1,\dots,x^{n+1}) =( x^1)^2 + \dots +(x^{n+1})^2  - 1 \qquad S^n = g^{-1}(0)\]
\[g'(x^1,\dots,x^{n+1})  = (\frac{dg}{dx^1},\dots , \frac{dg}{dx^{n+1}}) = \underbrace{(2x^1, \dots, 2x^{n+1})}_{1 \times (n+1)\; matix}\]
\begin{align*}
rank\;g' = 0 &\Leftrightarrow 2x^1=\dots =2x^{n+1}=0\\
&\Leftrightarrow (x^1,\dots ,x^{n+1})=(0,\dots ,0)
\end{align*}
but $(0,\dots ,0) \notin S^n$
\end{example}

\begin{example}Hyperbolic space
\[\mathbb{H}^n = \{(x^1,\dots,x^{n+1}) \in \RR^{n+1},  \; \; ( x^1)^2  - [( x^2)^2 + \dots +(x^{n+1})^2 ] = 1\} \]
$n$-dim hyperbolic space $g:\RR^{n+1} \rightarrow \RR$
\[g(x^1,\dots,x^{n+1}) = ( x^1)^2  - [( x^2)^2 + \dots +(x^{n+1})^2 ] - 1 \]
\[\mathbb{H}^n = g^{-1}(0) \Rightarrow (g:\RR^{n+1} \rightarrow \RR) \rightarrow (g: A \rightarrow \RR), \quad A = \{x \in \RR^{n+1}, \; x^1 > 0 \}. \]
\[g'(x^1,\dots,x^{n+1})  = (2x^1, - 2x^2 , \dots, - 2x^{n+1}) \]
\begin{align*}
rank\;g' = 0 &\Leftrightarrow x^1=\dots =x^{n+1}=0\\
&\Leftrightarrow (x^1,\dots ,x^{n+1})=(0,\dots ,0)
\end{align*}
but $(0,\dots ,0) \notin \mathbb{H}^n$ so $rank (g')= 1$ on $g^{-1}(0)$ so by theorem \ref{manifold1} $g^{-1}(0)$ is an $(n+1)-1$ dim manifold in $\RR^{n+1}$.
\end{example}

\begin{example}
Ellipsoid
\[\frac{x^2}{a^2} +\frac{y^2}{b^2} +\frac{z^2}{c^2} =1, \quad a,b,c>0\]
\[g(x,y,z) = \frac{x^2}{a^2} +\frac{y^2}{b^2} +\frac{z^2}{c^2} -1 \]
\[g'(x,y,z) = (\frac{2x}{a^2}, \frac{2y}{b^2} , \frac{2z}{c^2}) = 0 \Leftrightarrow x=y=z=0 \]
but $(0,0,0)$ does not belong to the ellipsoid.
\end{example}

\begin{example}The graph of a differentiable function\\
$f:U \rightarrow \RR, \quad U \subseteq \RR^2$ 
\[M = \{(x,y,z) \in \RR^3, \; z=f(x,y)\}\quad (mange patch) \]
$2$-dim manifold in $\RR3$
\[g(x,y,z)=f(x,y) - z\]
\[g'(x,y,z) = (\frac{df}{dx},\frac{df}{dy},-1) \neq 0 \Rightarrow rank(g') = 1\]
\end{example}


The following theorem gives the coordinate definition of a manifold M.

\begin{theorem}\label{man1}
A subset $M$ of $\RR^n$ is a $k$-dimensional manifold iff for every point $x \in M$ the following holds:\\
(C) There exists an open set $U \in \RR^n,\; x \in U$ and an open set $W \subset \RR^k$ and an injective
differentiable map $f : W \rightarrow \RR^n$ such that
\begin{enumerate}[(i)]
\item $f(W) = U \cap M$
\item rank $f'(y) = k \quad \forall y \in W$
\item $f^{-1} : f(W) \rightarrow W$ is continuous.
\end{enumerate}
\begin{proof}
 Lets assume that M is a manifold according to the deffnition (M). We choose the function $h : U \rightarrow V$ as in the definition. We define the set $W$ and the function $f$ as follows:
\[W = \{a \in \RR^k, \; (a, 0) \in h(U \cap M) \}, \quad f : W \rightarrow \RR^n, \quad  f(a) = h^{-1}(a, 0).\]
Here $(a, 0)$ is the vector with the last $n-k$ coordinates equal to $0$. Obviously $f(W) = U\cap M$, since
\[a \in W \Leftrightarrow (a, 0) \in h(U \cap M) \Leftrightarrow h^{-1}(a, 0) \in U \cap M \Leftrightarrow f(a) \in U \cap M.\]
We prove that $W$ is open. For $a \in W$, we have:
\[(a, 0) \in h(U \cap M) \Leftrightarrow h^{-1}(a, 0) \in U \cap M \Rightarrow h^{-1}(a, 0) \in U.\]
Since $h^{-1}$ is continuous, if $b$ is sufficiently close to $a$, so that $(a, 0)$ and $(b, 0)$ are sufficiently close, we can deduce that $h^{-1}(b, 0)$ is close enough to $h^{-1}(a, 0)$. Because $U$ open, if $h^{-1}(a, 0) \in U$, then also $h^{-1}(b, 0) \in U$. This gives $(b, 0) \in h(U)$. Because $h(U \cap M)$ consists exactly of the points with last $n- k$ components equal to $0, \; (b, 0) \in h(U\cap M) \Leftrightarrow b \in W$. We immediately see from the definition of $f$ and $W$ that $f^{-1}$ is continuous (it maps $h^{-1}(a, 0)$ to $a$ while $h$ is continuous).\\
We prove that the rank of $f'(y)$ is $k$ on $W$. For this we introduce another function
\[H : U \rightarrow \RR^k, \quad  H(z) = (h^1(z),\dots , h^k(z)),\]
i.e. $H$ has the same first $k$ coordinates as $h$ (and ignores the last $n - k$). We have
\[H(f(y)) = H(h^{-1}(y, 0)) = y, \quad  y \in W.\]
Therefore, $H'(f(y) )\cdot f'(y) = I_{k\times k}$ or, in terms of linear transformations:
\[DH(f(y)) \circ Df(y) = Id_{\RR^k}.\]
Because the composition is injective, $Df(y)$ is injective and the nullity plus rank theorem for $Df(y) : \RR^k \rightarrow \RR^n$ gives that the rank of $Df(y)$ is $k$.\\ \\
 \textit{The converse:} Suppose that $f : W \rightarrow \RR^n$ satisfies condition (C). We have $f'(y) \in M_{n\times k}$.
By rearranging the coordinates in $\RR^n$, we can assume that the rank of the first $k$ rows of $f'(a)$ is $k$. This means
\[det (D_jf^i(a))_{i,j=1,\dots ,k} \neq 0.\]
We define
\[g : W \times \RR^{n - k} \rightarrow \RR^n, \quad g(a, b) = f(a) + (0, b),\]
where $(0, b)$ has the first $k$ coordinates $0$. We have
\[g^i(a, b) = f^i(a), \quad i \leq k, \quad g^i(a, b) = f^i(a) + b^i, \quad i > k.\]
We compute its Jacobian matrix. For $i\leq k$
\begin{align*}
D_jg^i(a, b)  = D_jf^i(a) \Rightarrow D_jg^i(a, b) &= D_jf^i(a), \quad j \leq k,\\
and \quad D_jg^i(a, b) & = 0, \quad j > k.
\end{align*}
For $i > k$, however, we have
\[D_jg^i(a, b) = D_jf^i(a) + D_jb^i \Rightarrow D_jg^i(a, b) = \delta _{ij}, \quad j > k\]
while
\[D_jg^i(a, b)  = D_jf^i(a), \quad j \leq k.\]
The Jacobian matrix is therefore in block form
\[
 g'(a,b) =
 \left( \begin{array}{c|c}
 D_jf^i(a)_{i,j = 1,\dots , k}  & 0 \\
\hline
 D_jf^i(a)^{i=k+1 , \dots, n}_{j = 1,\dots , k}  & I_{(n-k)\times (n-k)} \\
 \end{array} \right)
\]
The calculation of the determinant in block form (which can be considered as successive expansion on the last column) gives that $det \;g'(a, b) \neq 0$. By the inverse function theorem, there exists an open set $V_1$ with $(a, 0) \in V_1$ and an open set $V_2$ containing $g(a, 0) = f(a)$, such that $g : V_1 \rightarrow V_2$ has a differentiable inverse $h : V_2 \rightarrow V_1$. Then, since $f(W) = U \cap M$, we have for $(x, 0) \in V_1,\; g(x, 0) \in M \Leftrightarrow f(x) \in M$: This gives
\[V_2 \cap M = \{g(x, 0), \; (x, 0) \in V_1\}.\]
\[h(V_2 \cap M) = g^{-1}(V_2 \cap M) = g^{-1}(\{g(x,  0), \; (x, 0) \in V_1\}) = V_1 \cap (\RR^k\times \{0\}).\]
\end{proof}
\end{theorem}

\begin{example}2-dim torus.

\begin{align*}
(x-2)^2 = z^2 &=1\\
(r-2)^2 + z^2 &= 1 \end{align*}
\begin{alignat*}{2}
z&=\sin\phi &\qquad x&=r\cos\theta\\
r-2&=\cos\phi &\qquad y&=r\sin\theta
\end{alignat*}
\[f(\theta,\phi)= ((2+\cos\phi)\cos\theta, \; (2+\cos\phi)\sin\theta, \; \sin\phi) \quad \theta, \phi \in (-\pi , \pi )\]
For theorem \ref{man1}, take $U=\RR^3$
\begin{align*}
f(W) &= U\cap M\\
f(W) &= M
\end{align*}

$f:\underset{\subseteq \RR^2}{W} \rightarrow \RR^3$
\[f'(\theta, \phi) = \begin{pmatrix}  (2+\cos\phi)(-\sin\theta) &-\sin\phi \cos\theta \\
(2+\cos\phi)\cos\phi & -\sin\phi \sin\theta\\
0 & \cos\phi \end{pmatrix}\]
\begin{align*} 2 \times 2\text{ minor}&: \quad \begin{vmatrix}  (2+\cos\phi)(-\sin\theta) &-\sin\phi \cos\theta \\
(2+\cos\phi)\cos\phi & -\sin\phi \sin\theta \end{vmatrix}\\ &= (2+\cos\phi)(-\sin\phi) \begin{vmatrix}  -\sin\theta & \cos\theta \\ \cos\phi &  \sin\theta \end{vmatrix}\\
&=  (2+\cos\phi)\sin\phi \neq 0 \; iff\; \sin\phi \neq 0 \Leftrightarrow \phi \neq 0\end{align*}
$\therefore$ rank $f' = 2$ whenever $\phi \neq 0$\\
When $\phi =0$ 
\[f'(\theta, \phi) = \begin{pmatrix} -3\sin\theta & 0\\ 3\cos\theta & 0 \\ 0 & 1 \end{pmatrix}\]
if $\theta = 0$ use:
\[\begin{vmatrix} 3\cos\theta & 0 \\ 0 & 1 \end{vmatrix} = \begin{vmatrix} 3 & 0 \\ 0 & 1 \end{vmatrix} = 3\]
if $\theta = 0$ use:
\[\begin{vmatrix} 3\sin\theta & 0 \\ 0 & 1 \end{vmatrix} = 3\sin\theta \neq 0 \quad on \; \theta\in (-\infty, \infty), \; \theta \neq 0\]
\end{example}
\begin{example}
Any nice surface of revolution is a 2-dim manifold in $\RR^3$ 
\[\gamma(t) = (r(t), z(t)) \quad t \in (a,b)\]
$\gamma$ does not have any self intersections $r(t)>0$. If $\gamma$ is differentiable and
\[\gamma'(t) = (r'(t) , z'(t)) \neq 0 \quad \forall t \in (a,b)\]
then when we rotate it arround the $z$-axis we get the surface
\[f(t,\theta) = (r(t)\cos\theta, \; r(t)\sin\theta, \; z(t)) \quad t \in (a,b), \; \theta \in (-\pi , \pi)\]
\[f'(t,\theta) = \begin{pmatrix} r'\cos\theta & -r\sin\theta \\ r'\sin\theta & r\cos\theta \\ z' & 0 \end{pmatrix}\]
\[\begin{vmatrix} r'\cos\theta & -r\sin\theta \\ r'\sin\theta & r\cos\theta \end{vmatrix} = r\cdot r'\]
\[r>0, \; r' \neq 0 \Rightarrow rank =2 \]
\end{example} 

\quad \\
We also need the definition of manifold with boundary. While a $k$-dimensional manifold in $\RR^n$ looks like a $k$-dim  slice of $\RR^n$, according to condition (M), for a manifold with boundary in $\RR^n$, the part close to the boundary looks likes a half-slice of dimension $k$. To make this precise we define the half-space
\[\mathbb{H}^k = \{x \in \RR^k, \;  x^k \geq 0 \}.\]
Then
\[h(U \cap M) = \{ y \in V: \; y^k \geq 0, \;y^{k+1} = y^{k+2} = \dots = y^{n} = 0\}\]
is the substitute for condition (M). More precisely:

\begin{definition} 
A subset $M$ of $\RR^n$ is a $k$-dimensional manifold with boundary if for every point $x$ of $M$ either condition (M) holds or (exclusive) the following condition holds: \\
(M') There is an open set $U$ of $\RR^n$ containing $x$, an open set $V$ contained in $\RR^n$ and a
diffeomorphism $h : U \rightarrow V$ such that
\[h(U \cap M) = V \cap (\mathbb{H}^k\times \{0\}) = \{ y \in V: \; y^k \geq 0, \;y^{k+1} = y^{k+2} = \dots = y^{n} = 0\}.\]
Moreover, $h^k(x) = 0$.The set of points where condition (M') holds is called the boundary of $M$ and is denoted
by $\partial M$.
\end{definition}

\subsection{Dual Space}


\begin{definition}
Let $g^{i}:\RR^n \rightarrow \RR$ be a linear map, such a map is called a linear functional. The set of all linear functionals from $\RR^n \rightarrow \RR$ is called the dual space of $\RR^{n}$, denoted $(\RR^n)^*$\\
let $g^{1}, \dots , g^{m}$ be linear functionals $g^{i}:\RR^n \rightarrow \RR$, then I can combine them to get a map $g:\RR^n \rightarrow \RR^{m}$ by $g(x) =(g^{1}(x), \dots , g^{m}(x)$)\\
$g:\RR^n \rightarrow \RR^{m}$ is linear such for $x,y \in \RR^n, \; \lambda \in \RR$
\begin{align*}
g(\lambda x +y) &= \lambda g(x) + g(y)
\shortintertext{this can be seen by}
g(\lambda x +y) &=(g^{1}( \lambda x + y), \dots , g^{m}( \lambda x + y) \\
&= ( \lambda g(x)^{1} +g^{1}(y), \dots ,\lambda g(x)^{m} +g^{m}(y))\\
&= \lambda (g^{1}(x), \dots, g^{m}(x)) + (g^{1}(y), \dots , g^{m}(y))
\end{align*}
$[g^{i}]$ is the matrix representation of $g^{i}$\\
$[g^{i}] = (g_{1}^{i}, \dots , g_{n}^{i})$
\[ [g]_{mxn} = \begin{pmatrix}
  g_{1}^{1}  & \cdots & g_{n}^{1} \\
  \vdots   & & \vdots  \\
  g_{1}^{m} & \cdots &g_{n}^{m}
 \end{pmatrix}\]
\end{definition}

\begin{theorem}
$f:\RR^{n} \rightarrow \RR^{m}$ is differentiable at a iff $f^{i}$ are differentiable at a, $i=1, \dots , m$
and $Df(a) = (Df^{1}, \dots , Df^{m}(a))$
\begin{proof}
assume $f$ is differentiable at $a$ we take the linear function $\Pi^{i}(x^{1}, \dots , x^{m}) = x^{i}$ and compose it with $f$ we get 
\[f^{i} =  \Pi^{i} \circ f\]
this is differentiable by chain rule since $f$ and $\Pi^{i}$ are differentiable $\forall i =1 , \dots , m$
\begin{align*}
\Rightarrow Df^{i} &= D\Pi^{i}(a) \cdot Df(a)
\shortintertext{$D\Pi^{i} = \Pi^{i}$}
\Rightarrow Df^{i} &= \Pi^{i}(a) \cdot Df(a)
\end{align*}
Now assume the all $f^{i}$ are differentiable at $a$ $\forall i=1, \dots , m$
\begin{align*}
&f(a+h) - f(a) -(Df^{1}(a)(h), \dots , Df^{m}(a)(h))\\
 &= (f^{1}(a+h), \dots , f^{m}(a+h)) - (f^{1}, \dots, f^{m}) - (Df^{1}*(a)(h), \dots , Df^{m}(a)(h))\\
&=(f^{1}(a+h) -f^{1}(a) - df^{1}(a) , \dots , f^{m}(a+h) -f^{m}(a) - df^{m}(a))
\end{align*}
So
\begin{align*}
 &\frac{|f(a+h) - f(a) -(Df^{1}(a)(h), \dots , Df^{m}(a)(h))|}{|h|} \\
&\leq \frac{|f^{1}(a+h) -f^{1}(a) - df^{1}(a)|}{|h|} , \dots ,\frac{| f^{m}(a+h) -f^{m}(a) - df^{m}(a)|}{|h|} \rightarrow 0
\end{align*}
\end{proof}
\end{theorem} 

\begin{remark}
If $T,S:\RR^{n} \rightarrow \RR^{m}$ are linear then $(T +S):\RR^{n} \rightarrow \RR^{m}, (T+S)(x) = T(x) + S(x)$ is linear.\\
If $\lambda \in \RR$ then $(\lambda T):\RR^{n} \rightarrow \RR^{m}, (\lambda T)(x) = \lambda \cdot T(x)$ is also linear.
\end{remark}
\begin{definition} A linear functional $f$ is a linear transformation $f:V \rightarrow \RR$
\[f(\lambda x + y ) = \lambda f(x) + f(y) \quad \forall x,y \in V, \; \forall \lambda \in \RR\]
\end{definition}

\begin{definition}[Dual Space]
\[V^{*} = \{f:V\rightarrow \RR : f \text{ are linear functionals}\}\]
\end{definition}

\begin{definition}
If $f,g \in V^{*}$ and $\lambda \in \RR$ then
\[f+g, \; \lambda f:V \rightarrow \RR\]
with:
\[(f+g)(x) = f(x) + g(x)\]
\[(\lambda f)  = \lambda f(x)\] \end{definition}

\begin{proposition}
\[\dim(V^{*}) = \dim(V)\]
\begin{proof}
We have $\{v_1, \dots , v_n\}$ a basis of $V$, $\forall i = 1,\dots , n$ define $\varphi_i:V\rightarrow \RR$ as follows:\\
Given $x\in V$
\[x  = x^1v_1 + \cdots + x^nv_n \quad (uniquely) \quad x^i \in \RR\]
\[\varphi_i(x) = x^i\]
$\varphi_i$ is a linear functional. If $y \in V$
\[y= y^1v_1 + \cdots + y^nv_n \quad v^i \in \RR\] 
for $\lambda \in \RR$
\[ \lambda x + y = (\lambda x^1 + y^1)v_1 + \cdots + (\lambda x^n + y^n)v_n\] 
\[\varphi_i(\lambda x + y) = \lambda x^i + y^i = \lambda\varphi_i(x) + \varphi_i(y)\]
\[\varphi_i(v_j) = \delta_{ij}\]
Is $\{\varphi_1, \dots , \varphi_n\}$ a basis for $V^*$?\\ 
Spanning:\\ Given $f \in V^*$, define $a^i \in \RR$
\[f(v_i) = a^i\]
we will show 
\[f=a^1\varphi_1 + \cdots + a^n\varphi_n\]
if $f$ and $a^1\varphi_1 + \cdots + a^n\varphi_n$ agree on the basis $\{v_1 , \dots , v_n\}$ then its true.
For $K=1,\dots, n$
\[f(v_k) = a^k\]
\[(a^1\varphi_1, \dots ,a^n\varphi_n)(v_k) = a^1\varphi_1(v_k) + \cdots + a^k\varphi_k(v_k) + \cdots + a^n\varphi_n(v_k) = a^k\cdot 1 \]
Linear Independence:\\
\[b^1\varphi_1 + \cdots + b^n\varphi_n = 0 \overset{?}{\Rightarrow} b^k = 0 \quad \forall k\]
Apply to a basis vector $v_k$
\[(b^1\varphi_1 + \cdots + b^n\varphi_n)(v_k) = b^1\cdot 0 +\cdots +b^{k-1}\cdot0 + b^k\cdot 1 + b^{k+1}\cdot 0 + \dots + b^n\cdot 0 = b^k \]
\end{proof}
Hence if $\{v_1, \dots , v_n\}$ a basis of $V$, then $\{\varphi_1, \dots , \varphi_n\}$ a basis of $V^*$
\end{proposition}



\subsection{Multilinear Algebra}



\begin{definition}
Let $V$ be a vector space over $\RR$, define
\[V^k = \underbrace{V \times \cdots \times V}_{k \; times }\]
To be
\[V^k = \{(v_1, \dots, v_k) : v_i \in V \}.\]
This is a vector space with opperations
 \[(v_1, \dots , v_k) + (w_1, \dots , w_k) = (v_1 + w_1, \dots , v_k + w_k)\]
\[\lambda(v_1 , \dots , v_k) = (\lambda v_1 , \dots , \lambda v_k) \]

\end{definition}

\begin{definition}
$\mathcal{T}:V^k \rightarrow \RR$ is called multilinear if  $\forall i = 1, \dots , k$
\begin{align*}
\mathcal{T}(v_1, \dots , v_{i-1},v_i +v_i^{'}, v_{i+1}, \dots , v_k) &= \mathcal{T}(v_1, \dots , v_{i-1},v_i , v_{i+1}, \dots , v_k)\\ &+ \mathcal{T}(v_1, \dots , v_{i-1},v_i^{'}, v_{i+1}, \dots , v_k)\\
\mathcal{T}(v_1, \dots , v_{i-1},\lambda v_i , v_{i+1}, \dots , v_k) &= \lambda\mathcal{T}(v_1, \dots , v_{i-1},v_i , v_{i+1}, \dots , v_k)\end{align*}
$\mathcal{T}$ is not linear on $V^k$ but linear on each of its components. A $\mathcal{T}$ like this is called a k-tensor on $V$.
\end{definition}

\begin{definition}
\[\mathcal{J}^k(V) = \{\mathcal{T}:V^k \rightarrow \RR : k-multilinear\}\]
\end{definition}

\begin{note} if $k=2$ then $\mathcal{T}$ is called bilinear, and
\begin{align*}
\mathcal{T}(v_1 + v_2 , w) &= \mathcal{T}(v_1, w)+ \mathcal{T}(v_2, w)\\
\mathcal{T}(\lambda v, w) & = \lambda\mathcal{T}(v, w)\\
\mathcal{T}(v,w_1 + w_2) &= \mathcal{T}(v, w_1)+ \mathcal{T}(v, w_2)\\
\mathcal{T}(v, \lambda w) & = \lambda\mathcal{T}(v, w)
\end{align*}

also
$\mathcal{T}$ is called Symmetric if \[\mathcal{T}(v,w) =  \mathcal{T}(w,v)\]
$\mathcal{T}$ is called positive definite if \[\mathcal{T}(v,w)\geq 0 \quad \forall \; v,w\]
 \end{note}

\begin{definition}[Symmetric k-tensor]
$\mathcal{T}$ is a Symmetric k-tensor if $\forall v_i , \dots , v_k \in V$
\[\mathcal{T}(v_1, \dots , v_{i},v_{i+1} , \dots, v_{j}, \dots , v_k) = \mathcal{T}(v_1, \dots , v_{j},v_{i+1} , \dots, v_{i}, \dots , v_k)\]
\end{definition}

\begin{definition}[Alternating k-tensor]$\mathcal{T}$ is a Alternating k-tensor if $\forall v_i , \dots , v_k \in \mathcal{V}$
\[\mathcal{T}(v_1, \dots , v_{i},v_{i+1} , \dots, v_{j}, \dots , v_k) = -\mathcal{T}(v_1, \dots , v_{j},v_{i+1} , \dots, v_{i}, \dots , v_k)\]
\end{definition}

\begin{example} $V=\RR^2 , \; V^2 = \RR^2 \times \RR^2$
\[\mathcal{T}(v_1, v_2) = v_1^1v_2^2 - v_2^1v_1^2 \qquad \boxed{determinant}\]
\[v_1=(v_1^1, v_1^2) \qquad v_2=(v_2^1, v_2^2)\]
\begin{align*}
\begin{vmatrix} \lambda v_1 + v_1^{'} \\ v_2 \end{vmatrix} &= \lambda\begin{vmatrix} v_1 \\ v_2 \end{vmatrix} + \begin{vmatrix} v_1^{'} \\ v_2 \end{vmatrix} \\
\begin{vmatrix} v_1\\ \lambda v_2 + v_2^{'} \end{vmatrix} &= \lambda\begin{vmatrix} v_1 \\ v_2 \end{vmatrix} + \begin{vmatrix} v_1 \\ v_2^{'} \end{vmatrix}\\
\begin{vmatrix} v_1 \\ v_2 \end{vmatrix} &= -\begin{vmatrix} v_2 \\ v_1 \end{vmatrix}
\end{align*}



$\det$ on $k\times k$-matrices as a function of $k$ vectors in $\RR^k$ is an aternating $k$-tensor. 
\end{example}

\begin{definition}
If $\mathcal{T},\mathcal{S} \in \mathcal{J}^k(V)$, we define:
\begin{align*}
(\mathcal{T} + \mathcal{S})(v_1, \dots , v_k) &= \mathcal{T}(v_1, \dots , v_k) + \mathcal{S}(v_1, \dots , v_k)\\
\intertext{Similarly if $\lambda \in \RR, \; \lambda\mathcal{T} \in \mathcal{J}^k(V)$}
(\lambda\mathcal{T})(v_1, \dots , v_k) &= \lambda\mathcal{T}(v_1, \dots , v_k)\quad \forall v_i \in v
\end{align*}
\end{definition}

\begin{definition}
Let $\mathcal{T} \in \mathcal{J}^k(V), \; \mathcal{S} \in  \mathcal{J}^l{}(V), \quad k,l \in \NN, \quad \mathcal{T}:V^k\rightarrow \RR, \;\mathcal{S}:V^l\rightarrow \RR$. Define $\mathcal{T} \otimes \mathcal{S} \in \mathcal{J}^{k+l}$:
\[\mathcal{T} \otimes \mathcal{S} (v_1,\dots,v_k, v_{k+1},\dots , v_{k+l}) = \mathcal{T}(v_1,\dots,v_k) \cdot \mathcal{S} (v_k,\dots, v_{k+l})\]
\[\mathcal{T}, \mathcal{S} \in \mathcal{J}^{k+l} \Rightarrow \mathcal{T} \otimes \mathcal{S} \neq \mathcal{S} \otimes \mathcal{T}\quad \text{in general}\]
\begin{properties}\quad \\
\begin{enumerate}
\item $\mathcal{T} \otimes \mathcal{S} \in \mathcal{J}^{k+l}$
\item$(\mathcal{S}_1 + \mathcal{S}_2)\otimes\mathcal{T} = \mathcal{S}_1\otimes \mathcal{T} + \mathcal{S}_2 \otimes\mathcal{T}$
\item $\mathcal{S}\otimes (\mathcal{T}_1 + \mathcal{T}_2) = \mathcal{S}\otimes \mathcal{T}_1 + \mathcal{S}\otimes \mathcal{T}_2$
\item $(\lambda\mathcal{S})\otimes\mathcal{T} = \lambda(\mathcal{S}\otimes\mathcal{T}) = \mathcal{S}\otimes(\lambda\mathcal{T})$
\item $(\mathcal{S}\otimes\mathcal{T})\otimes\mathcal{U} = \mathcal{S}\otimes(\mathcal{T}\otimes\mathcal{U})$
\item $\mathcal{J}^1(V) = V^{*}$
\end{enumerate}
\end{properties}
\end{definition}

\begin{theorem}
Let $i_1,\dots , i_k \in \{1,\dots, n\}$, $V$ has a basis $\{v_1, \dots , v_n\}$ $\dim V = n$. Let $\{\varphi_1, \dots , \varphi_n\}$ be the basis basis of $V^{*}, \; \varphi_i(v_i)=\delta_{ij}$. Then
\[\varphi_{i_1}\otimes \cdots \otimes \varphi_{i_k} \quad where \quad \{i_1, \dots ,i_k\} \subseteq \{1, \dots , n\}\]
form basis for $\mathcal{J}^k(V)$
\[\dim(\mathcal{J}^k(V)) = n^k\]
\begin{proof}
Clearly $\varphi_{i_1}\otimes \cdots \otimes \varphi_{i_k} \in \mathcal{J}^k(V)$ since $\varphi_{i_j} \in V^* = \mathcal{J}_1(V)$. The set spans $\mathcal{J}^k(V)$ and is $L.I.$ Let $\mathcal{T} \in \mathcal{J}^k(V).$ need to write
\[\mathcal{T} = \sum_{\substack{
i_1 = 1,\dots , n \\
\vdots\\
i_k = 1,\dots , n 
}} a^{i_1i_2\cdots i_k}\varphi_{i_1}\otimes\cdots\otimes\varphi_{i_k}\]
Plug $(v_{j_1}, \dots , v_{j_k})$ into the suspected identity.
\begin{align*}\mathcal{T}(v_{j_1}, \dots , v_{j_k}) &= \sum\limits_{i_1,\dots , i_k}a^{i_1i_2\cdots i_k}\varphi_{i_1}\otimes\cdots\otimes\varphi_{i_k}(v_{j_1}, \dots , v_{j_k}) \\
&= \sum\limits_{i_1,\dots , i_k}a^{i_1i_2\cdots i_k}\varphi_{i_1}(v_{j_1})\cdots\varphi_{i_k}(v_{j_k})\\
&= \sum\limits_{i_1,\dots , i_k}a^{i_1i_2\cdots i_k}\delta_{i_1 j_1}\cdots\delta_{i_k j_k}\\
&= a^{j_1\cdots j_k} \end{align*}

Define
\[a^{i_1i_2\cdots i_k} = \mathcal{T}(v_{j_1}, \dots , v_{j_k})\]
Let $w_1,\dots , w_k \in V$
\[w_1 = \sum\limits_{j=1}^{n}a^{1_j}v_j \qquad \dots \qquad w_k = \sum\limits_{j=1}^{n}a^{k_j}v_j\]
\begin{align*} \mathcal{T}(w_1 , \dots , w_k) &= \mathcal{T}\left( \sum\limits_{j_1}a^{1_{j_1}}v_{j_1} , \dots , \sum\limits_{j_k}a^{k_{j_k}}v_{j_k} \right)\\
&= \sum\limits_{j_1, \dots, j_k =1}^{n}a^{1_{j_1}}\cdots a^{k_{j_k}}\cdot\mathcal{T}(v_{j_1}, \dots , v_{j_k})\\
&=\sum\limits_{j_1, \dots, j_k}a^{1_{j_1}}\cdots a^{k_{j_k}}\cdot a^{i_1i_2\cdots i_k}
\end{align*}
\begin{align*}  \sum\limits_{i_1,\dots , i_k}a^{i_1\cdots i_k}\varphi_{i_1}\otimes\cdots\otimes\varphi_{i_k}(w_1, \dots , w_k) &=  \sum\limits_{i_1,\dots , i_k}a^{i_1\cdots i_k}\varphi_{i_1}(w_1)\otimes\cdots\otimes\varphi_{i_k}(w_k)\\
&=\sum\limits_{j_1, \dots, j_k}a^{i_1i_2\cdots i_k}\cdot a^{1_{i_1}}\cdots a^{k_{i_k}} \end{align*}
Relabel: \begin{align*} i_1 &\mapsto j_1 \\ &\vdots \\ i_k &\mapsto j_k \end{align*}
So it Spans. Now check L.I.
\[ \sum\limits_{i_1,\dots , i_k=1}^{n}a^{i_1\cdots i_k}\varphi_{i_1}\otimes\cdots\otimes\varphi_{i_k} =0\]
Plus $v_{j_1}, \dots , v_{j_k}$ in to it
\begin{align*} \sum\limits_{i_1,\dots , i_k=1}^{n}a^{i_1\cdots i_k}\varphi_{i_1}\otimes\cdots\otimes\varphi_{i_k}(v_{j_1}, \dots , v_{j_k})  &= \sum\limits_{i_1,\dots , i_k}a^{i_1\cdots i_k}\delta_{i_1 j_1}\cdots\delta_{i_k j_k}\\
&= a^{j_1\cdots j_k} = 0 \end{align*}
put in all possible combinations of basis vectors 
\[\Rightarrow \text{all coeffecitents }\underline{Zero}\]
\end{proof}
\end{theorem}

\subsection{Alternating Tensors}

\begin{remark}
for $f:\RR \rightarrow \RR$. If $f$ is even:
\[f(-x) = f(x)\]
if $f$ is odd:
\[f(-x) = -f(x)\]
Every function can be writen as
\[f = \underset{even}{f_1} + \underset{odd}{f_2}\]
Where
\[f_1 = \frac{f(x) + f(-x)}{2} \qquad f_2=\frac{f(x) - f(-x)}{2}\]
or $\sigma$ is a bijection on $\RR \rightarrow \RR$, $\sigma^2 = id$.
\[x \overset{\sigma}{\mapsto} -x\]
\[\frac{f(x) +  f(\sigma x)}{2}\]
Let $S_k$ be that symmetric group on $k$ letters.
\[S_k \overset{hom}{\rightarrow}\{\pm 1\} \quad \text{multiplicitave group}\]
\[ \sigma \mapsto \begin{cases} +1 & if \; \sigma \; even \\
-1 & if \; \sigma \; odd \end{cases}\]
\[\sigma \mapsto sign(\sigma).\]
\end{remark}

\begin{definition}
If $\mathcal{T} \in \mathcal{J}^k(V)$
\[\alt(\mathcal{T})(w_1, \dots , w_k) = \frac{1}{k!}\sum\limits_{\sigma \in S_k} sgn(\sigma)\mat(w_{\sigma(1)}, \dots , w_{\sigma(k)}).\]
\end{definition}

\begin{example}
$k=2$
\[\alt(\mathcal{T})(w_1 , w_2) = \frac{1}{2!}(\mat(w_1, w_2) - \mat(w_2, w_1))\]
\end{example}

\begin{definition}
The set of alternating $k$-tensors is denoted by $\Lambda^k(V)$, it is a subspace of $\mathcal{J}^k(V)$ 
\end{definition}

\begin{theorem}\quad\\
\begin{enumerate}[(a)]
\item if $\mat \in \mathcal{J}^k(V), \; \alt(\mat) \in \mathcal{J}^k(V)$ and $\alt(T)$ is alternating
\item if $\mathcal{W}$ is alternating, $\alt(\mathcal{W}) = \mathcal{W}$
\item $\alt(\alt(\mat)) = \alt(\mat)$
\end{enumerate}
\begin{proof}
(c) follows from (b), use $\maw=\alt(\mat)$ which is alternating by (a)
\[\alt(\mat) = \maw = \alt(\maw) = \alt(\alt(\mat))\]
Proof of (a):\\
Show $\alt(\mat) \in \mathcal{J}^k(V).$ I will show it is alternating.
\[\alt(\mat)(w_1, \dots, w_i, \dots , w_j , \dots , w_k) = -\alt(\mat)(w_1, \dots , w_j,\dots, w_i , \dots , w_k)\]
\begin{align*}
i &\mapsto j  \\
j &\mapsto i\\
k &\mapsto k \quad \text{if }k\neq i,j \end{align*}

\begin{equation}
 \left.\begin{aligned}
       S_k &\rightarrow S_k \\
        \sigma &\mapsto \sigma(ij) = \sigma^{1} \\
even &\mapsto odd\\
odd &\mapsto  even 
       \end{aligned}
 \right\}
 \qquad \text{Bijection}
\end{equation}

\begin{align*}
\alt(\mat)(w_1, \dots , \overset{i^{th}}{w_j} , \dots , \overset{j^{th}}{w_i} , \dots , w_k) &= \frac{1}{k!}\sum\limits_{\sigma \in S_k}sgn(\sigma)\alt(\mat)(w_{\sigma(1)}, \dots , \overset{i^{th}}{w_{\sigma(j)}} , \dots , \overset{j^{th}}{w_{\sigma(i)}} , \dots , w_{\sigma(k)}) \\
&=  \frac{1}{k!}\sum\limits_{\sigma' \in S_k}sgn(\sigma')\alt(\mat)(w_{\sigma'(1)}, \dots , w_{\sigma'(i)} , \dots , w_{\sigma'(j)} , \dots , w_{\sigma'(k)})\\
&= -\alt(\mat)(w_1, \dots , w_k) \end{align*}

 Proof of (b):\\
Let $\omega$ be alternating.
\begin{align*}
\omega(w_1, \dots , \overset{i^{th}}{w_j} , \dots , \overset{j^{th}}{w_i} , \dots , w_k) &= - \omega(w_1, \dots , \overset{i^{th}}{w_i} , \dots , \overset{j^{th}}{w_j} , \dots , w_k)\\ &=\omega(w_{\sigma(1)} , \dots , w_{\sigma(k)})\\
&=sgn(\sigma)\omega(w_1,  \dots , w_k) \quad \sigma \in S_k \\
&=\alt(\omega)(w_1,  \dots , w_k)\\
&= \frac{1}{k!}\sum\limits_{\sigma \in S_k}sgn(\sigma)\omega(w_{\sigma(1)}, \dots , w_{\sigma(k)})\\
&= \frac{1}{k!}\sum\limits_{\sigma \in S_k}sgn(\sigma)^2 \omega(w_{1}, \dots , w_{k})\\
&=\cancel{\frac{1}{k!}}\cancel{|S_k|}\omega(w_{1}, \dots , w_{k})
\end{align*}
\[\therefore \; \alt(\omega)=\omega\]
\end{proof}
\end{theorem}

\begin{remark}
If $\omega \in \Lambda^k(V), \; \eta \in \Lambda^l(V)$ then 
\[\omega \otimes \eta \in \maj^{k+l}(V)\]
\end{remark}

\begin{definition}
\[\omega \wedge \eta = \frac{(k+l)!}{k!\,l!}\alt(\omega \otimes \eta) \quad \in \Lambda^{k+l}(V)\]
\begin{properties} if $\omega, \;\omega_1,\; \omega_2 \in \Lambda^k(V), \; \eta, \; \eta_1, \; \eta_2 \in \Lambda^l(V)$
\begin{itemize}
\item $(\omega_1 + \omega_2)\wedge\eta = \omega_1\wedge\eta + \omega_2\wedge\eta$
\item $\omega\wedge(\eta_1 + \eta_2)  = \omega\wedge\eta_1 + \omega\wedge\eta_2$
\item $(\alpha\omega)\wedge\eta = \alpha(\omega\wedge\eta) = \omega\wedge(\alpha\eta) \quad \alpha \in \RR$
\item $\omega\wedge\eta = (-1)^{k\cdot l}\eta\wedge\omega$
\end{itemize}
\end{properties}
\end{definition}

\begin{definition}
Let $V,W$ be vector spaces $f:V\rightarrow W$ be a linear transformation. \\ If $\mat$ is a linear functional on $W$, $\mat : W\rightarrow \RR$ then \[\mat \circ f\] is a linear functional on $V$.

\[\begin{diagram}
\node{V} \arrow{e,t}{f}  \arrow{se,b}{\mat \circ f}
\node{W}  \arrow{s,r}{\mat} \\
 \node[2]{\RR}
\end{diagram}\]

\begin{notation}
\[f^{*}(\mat) = \mat \circ f\]
$f^{*}(\mat)$ is called the pullback of $\mat$ by $f$.
\[f^{*}:W^{*} \rightarrow V^{*}\]
by $f^{*}(\mat) = \mat \circ f$
\end{notation}

\end{definition}

\begin{definition}[Pullback of Tensors]
If $\mat$ is a $k$-tensor on $W$ ie $\mat \in \maj^k(W)$ we define the pullback $f^{*} \in \maj^k(V)$ by:
\[f^{*}(\mat)(v_1, \dots , v_k) = \mat(f(v_1), \dots , f(v_k))\]
This is a $k$-tensor on $V$.
\begin{proof}[Need to show linearity in the $i^{th}$ entry]
Let $v_i, v_i' \in V, \; \lambda \in \RR$.
\begin{align*}
f^{*}(\mat)(v_1,\dots,  \lambda v_i + v_i', \dots , v_k) &= \mat(f(v_1), \dots , f(\lambda v_i + v_i'), \dots , f(v_k))\\
&= \mat(f(v_1), \dots , f(\lambda v_i) + f(v_i'), \dots , f(v_k))\\
&= \lambda\mat(f(v_1), \dots , f( v_i), \dots , f(v_k))+ \mat(f(v_1), \dots , f(v_i'), \dots , f(v_k))\\
&=\lambda f^{*}(\mat)(v_1,\dots, v_i, \dots , v_k) + f^{*}(\mat)(v_1,\dots, v_i', \dots , v_k)
\end{align*}
\end{proof}
\end{definition}

\begin{properties}\quad\\
\begin{enumerate}[(a)]
\item \[ f^*(\mat \otimes \mas) = f^*(\mat)\otimes f^*(\mas) \quad \mat \in \maj^k(W), \; \mas \in \maj^l(W)\]
\item \[ f^*(\mat \wedge \mas) = f^*(\mat)\wedge f^*(\mas) \quad \mat \in \Lambda^k(W), \; \mas \in \Lambda^l(W)\]
\end{enumerate}
\end{properties}

\begin{theorem}\quad \\
\begin{enumerate}[(a)]
\item if $\mas \in \maj^k(V), \; \mat \in \maj^l(V)$ and $\alt(\mas)=0$ then 
\[\alt(\mas\otimes\mat) = \alt(\mat\otimes\mas)=0\]
\item if $\omega \in \Lambda^k(V), \; \eta \in \Lambda^l(V), \; \vartheta \in \Lambda^m(V)$ then
\[\alt(\alt(\omega\otimes\eta)\otimes\vartheta) = \alt(\omega\otimes\eta\otimes\vartheta) = \alt(\omega\otimes\alt(\eta\otimes\vartheta) )\]
\item if $\omega \in \Lambda^k(V), \; \eta \in \Lambda^l(V), \; \vartheta \in \Lambda^m(V)$ then
\[(\omega\wedge\eta)\wedge\vartheta = \omega\wedge(\vartheta\wedge\eta) = \frac{(k+l+m)!}{k!\,l!\,m!}\alt(\omega\otimes\eta\otimes\vartheta)\]
\end{enumerate}
\begin{proof}\quad \\
Proof of (a): 
\[\alt(\mas\otimes\mat) = \frac{1}{(k+l)!}\sum\limits_{\sigma \in S_{k+l}}sgn(\sigma)(\mas\otimes\mat)(w_{\sigma(1)}, \dots, w_{\sigma(k)}, w_{\sigma(k+1)}, \dots , w_{\sigma(k+l)})\]
Let $G$ be the subgroup of $S_{k+l}$ such that
\[G= \left\{ \sigma \in S_{k+l} : \begin{aligned} \sigma(k+1) &= k+1\\ &\vdots \\ \sigma(k+l) &= k+l \end{aligned} \right\}\]
The contribution of these to the sum is:
\begin{align*}
&\frac{1}{(k+l)!}\sum\limits_{\sigma \in G}sgn(\sigma)\mas(w_{\sigma(1)}, \dots, w_{\sigma(k)})\cdot\mat( w_{k+1}, \dots , w_{k+l})\\
&=\frac{1}{(k+l)!}k!\,\alt(\mas)(w_1, \dots, w_k)\cdot\mat( w_{k+1}, \dots , w_{k+l})=0
\end{align*}
Let $G\sigma_0$ be a coset of $G$ in $S_{k+l}, \; \sigma_0 \neq 0$
\[G\sigma_0 = \{\sigma'\cdot\sigma_0:\sigma' \in G\}\]
define 
\[(z_1, \dots, z_{k+l}) = (w_{\sigma_0(1)}, \dots , w_{\sigma_0(k+l)})\]
The contribution of these elements is
\begin{align*}
&\frac{1}{(k+l)!}\sum\limits_{\sigma' \in G}sgn(\sigma'\cdot\sigma_0)\mas(z_{\sigma'(1)}, \dots, z_{\sigma'(k)})\cdot\mat(z_{\sigma'(k+1)}, \dots, z_{\sigma'(k+l)})\\
&= \frac{1}{(k+l)!}\sum\limits_{\sigma' \in G}sgn(\sigma')sgn(\sigma_0)\mas(z_{\sigma'(1)}, \dots, z_{\sigma'(k)})\cdot\mat(z_{k+1}, \dots, z_{k+l})\\
&=\frac{1}{(k+l)!}sgn(\sigma_0)\mat(z_{k+1}, \dots, z_{k+l})k!\alt(\mas)(z_1, \dots, z_{k}) =0 
\end{align*}
Proof of (b):
\[\alt(\omega\otimes\eta) - \omega\otimes\eta =\mas\]
\begin{align*}
\alt(\mas) &= \alt(\alt(\omega\otimes\eta) - \omega\otimes\eta) = \alt(\alt(\omega\otimes\eta)) - \alt(\omega\otimes\eta)\\ &= \alt(\omega\otimes\eta) - \alt(\omega\otimes\eta) = 0 \end{align*}
Apply (a) with $\mas$
\[alt(\mas\otimes\vartheta) = 0\]
\[\alt([\alt(\omega\otimes\eta) - \omega\otimes\eta]\otimes\vartheta) = 0\]
\[\alt(\alt(\omega\otimes\eta)\otimes\vartheta) - \alt(\omega\otimes\eta\otimes\vartheta) = 0\]
Proof of (c):
\begin{align*}
(\omega\wedge\eta)\wedge\vartheta &= \frac{((k+l)+m)!}{(k+l)!\,m!}\alt((\omega\wedge\eta)\otimes\vartheta)\\
&=\frac{((k+l)+m)!}{(k+l)!\,m!}\alt(\frac{(k+l)!}{k!\,l!}\alt(\omega\otimes\eta)\otimes\vartheta)\\
&=\frac{((k+l)+m)!}{\cancel{(k+l)!}\,m!}\frac{\cancel{(k+l)!}}{k!\,l!}\alt(\alt(\omega\otimes\eta)\otimes\vartheta)\\
&= \frac{(k+l+m)!}{k!\, l!\,m!}\alt(\omega\otimes\eta\otimes\vartheta)
\end{align*}
\end{proof}
\end{theorem}

\begin{theorem}
Let $dim\, V = n$, then the following is a basis for $\Lambda^k(V)$
\[\varphi_{i_1}\wedge\cdots\wedge\varphi_{i_k}, \quad 1 \leq i_1 < \dots < i_k \leq n. \]
therefore
\[dim\, \Lambda^k(V) = {n \choose k}\] Since choosing a subset of $k$ items from a set of $n$ items and reordering.
\begin{proof}
$\mat \in \Lambda^k(V)$ then $\alt(\mat) = \mat$. Since \[\varphi_{i_1} \otimes \cdots \otimes \varphi_{i_k}\] is a basis for $\maj^k(V)$ we have
\[\mat = \sum\limits_{i_1, \dots , i _k = 1, \dots , n}a^{i_1\cdots i_k}\varphi_{i_1} \otimes \cdots \otimes \varphi_{i_k}\]
Apply $\alt$ on both sides
\[ \mat = \sum\limits_{i_1, \dots , i _k}a^{i_1\cdots i_k}\alt(\varphi_{i_1} \otimes \cdots \otimes \varphi_{i_k})\]
$\alt(\varphi_{i_1} \otimes \cdots \otimes \varphi_{i_k})$ is a multiple of $\varphi_{i_1}\wedge\cdots\wedge\varphi_{i_k}$. Since 
\[\varphi_{i_j}\wedge\varphi_{i_s} = - \varphi_{i_s}\wedge\varphi_{i_j}\]
you can reorder to 
\[\sum\limits_{i_1 < \dots < i_k}\]
So $\varphi_{i_1}\wedge\cdots\wedge\varphi_{i_k}$ with $i_1 <\dots<i_k$ span $\Lambda^k(V)$. It is easy to see that they are L.I.
\end{proof}
\end{theorem}
\begin{example}
$dim\,V = 3$, $k=1$
\[dim\,\Lambda^1(V) = {3 \choose 1} = 3\]
\[\Lambda^1(V) = \maj^1(V) = V^{*}\]
if $\{v_i\}_{i=1,2,3}$ is a basis of $V$, then the dual basis $\varphi_1, \varphi_2, \varphi_3$ is a basis of $\Lambda^1(V)$
\end{example}
\begin{example}
$dim\,V = 3$, $k=2$
\[dim\,\Lambda^2(V) = {3 \choose 2} = 3\]
\[\Lambda^1(V) = \maj^1(V) = V^{*}\]
basis is \[\varphi_1\wedge\varphi_2 ,\; \varphi_1\wedge\varphi_3 \; and \;\varphi_2\wedge\varphi_3\]
\begin{align*}
(\varphi_{1}\wedge\varphi_{2})(w_1,w_2)&= \frac{(1+1)!}{1!\,1!}\alt(\varphi_{1}\otimes\varphi_{2})(w_1,w_2)\\
&=2!\frac{1}{2!}(\varphi_{1}\otimes\varphi_{2}(w_1,w_2) -\varphi_{1}\otimes\varphi_{2}(w_2,w_1))\\
&=\varphi_1(w_1)\cdot\varphi_2(w_2) - \varphi_1(w_2)\cdot\varphi_2(w_1) \\
&=\varphi_{1}\otimes\varphi_{2}(w_1,w_2) - \varphi_{2}\otimes\varphi_{1}(w_1,w_2)\\
&\quad\\
\therefore \; \varphi_1\wedge\varphi_2 & = \varphi_{1}\otimes\varphi_{2} -\varphi_{2}\otimes\varphi_{1}\\
\varphi_1\wedge\varphi_3 & = \varphi_{1}\otimes\varphi_{3} -\varphi_{3}\otimes\varphi_{1}\\
\varphi_2\wedge\varphi_3 & = \varphi_{2}\otimes\varphi_{3} -\varphi_{3}\otimes\varphi_{2}\\
&\quad\\
\therefore \; \varphi_1\wedge\varphi_2 & = \varphi_{1}\otimes\varphi_{2} -\varphi_{2}\otimes\varphi_{1} = - \varphi_2\wedge\varphi_1\\
\varphi_1\wedge\varphi_3 &= -\varphi_3\wedge\varphi_1\\
\varphi_2\wedge\varphi_3 &= -\varphi_3\wedge\varphi_2\\
&\quad\\
(\varphi_1\wedge\varphi_1)(w_1, w_2) &= \varphi_1(w_1)\varphi_1(w_2) - \varphi_1(w_1)\varphi_1(w_2) =0
\end{align*}
\[\therefore \; \boxed{\varphi_1\wedge\varphi_1 = \varphi_2\wedge\varphi_2 = \varphi_3\wedge\varphi_3 = 0}\]
\end{example}
\begin{example}
$dim\,V = 3$, $k=3$
\[dim\,\Lambda^3(V) = {3 \choose 3} = 1\quad basis:\; \varphi_1\wedge\varphi_2\wedge\varphi_3\]
\begin{align*}
(\varphi_1\wedge\varphi_2\wedge\varphi_3)(w_1,w_2,w_3) &= 3!\alt(\varphi_1\otimes\varphi_2\otimes\varphi_3)\\
&=\sum\limits_{\sigma \in S_3}sgn(\sigma)\varphi_1\otimes\varphi_2\otimes\varphi_3(w_{\sigma(1)}, w_{\sigma(2)}, w_{\sigma(3)})\\ 
&=\varphi_1(w_1)\varphi_2(w_2)\varphi_3(w_3) - \varphi_1(w_2)\varphi_2(w_1)\varphi_3(w_3)\\ & \quad - \varphi_1(w_3)\varphi_2(w_2)\varphi_3(w_1) - \varphi_1(w_1)\varphi_2(w_3)\varphi_3(w_2) \\ & \quad + \varphi_1(w_2)\varphi_2(w_3)\varphi_3(w_1) + \varphi_1(w_3)\varphi_2(w_1)\varphi_3(w_2)
\end{align*}
\begin{align*}
\varphi_1\wedge\varphi_2\wedge\varphi_3 &= \varphi_1\otimes\varphi_2\otimes\varphi_3 - \varphi_2\otimes\varphi_1\otimes\varphi_3 - \varphi_2\otimes\varphi_2\otimes\varphi_1\\ & \quad- \varphi_1\otimes\varphi_3\otimes\varphi_2 + \varphi_3\otimes\varphi_1\otimes\varphi_2 + \varphi_2\otimes\varphi_3\otimes\varphi_1
\end{align*}
\end{example}


\subsection{Differential Forms}

\begin{definition}[Tangent Space]\quad \\
\[\begin{diagram}
\node{P}\arrow{n} \arrow{s} \arrow{sse} \arrow[1]{nw} \arrow{sw} \arrow[2]{e}\\
\node{}\\
\node{}
\end{diagram} \]
\[\RR_p^n = \{(p,v) : v \in \RR^n\} \quad \text{This is the tangent space at p}\]
\begin{align*}
(p,v) + (p,w) &= (p,v+w)\\
\lambda(p,v) &= (p,\lambda v) 
\end{align*}
with these opperations $\RR_p^n$ is a vector space.
The opperation $(p,v) + (q,w)$ makes no sense if $p \neq q$. 
\begin{notation} $v_p = (p,v)$ \end{notation}
on $\RR_p^n$ we have:
\[ \langle (p,v), (p,w)\rangle = \langle v,w \rangle \]
\end{definition}

\begin{definition}[Vector Field]
A vector field in $\RR^n$ is a function
\[F:p \rightarrow F(p)\in \RR_p^n\]
\[F(p) = (p,v)\]
\[v=(F^1(p), \dots , F^n(p))\]
\begin{properties} \quad \\
\begin{itemize}
\item if the components $F^i, \; i \in \{1, \dots, n\}$ are continuous, the vector field is continuous
\[F^i:p\rightarrow F^i(p)\]
\item if the components are differentiable then the vector function is differentiable.
\item if $F,G$ are vector fields in $\RR^n$, then $F+G$ is also a vector field in $\RR^n$
\[(F+G)(p) = F(p) + G(p)\]
\item $\lambda F$ is a vector field $\forall \lambda \in \RR$ and 
\[ (\lambda F)(p) = \lambda \cdot F(p)\]
\item if $f:\RR^n \rightarrow \RR$ is a function (continuous and differentiable) then $f\cdot F$ is a new vector field on $\RR^n$.
\[(f\cdot F)(p) = f(p)\cdot F(p)\]
\end{itemize}
\end{properties}
\end{definition}

\begin{definition}[Divergence]
If $F$ is a vector field then its divergence is defined to be:
\[(\dive F )(p) = \sum\limits_{i=1}^{n}D_iF^i(p) \in \RR\]
So $\dive F : \RR^n \rightarrow  \RR$. 
\begin{notation} \[\dive F = \nabla \cdot F\] \end{notation}
\end{definition}

\begin{definition}[k-form]
Given $p \in \RR^n$, let $\omega(p)\in \Lambda^k(\RR_p^k) $
\[\omega(p) = \sum\limits_{i_1<\dots< i_k}\omega_{i_1\cdots i_k}(p)\varphi_{i_1}(p)\wedge\cdots\wedge\varphi_{i_k}(p)\]
it is defined by ${n \choose k}$ functions $p \rightarrow \omega_{i_1\cdots i_k}(p), \; i_1<\cdots <i_k$
\begin{properties}\quad \\
\begin{itemize}
\item if these functions are continuous, then the k-form is continuous. 
\item if these functions are differentiable, then $\omega$ is a differential k-form.
\item if $\omega$ and $\eta$ are differentiable k-forms on $\RR^n$, $\omega + \eta$ is a differentiable k-form on $\RR^n$
\[(\omega + \eta)(p) = \omega(p) + \eta(p)\]
\item if $f:\RR^n \rightarrow \RR$ is a differentiable function then $f\cdot\omega$ is a differentiable k-form.
\[(f\cdot\omega)(p) = f(p)\omega(p)\]
\item if $\omega$ is a differentiable k-form and $\eta$ is a differentiable l-form, then $\omega\wedge\eta$ is a differentiable $(k+l)$- form. 
\[(\omega\wedge\eta)(p) = \omega(p)\wedge\eta(p)\]
\end{itemize}
\end{properties}
\end{definition}

\begin{definition}
Let $f:\RR^n \rightarrow \RR$ be differentiable, then 
\[Df(p):\RR^n \rightarrow \RR\]
is a linear map
\[Df(p) \in (\RR_p^n)^*  = \maj^1(\RR_p^n) = \Lambda^1(\RR_p^n)\]
\end{definition}

\begin{note}
we define the following 1-form
\[df(p) \in \Lambda^1(\RR_p^n)\]
\[df(p)(v_p) = Df(p)(v), \quad v_p= (v,p)\]
Let $f=\pi^i$, the projection into the i-component.
\[\pi^i(x^1, \dots, x^n) = x^i \]
is a linear map, sometimes denoted $x^i(x) = x^i$
\[d\pi^i(p)(v_p) = D\pi^i(p)(v) =\pi^i(p)(v) = \pi^i(v) = v^i\]
But this is the same as $\varphi_i(v)$
\[\therefore \; d\pi^i = \varphi_i = dx^i\]
A differentiable k-form on $\RR^n$ will look like 
\[\omega(p) = \sum\limits_{i_1<\dots< i_k}\omega_{i_1\cdots i_k}(p)dx^{i_1}(p)\wedge\cdots\wedge dx^{i_k}(p)\]
\[\omega = \sum\limits_{i_1<\dots< i_k}\omega_{i_1\cdots i_k}dx^{i_1}\wedge\cdots\wedge dx^{i_k}\]
\end{note}


\begin{example} $\RR^2:(x^1, x^2 ) = (x,y)$ ie $n=2$
\begin{alignat*}{2}
k&=0 &\qquad &\omega = f(x,y)\\
k&=1 &\qquad &\omega = f(x,y)dx + g(x,y)dy\\
k&=2 &\qquad &\omega = f(x,y,z)dx\wedge dy
\end{alignat*}
\end{example}

\begin{example} $\RR^3:(x^1, x^2 , x^3) = (x,y,z)$
\begin{alignat*}{2}
k&=0 &\qquad &\omega = f(x,y,z)\\
k&=1 &\qquad &\omega = f(x,y,z)dx + g(x,y,z)dy + h(x,y,z)dz\\
k&=2 &\qquad &\omega = f(x,y,z)dx\wedge dy + g(x,y,z)dx\wedge dz + h(x,y,z)dy\wedge dz\\
k&=3 &\qquad &\omega = f(x,y,z)dx\wedge dy \wedge dz
\end{alignat*}
\[dx\wedge dx = dy\wedge dy =dz\wedge dz =0\]
\begin{align*}
dx\wedge dy &= -dy \wedge dx\\
dx\wedge dz &= -dz \wedge dx\\
dy\wedge dz &= -dz \wedge dy 
\end{align*}
\end{example}

\begin{theorem}
Let $f:\RR^n \rightarrow \RR$ be differentiable, then the 1-form $df$ is:
\[df = D_1fdx^1 + D_2fdx^2 + \cdots + D_nfdx^n \]
\begin{proof}
$df(p) \in \Lambda^1(\RR_p^n)$
\[df(p)(V_p) = df(p)(v) = (D_1f(p), \dots , D_nf(p))\cdot \left(\! \!\begin{array}{c} v^1\\ \vdots \\ v^n \end{array} \! \! \right) = \sum\limits_{i=1}^{n}D_if(p)v^i \]
Calculate:
\begin{align*}
(D_1fdx^1 + \cdots + D_nfdx^n )(p)(V_p) &= [D_1f(p)dx^1(p)+ \cdots + D_nf(p)dx^n(p)] (V_p)\\
&=D_1f(p)dx^1(p)(V_p)+ \cdots + D_nf(p)dx^n(p)(V_p)\\
&=D_1f(p)v^1+ \cdots + D_nf(p)v^n\\
&=\sum\limits_{i=1}^{n}D_if(p)v^i 
\end{align*}
\end{proof}
\end{theorem}

\begin{example}
for $\RR^3$
\[ df = \frac{df}{dx}dx + \frac{df}{dy}dy + \frac{df}{dz}dz \]
\end{example}

\begin{definition}[The Opperator $d$ on $k$-forms]
\[\omega = \sum\limits_{i_1<\dots <i_k}\omega_{i_1\cdots i_k}dx^{i_1}\wedge\cdots\wedge dx^{i_k}\quad \text{(k-form)}\]
\[d\omega = \sum\limits_{i_1<\dots <i_k}\sum\limits_{\alpha=1}^nD_i\omega_{i_1\cdots i_k}x^\alpha\wedge dx^{i_1}\wedge\cdots\wedge dx^{i_k}\quad \text{(k+1)-form}\]
\end{definition}

\begin{example}
$n=3, \; k=1$
\[\omega  = f\,dx + g\,dy + h\,dz\]
\begin{align*}
d\omega &= \frac{df}{dx}\cancel{dx\wedge dx} + \frac{df}{dy}dy\wedge dx + \frac{df}{dz}dz\wedge dx\\
& \quad + \frac{dg}{dx}dx\wedge dy + \frac{dg}{dy}\cancel{dy\wedge dy} + \frac{df}{dz}dz\wedge dy\\
& \quad + \frac{dh}{dx}dx\wedge dz + \frac{dh}{dy}dy\wedge dz + \frac{dh}{dz}\cancel{dz\wedge dz} \\
&=  \left(\frac{dg}{dx} - \frac{df}{dy} \right) dx\wedge dy + \left(\frac{dh}{dy} - \frac{dg}{dz} \right) dy\wedge dz +\left(\frac{dh}{dx} - \frac{df}{dz} \right) dx\wedge dz  
\end{align*}

\[\begin{vmatrix} i & j & k\\ dx &dy &dz \\ f &g &h \end{vmatrix} =  \left(\frac{dh}{dy} - \frac{dg}{dz} \right)\mathbf{i}  - \left(\frac{dh}{dx} - \frac{df}{dz} \right) \mathbf{j} +  \left(\frac{dg}{dx} - \frac{df}{dy} \right) \mathbf{z} \]
\[\mathbf{i} \leftrightarrow dy\wedge dz \quad \mathbf{j} \leftrightarrow dz \wedge dx \quad \mathbf{k} \leftrightarrow dx \wedge dy\]
\end{example}

\begin{example}
$n=3, \; k=2$
\[\omega  = f \,dy \wedge dz + g\, dz \wedge  dx  + h \,dx \wedge dy\]
\begin{align*} d\omega &= \frac{df^1}{dx}dx\wedge dy \wedge dz + \frac{df^1}{dy}dy\wedge dy \wedge dz + \frac{df^1}{dz}dz\wedge dy \wedge dz\\
& \quad +\frac{df^2}{dx}dx\wedge dz \wedge dx + \frac{df^2}{dy}dy\wedge dz \wedge dx + \frac{df^2}{dz}dz\wedge dz\wedge dx\\
& \quad + \frac{df^3}{dx}dx\wedge dx \wedge dy + \frac{df^3}{dy}dy\wedge dx \wedge dy + \frac{df^3}{dz}dz\wedge dx \wedge dy\\
&= \left( \frac{df^1}{dx} + \frac{df^2}{dy} + \frac{df^3}{dz} \right)dx\wedge dy \wedge dz
\end{align*}
\end{example}

\begin{example} $2$-form $\leftrightarrow F=(f^1, f^2, f^3)$
\[\dive F = \frac{df^1}{dx} + \frac{df^2}{dy} + \frac{df^3}{dz} \leftrightarrow d\omega\]
\end{example}

\begin{example}
$n=3, \; k=3$
\[\omega = f \,dx\wedge dy \wedge dz\] 
\[d\omega = 0 \qquad \text{4-form on }\RR^3 \quad {3 \choose 4}=0\]
\end{example}

\begin{example}
$n=2, \;k=1$
\[\omega = f\, dx + g\,dy\]
\begin{align*}
d\omega &= \frac{df}{dx}dx\wedge dx + \frac{df}{dy}dy\wedge dx + \frac{dg}{dx}dx\wedge dy + \frac{dg}{dy}dy\wedge dy\\
&= \left(\frac{dg}{dx} -  \frac{df}{dy}\right)dx \wedge dy
\end{align*}
\end{example}

\begin{example}
$n=2, \;k=2$
\[\omega = f\, dx\wedge dy\]
\[d\omega =0\]
\end{example}
\begin{example}
$n=1, \;k=1$
\[\omega = f\, dx\]
\[d\omega =0\]
\end{example}

\begin{theorem}\quad \\
\begin{enumerate}[(a)]
\item $d(\omega + \eta) = d(\omega) + d(\eta)$
\item if $\omega$ is a k-form,  $\eta$ is an l-form,  $\omega\wedge\eta$ is a $(k+l)$- form.
\[d(\omega\wedge\eta) = (d\omega)\wedge\eta + (-1)^k\omega\wedge(d\eta)\]
\item $d(d\omega) = 0$
\end{enumerate}

\begin{proof}\quad\\
(c):
\[\omega = \sum\limits_{i_1<\dots <i_k}\omega_{i_1\cdots i_k}dx^{i_1}\wedge\cdots\wedge dx^{i_k}\]

\begin{align*}
d(d\omega) &= d\left(\sum\limits_{i_1<\dots <i_k}\sum\limits_{i=1}^n D_i\omega_{i_1\cdots i_k}x^i\wedge dx^{i_1}\wedge\cdots\wedge dx^{i_k}\right)\\
&=\sum\limits_{i_1<\dots <i_k}\sum\limits_{i=1}^n\sum\limits_{j=1}^n D_j\left( D_i\omega_{i_1\cdots i_k}\right)dx^j\wedge dx^i\wedge dx^{i_1}\wedge\cdots\wedge dx^{i_k}
\end{align*}
if $i=j$ then $dx^i\wedge dx^j =0$, if $i \neq j: \; (i,j) , (j,i)$
\[  D_jD_i\omega_{i_1\cdots i_k}dx^j\wedge dx^i -D_iD_j\omega_{i_1\cdots i_k}\overset{swapped\: in\: i,j}{ dx^j\wedge dx^i}\]
for functions which have continuous mixed partial derivitives we have proved 
\[D_iD_j\omega_{i_1\cdots i_k}= D_jD_i\omega_{i_1\cdots i_k}\]
\[\Rightarrow \text{property }(c)\]
(b):\quad \\
Take
\[\omega\wedge\eta = \sum\limits_{i_1<\dots <i_k} \sum\limits_{j_1<\dots <j_c}\omega_{i_1\cdots i_k}\eta_{j_1\cdots j_c}dx^{i_1}\wedge\cdots\wedge dx^{i_k}\wedge dx^{j_1}\wedge dx^{j_c}\]



\begin{align*} d(\omega\wedge\eta) &= \sum\limits_{i_1<\dots <i_k} \sum\limits_{j_1<\dots <j_c}\sum\limits_{\alpha =1}^{n}D_\alpha(\omega_{i_1\cdots i_k}\eta_{j_1\cdots j_c})dx^\alpha\wedge dx^{i_1}\wedge\cdots\wedge  dx^{i_k}\wedge  dx^{j_1}\wedge dx^{j_c}\\
\intertext{\[= \sum\limits_{i_1<\dots <i_k} \sum\limits_{j_1<\dots <j_c}\sum\limits_{\alpha =1}^{n}((D_\alpha\omega_{i_1\cdots i_k})\eta_{j_1\cdots j_c} + \omega_{i_1\cdots i_k}(D_\alpha\eta_{j_1\cdots j_c}))dx^\alpha\wedge dx^{i_1}\wedge\cdots\wedge  dx^{i_k}\wedge  dx^{j_1}\wedge dx^{j_c}\]} 
&= \sum\limits_{i_1<\dots <i_k} \sum\limits_{j_1<\dots <j_c}\sum\limits_{\alpha =1}^{n}(D_\alpha\omega_{i_1\cdots i_k})\eta_{j_1\cdots j_c}dx^{\alpha}\wedge dx^{i_1}\wedge\cdots\wedge  dx^{i_k}\wedge  dx^{j_1}\wedge dx^{j_c}\\
&\quad + \sum\limits_{i_1<\dots <i_k} \sum\limits_{j_1<\dots <j_c}\sum\limits_{\alpha =1}^{n} \omega_{i_1\cdots i_k}(D_\alpha\eta_{j_1\cdots j_c})\overset{*}{dx^\alpha}\wedge dx^{i_1}\wedge\cdots\wedge  dx^{i_k}\wedge  dx^{j_1}\wedge dx^{j_c}
\intertext{* shift the $dx^\alpha$ term down to one place infront of the $dx^{j_1}$ term.}
&= \left(\sum\limits_{i_1<\dots <i_k}\sum\limits_{\alpha =1}^{n}D_\alpha\omega_{i_1\cdots i_k}dx^{\alpha}\wedge dx^{i_1}\wedge\cdots\wedge  dx^{i_k}\right)\wedge  \left( \sum\limits_{j_1<\dots <j_c}\eta_{j_1\cdots j_c} dx^{j_1}\wedge dx^{j_c}\right) \\
&\quad + \left(\sum\limits_{i_1<\dots <i_k} \omega_{i_1\cdots i_k} dx^{i_1}\wedge\cdots\wedge  dx^{i_k}\right) \wedge(-1)^k\left( \sum\limits_{j_1<\dots <j_c}\sum\limits_{\alpha =1}^{n}D_\alpha\eta_{j_1\cdots j_c}dx^\alpha\wedge  dx^{j_1}\wedge dx^{j_c}\right)\\
&\quad \\
&= d(\omega)\wedge\eta + (-1)^k\omega\wedge d(\eta)
\end{align*}
\end{proof}
\end{theorem}

\subsection{Closed and Exact forms}

\begin{definition}
Let $\omega$ be a $k$-form, $\omega$ is called closed if
\[d\omega = 0\]
\end{definition}

\begin{definition}
Let $\omega$ be a $k$-form, $\omega$ is called exact if $\exists$ a (k-1)-form $\eta$ such that:
\[ d\eta = \omega \]
\end{definition}

\begin{proposition}
If $\omega$ is exact then it is closed.
\begin{proof}
\[d(\omega) = d(d(\eta)) = 0\]
\end{proof}
\end{proposition}

\begin{example}
$n=2,\;k=1$
\[\omega = p(x,y)dx + q(x,y)dy\]
\[d\omega = \left(-\frac{dp}{dy} + \frac{dq}{dx}\right)dx\wedge dy\]
$\omega$ is closed if $\frac{dp}{dy} = \frac{dq}{dx}$
\end{example}

\begin{example}
when $\omega$ is an exact 1- form we have a $\eta = f$ 0-form such that
\[\omega = d\eta = df = \frac{df}{dx}dx + \frac{df}{dy}dy\]
\[grad(f) =  \frac{df}{dx}\mathbf{i} + \frac{df}{dy}\mathbf{j}\]
\end{example}
\begin{definition}
For a vector field 
\[F=P\mathbf{i} + Q\mathbf{j}\]
if
\[\frac{dP}{dy} = \frac{dQ}{dx}\]
we call it a conservative field. Also $F$ is a conservative field if
\[F = grad(f)\]
\end{definition}

\begin{example}
\[\omega = xy^2dx + ydy\]
\begin{align*}
d\omega &= \frac{d(xy^2)}{dy}dy\wedge dx + \frac{d(y)}{dx}dx\wedge dy\\
&= -2xydy\wedge dx \neq 0 \end{align*}
\[\Rightarrow \text{not closed and not exact}\] 
\end{example}

\begin{example}
\[\omega = xy^2dx + x^2ydy\]
\[d\omega = 2xydy\wedge dx + 2xydx\wedge dy = 0 \]
\[\Rightarrow \text{closed}\] 
Is it exact?
\[\exists f=\frac{x^2y^2}{2} +h\]
\[\frac{df}{dx}dx + \frac{df}{dy}dy = xy^2dx + x^2ydy =\omega\]
so it is also exact.
\end{example}

\begin{proposition}
If $n>2, \; k=1$ 
\[\omega = \omega_1dx^1 + \cdots + \omega_ndx^n\]
is closed. Is it exact? i.e does there exist $f$ st
\[\omega = df = D_1fdx^1 + \cdots + d_nfdx^n\]
$ \omega_i = D_if$ and can assume $f(0)=0$ you can recover the $f$ by integration in one variable t. $f:\RR^n \rightarrow \RR$.
\[f(x) -\cancel{f(0)} = \int\limits_{0}^1\frac{d}{dt}[f(tx)]dt\]
\begin{align*}
f(x) &= \int\limits_{0}^1\sum\limits_{\alpha=1}^nD_\alpha f(tx)\frac{d}{dt}(tx^\alpha)dt\\
 &= \int\limits_{0}^1\sum\limits_{\alpha=1}^nD_\alpha f(tx)x^\alpha dt\\
&= \int\limits_{0}^1\sum\limits_{\alpha=1}^n\omega_\alpha(tx)x^\alpha dt
\end{align*}
\end{proposition}

\begin{definition}[Star-Shaped region]  
$A$ is a star-shaped region with respect to 0 (or p) if  $\forall t \in [0,1], \; \forall x \in A$
 \[t\cdot x \in A\]
i.e.\[p +t(x-p) \in A\]
\end{definition}

\begin{lemma}[Poincar\'{e} Lemma]
If $A$ is star-shaped with respect to 0 and $\omega$ is a closed form on $A$ then $\omega$ is an exact form on $A$. 
\begin{proof}
For any $l$form $\omega$, i will define an (l-1)-form $I(\omega)$ such that 
\begin{itemize}
\item \[I(\lambda\omega_1 + \omega_2) = \lambda I(\omega_1) + I(\omega_2)\]
\item \[I(0) =0\]
\item \[\underbracket[1pt][2pt]{d(\underbracket[1pt][2pt]{I(\omega)}_{l-1})}_{l} +\underbracket[1pt][2pt]{I(\underbracket[1pt][2pt]{d(\omega)}_{l+1})}_{l} = \omega\]
\end{itemize}
Then if $\omega$ is closed, $d\omega =0$ so $I(d\omega)=0$ so we get 
\[d(I(\omega)) = \omega \Rightarrow \omega \; is \; exact\]
\[\omega = \sum\limits_{i_1<\dots <i_l}\omega_{i_1\cdots i_l}dx^{i_1}\wedge\cdots\wedge dx^{i_l}\]
\[I(\omega) = \sum\limits_{i_1<\dots <i_l}\sum\limits_{\alpha=1}^{l}(-1)^{\alpha-1}\int\limits_{0}^1 t^{l-1}\omega_{i_1\cdots i_l}(tx)x^{i_\alpha}dt\, dx^{i_1}\wedge\cdots\wedge \hat{dx^{\alpha}}\wedge\cdots\wedge dx^{i_l}\]
\[I(0)=0 \qquad \text{trivial to show}\]
\[I(\lambda\omega_1 + \omega_2) = \lambda I(\omega_1) + I(\omega_2) \qquad \text{trivial to show}\]
\end{proof}
\end{lemma}

\begin{definition}
The set $I^k=[0,1]^k$ is called the standard $k$-cube. A continuous map $C:I^k \rightarrow A$, where $A$ is open in $\RR^n$ is called a singular $k$-cube. 
\end{definition}

\begin{example}
$k=1$, $C:[0,1] \rightarrow A$ (curve)
\end{example}

\begin{example}
$K=2$, $C:[0,1]^2 \rightarrow A$ parameterisation of a curve/surface.
\end{example}

\begin{example}$k=0$, $[0,1]^0 = \{0\}$ A singular 0-cube, $\{0\} \rightarrow A$ maps to a point on some surface. 
\end{example}

\begin{example}$k=1$, $\partial(I^1) = +1 , -0$ The motivation for this is $\int_{0}^1 f'(x) dx = f(1) - f(0)$
\end{example}

\begin{example}
$k=2$ $I^2 = [0,1]^2$

\xymatrix{
(0,1) \ar[d]_{-\gamma_4} 
& (1,1) \ar[l]_{-\gamma_3}  \\
(0,0)\ar[r]_{\gamma_1} & (1,0)\ar[u]_{\gamma_2 } }

\xymatrix{
. \ar[r]^{\gamma_3} 
& .  \\
.\ar[r]_{\gamma_1} \ar[u]^{\gamma_4} & .\ar[u]_{\gamma_2 }   }
\[\partial(I^2) = \gamma_1 + \gamma_2 - \gamma_3 - \gamma_4\]
\begin{alignat*}{2}
\gamma_1 &\qquad I_{(2,0)}^2=\{(x,0) , 0\leq x \leq 1\}\\
\gamma_2 &\qquad I_{(1,1)}^2=\{(1,y) , 0\leq y \leq 1\}\\
\gamma_3 &\qquad I_{(2,1)}^2=\{(x,1) , 0\leq x \leq 1\}\\
\gamma_4 &\qquad I_{(1,0)}^2=\{(0,y) , 0\leq y \leq 1\}
\end{alignat*}
\begin{note}
$I_{(a,b)}^n$ means: fix $a^{th}$ variable and set $a^{th}$ variable to $b$. \end{note}
\end{example}

\begin{example}
$k=3$

\xymatrix@!0{
&.  \ar@{-}[rr]\ar@{-}'[d][dd]
& &.  \ar@{-}[dd]
\\
. \ar@{-}[ur]\ar@{-}[rr]\ar@{-}[dd]
& & . \ar@{-}[ur]\ar@{-}[dd]
\\
& .\ar@{-}'[r][rr]
& & .
\\
. \ar@{-}[rr]\ar@{-}[ur]
& & . \ar@{-}[ur]
}


\begin{alignat*}{2}
top: &\qquad I_{(3,1)}^3=\{(x,y,1) , 0\leq x \leq 1, \; 0 \leq y \leq 1\}\\
base: &\qquad I_{(3,0)}^3=\{(x,y,0) , 0\leq x \leq 1, \; 0 \leq y \leq 1\}\\
front: &\qquad I_{(1,1)}^3=\{(1,y,z) , 0\leq y \leq 1, \; 0 \leq z \leq 1\}\\
back: &\qquad I_{(1,0)}^3=\{(0,y,z) , 0\leq y \leq 1, \; 0 \leq z \leq 1\}\\
left: &\qquad I_{(2,0)}^3=\{(x,0,z) , 0\leq x \leq 1, \; 0 \leq z \leq 1\}\\
right: &\qquad I_{(2,1)}^3=\{(x,1,z) , 0\leq x \leq 1, \; 0 \leq z \leq 1\}
\end{alignat*}
\[\therefore \partial I^3 = I_{(3,1)}^3 - I_{(3,0)}^3 + I_{(1,1)}^3 - I_{(1,0)}^3 + I_{(2,0)}^3 - I_{(2,1)}^3\]
\end{example}

\begin{definition}
Give an n-cube $I^n = [0,1]^n$, we define the various faces of it to be 
\begin{align*}
I_{(i,0)}^n &= \{(x^1, x^2, \dots , x^{i-1}, 0 , x^{i+1}, \dots , x^n), 0 \leq x^j \leq 1\} \\
I_{(i,1)}^n &= \{(x^1, x^2, \dots , x^{i-1}, 1 , x^{i+1}, \dots , x^n), 0 \leq x^j \leq 1\}
\end{align*}
We define the boundry of $I^n$ to be 
\[\partial I^n = \sum_{i=1}^n\sum_{\alpha=0}^{1}(-1)^{i+\alpha} I_{(i,\alpha)}^n \]
\end{definition}

We form forma sums of singular n-cubes with integer coefficents. (This is the construction of a certain abelian group, or $\ZZ$-module.)
\begin{example}
\begin{align*}
C_1&:I^1 \rightarrow A\\
C_2&:I^1 \rightarrow A
\end{align*}
\[3C_1 + (-5)C_2 \]
These are singular n-chains 
\end{example}

\begin{definition}
A singular n-chain, $C \in A$, is a finite linear combination of singular n-cubes with integer coefficents. 
\[ C = \sum_{j=1}^{m} m_j C_j \quad m_j \in \ZZ, \; C_j:I^n \rightarrow A \]
\end{definition}

\begin{definition}
If $C$ is a singular n-cube, $C:I\rightarrow A$. Then
\[\partial C = \sum_{i=1}^{n}\sum_{\alpha=0}^{1} (-1)^{i+\alpha}C(I_{(i,\alpha)}^{n})\]
for a singular n-chain 
\[ C = \sum_{j=1}^{m} m_j C_j \]
where $C_j$ are singular n-cubes
\[ \partial C = \sum_{j=1}^{m} m_j \partial (C_j) \]
\end{definition}

\quad \\
In  $\RR^k$ we will define integration of a $k$-form on a $k$-cube and a $k$-form on a $k$-cube face.\\


Let $\omega$ be a k-form on $I^k$
\[\forall p \in I^k : \omega(p) \in \Lambda^k(\RR_p^k)\]
\[\omega = f(x^1, \dots, x^k)dx^1 \wedge \cdots \wedge dx^k \]
\begin{align*}
\int\limits_{I^k} \omega &= \int\limits_{I^k} f(x^1, \dots, x^k)dx^1 \wedge \cdots \wedge dx^k \\
&= \int\limits_{[0,1]^k} f(x^1, \dots, x^k)dx^1 \wedge \cdots \wedge dx^k \quad (Riemann \; Integral)\\
\intertext{By Fubini}
&= \int_0^1\left( \int_0^1\left( \cdots \left( \int_0^1 f(x^1, \dots, x^k)dx^1 \right) dx^2 \right) \cdots \right) dx^k\\
\intertext{this can be evaluated}
\end{align*}

On $\RR^k$, let $\eta$ be a k-1 form on $I_{(i,\alpha)}^k$. The basis of k-1 forms in $\RR^k$ is 
\[dx^1 \wedge \cdots \wedge \hat{dx^j} \wedge \cdots\wedge dx^k \quad j = 1, \dots k \]
Assume $\eta$ is given by: 
\[\eta = g(x^1 , \dots , x^k) dx^1 \wedge \cdots \wedge \widehat{dx^j} \wedge \cdots\wedge dx^k \]
\[\int\limits_{I_{(i,\alpha)}^k} \eta = \begin{cases}\int\limits_{[0,1]^k}\! \!  g(x^1 , \dots, x^{j-1}, \alpha, x^{j+1}, \dots , x^k) dx^1 \wedge \cdots \wedge \widehat{dx^j} \wedge \cdots\wedge dx^k  &if \, i=j \\
0 & if \, i \neq j \end{cases}\]

\begin{example}

\[\xymatrix{
. \ar@{..}[r] 
& .  \\
.\ar[r] \ar@{..}[u] & .\ar@{..}[u]   } \qquad \int\limits_{I_{(2,0)}^k} dy =0 \]
\[\xymatrix{.   \\
. \ar[u]  } \qquad \int\limits_{I_{(1,0)}^k} dx =0 \]
\end{example}

If 
\begin{align*}
\eta &= \sum_{j=1}^n g_j (x^1 , \dots , x^k) dx^1 \wedge \cdots \wedge \widehat{dx^j} \wedge \cdots\wedge dx^k \\
\intertext{then}
\int\limits_{I_{(i,\alpha)}^k} \eta &= \sum_{j=1}^n \int\limits_{I_{(i,\alpha)}^k} g_j (x^1 , \dots , x^k) dx^1 \wedge \cdots \wedge \hat{dx^j} \wedge \cdots\wedge dx^k 
\end{align*}

If $\omega$ is a 0-form, then $\omega$ is a function $f(x^1, \dots, x^k)$, 0-cube is the point $\{0\}$. Then
\[\int\limits_{I^0} \omega = f(0,\dots , 0)\] 

If 
\[ C = \sum_{j=1}^{m} m_j C_j \]
where $C_j$ are all k-cubes standard, then 
\[ \int\limits_{C}\omega  = \sum_{j=1}^{m} m_j \int\limits_{C_j} \omega \]
If 
\[ C = \sum_{j=1}^{m} m_j C_j \]
where $C_j$ are all k-1 cubes, then  
\[ \int\limits_{C}\eta  = \sum_{j=1}^{m} m_j \int\limits_{C_j} \eta \]

\[I^1 = \xymatrix{
{}_0\ar@{|-|}[r] 
& {}_1} \quad \int\limits_{I^1} \omega, \quad \int\limits_{5I^1} \omega = 5\int\limits_{I^1} \omega\]
\xymatrix{
. \ar@{-}[r]^{I_{(2,1)}^2}
& .  \\
.\ar@{-}[r]_{I_{(2,0)}^2} \ar@{-}[u]^{I_{(1,0)}^2} & .\ar@{-}[u]_{I_{(1,1)}^2}   }
\[\int\limits_{\partial I^2} \overbrace{\eta}^{1-form} = + \int\limits_{ I_{(2,0)}^2} \eta  + \int\limits_{ I_{(1,1)}^2} \eta  - \int\limits_{ I_{(2,1)}^2} \eta -\int\limits_{ I_{(1,0)}^2} \eta \]

\begin{lemma}[poincar\'{e} lemma] 
If $A$ is star-shaped with respect to 0 and $\omega$ is a closed $l$-form on $A$, then $\omega$ is exact
\[\omega = \sum\limits_{i_1 < \dots < i_l}\omega_{i_1 \cdots i_l} dx^{i_1} \wedge \cdots \wedge dx^{i_l}\]
\[I(\underbrace{\omega}_{\mathclap{l-form}}) = \sum\limits_{i_1 < \dots < i_l} \sum\limits_{\alpha}^{l} (-1)^{\alpha-1} \int_0^1 t^{l-1} \omega_{i_1 \cdots i_l} (tx) dt x^{i_\alpha} dx^{i_1} \wedge \cdots \wedge \widehat{dx^{i_\alpha}}  \wedge \cdots \wedge dx^{i_l}\]
\end{lemma}
\begin{align*}
I(0) &= 0 \\
I(\omega_1 + \omega_2) &= I(\omega_1) + I(\omega_2)
\intertext{\[\boxed{\text{if $\omega$ is closed}, \; d\omega = 0 }\]}
dI(\omega) + I(d\omega) &= \omega \quad (unproved)\\
\intertext{If $\omega$ is closed $d\omega = 0$, so}
dI(\omega) + \cancel{I(0)} &= \omega \qquad \boxed{\omega\; is \; exact}
\end{align*}
Because $I$ is a linear opperator, it suffices to prove it for
\[ \omega = f(x^1, \dots, x^n) dx^{i_1} \wedge \cdots \wedge dx^{i_l}\]
\[ d\omega = \sum_{\beta = 1}^{n} D_{\beta}[f(x^1, \dots, x^n)] dx^{\beta} \wedge dx^{i_1} \wedge \cdots \wedge dx^{i_l}\]


\begin{align*}
I(\omega) &=  \sum\limits_{\alpha}^{l} (-1)^{\alpha-1} \int_0^1 t^{l-1} f(tx) dt\, x^{i_\alpha} dx^{i_1} \wedge \cdots \wedge \widehat{dx^{i_\alpha}}  \wedge \cdots \wedge dx^{i_l}\\
d(I(\omega)) &= \sum\limits_{\beta=1}^{n} \sum\limits_{\alpha}^{l} (-1)^{\alpha-1} \int_0^1 t^{l-1}D_\beta(f(tx)x^{i_\alpha}) dt \, dx^{i_1} \wedge \cdots \wedge \widehat{dx^{i_\alpha}}  \wedge \cdots \wedge dx^{i_l}\\
&=\sum\limits_{\beta=1}^{n} \sum\limits_{\alpha}^{l} (-1)^{\alpha-1} \int_0^1 t^{l-1} \left(\delta_{\beta_1,i_\alpha}f(tx) +x^{i_\alpha}(D_\beta f)(tx)\cdot t \right)dt\,dx^\beta \wedge dx^{i_1} \wedge \cdots \wedge \widehat{dx^{i_\alpha}}  \wedge \cdots \wedge dx^{i_l}\\
&= \sum\limits_{\alpha}^{l} (-1)^{\alpha-1} \int_0^1 t^{l-1}f(tx) dt\,dx^{i_\alpha} \wedge dx^{i_1} \wedge \cdots \wedge \widehat{dx^{i_\alpha}}  \wedge \cdots \wedge dx^{i_l}\\
&\quad +\sum\limits_{\beta=1}^{n} \sum\limits_{\alpha}^{l} (-1)^{\alpha-1} \int_0^1 t^{l}x^{i_\alpha}(D_\beta f)(tx) dt\,dx^\beta \wedge dx^{i_1} \wedge \cdots \wedge \widehat{dx^{i_\alpha}}  \wedge \cdots \wedge dx^{i_l}\\
&= l \int_0^1 t^{l-1}f(tx) dt\, dx^{i_1} \wedge \cdots \wedge \underbrace{dx^{i_\alpha}}_{put \;back}  \wedge \cdots \wedge dx^{i_l}\\
(1) \quad &\quad+\sum\limits_{\beta=1}^{n} \sum\limits_{\alpha}^{l} (-1)^{\alpha-1} \int_0^1 t^{l}x^{i_\alpha}(D_\beta f)(tx) dt\,dx^\beta \wedge dx^{i_1} \wedge \cdots \wedge \widehat{dx^{i_\alpha}}  \wedge \cdots \wedge dx^{i_l}\\
\intertext{}
I(d\omega) &= \sum\limits_{j \in \{\beta, i_1, \dots, i_l\}} \sum\limits_{\beta}^{n} (-1)^{j-1} \int_0^1 t^{l} D_\beta f(tx) dt \, x^j\left(dx^{\beta}\wedge dx^{i_1} \wedge \cdots \wedge \widehat{dx^{j}}  \wedge \cdots \wedge dx^{i_l}\right)\\
&=  \sum\limits_{\beta}^{n} \int_0^1 t^{l} (D_\beta f)(tx) dt \,  x^{\beta} dx^{i_1} \wedge \cdots \wedge dx^{j}  \wedge \cdots \wedge dx^{i_l}\\
(2) \quad &\quad + \sum\limits_{\alpha =1}^{l} \sum\limits_{\beta}^{n} (-1)^{\alpha} \int_0^1 t^{l} (D_\beta f)(tx) dt \, x^\alpha \,dx^{\beta}\wedge dx^{i_1} \wedge \cdots \wedge \widehat{dx^{i_\alpha}}  \wedge \cdots \wedge dx^{i_l}
\end{align*}
(1) cancels (2). 
\begin{align*}
dI(\omega) + I(d\omega) &= l \int_0^1 t^{l-1}f(tx) dt\, dx^{i_1} \wedge \cdots \wedge dx^{i_\alpha} \wedge \cdots \wedge dx^{i_l}\\
&\quad + \sum\limits_{\beta}^{n} (-1)^{j-1} \int_0^1 t^{l} (D_\beta f)(tx) dt \,  x^{\beta} dx^{i_1} \wedge \cdots \wedge dx^{j}  \wedge \cdots \wedge dx^{i_l}\\
 &=\left(\int_0^1 \left[ l t^{l-1}f(tx) + \sum\limits_{\beta}^{n}  t^{l}x^{\beta} (D_\beta f)(tx)\right] dt \right) dx^{i_1}  \wedge\cdots \wedge dx^{i_l}\\
 &=\int_0^1\frac{d}{dt} \left( t^l f(tx) \right)dt \, dx^{i_1} \cdots \wedge \wedge dx^{i_l}\\
 &= \left. t^l f(tx) \right|_{t=0}^{t=1} \, dx^{i_1} \cdots \wedge \wedge dx^{i_l}\\
&=  f(1\cdot x)dx^{i_1}  \wedge \cdots \wedge dx^{i_l} -0 \\
&= \omega
\end{align*}

\subsection{Stoke's Theorem}

\begin{theorem}[Stoke's Theorem]
\[\int\limits_{\partial C} \omega = \int\limits_{C} d\omega\]
where $\omega$ is a (k-1)-form, $d\omega$ is a k-form, $\partial C$ is a (k-1) singular chain and $C$ is a k-singular chain. 
\begin{proof}[Proof of stokes theorem on $\RR^k$ for $\omega$ k-1 form]
$C=I^k$ standard k-cube
\[\int\limits_{\partial I^k} \omega = \int\limits_{I^k} d\omega \]
we know $\int\limits_{C} \eta$ is linear in $\eta$ , ie 
\[\int\limits_{C} \lambda\eta_1 + \eta_2 =  \lambda \int\limits_{C} \eta_1 + \int\limits_{C} \eta_2 \]
therefore it suffices to prove it for: 
\begin{align*}
\omega &= f(x^1, \dots , x^k) dx^1 \wedge \cdots \wedge \widehat{dx^j}\wedge \cdots  \wedge dx^k\\
d\omega &= \sum\limits_{\beta=1}^k D_\beta f(x^1, \dots , x^k)dx^\beta \wedge dx^1 \wedge \cdots \wedge \widehat{dx^j}\wedge \cdots  \wedge dx^k\\
 &= D_j f(x^1, \dots , x^k)dx^j \wedge dx^1 \wedge \cdots \wedge \widehat{dx^j}\wedge \cdots  \wedge dx^k\\
 &=(-1)^{j-1} D_j f(x^1, \dots , x^k)  dx^1 \wedge \cdots \wedge dx^j\wedge \cdots  \wedge dx^k
\end{align*}

\begin{align*}
 \int\limits_{I^k} d\omega &= (-1)^{j-1}\int\limits_{I^k} D_j f dx^1 \wedge \cdots \wedge dx^k \\
&=(-1)^{j-1}\int\limits_{[0,1]^k} D_j f dx^1  \cdots  dx^k \\
&=(-1)^{j-1}\int\limits_{0}^1\cdots \int\limits_{0}^1 \left( \int\limits_{0}^1 D_j f dx^j \right) dx^1  \cdots  \widehat{dx^j}\cdots dx^k \\
&=(-1)^{j-1}\int\limits_{0}^1\cdots \int\limits_{0}^1 \left. f(x^1, \dots, x^k) \right|_{x^j=0}^{x^j=1} dx^1  \cdots  \widehat{dx^j}\cdots dx^k \\
&=(-1)^{j-1}\int\limits_{0}^1\cdots \int\limits_{0}^1  f(x^1, \dots,x^{j-1}, 1, x^{j+1}, \dots,  x^k)  dx^1  \cdots  \widehat{dx^j}\cdots dx^k \\
&\quad -(-1)^{j-1}\int\limits_{0}^1\cdots \int\limits_{0}^1  f(x^1, \dots,x^{j-1}, 0, x^{j+1}, \dots,  x^k)  dx^1  \cdots  \widehat{dx^j}\cdots dx^k 
\intertext{}\int\limits_{\partial I^k} \omega &= \sum_{i=1}^{k} \sum_{\alpha=0,1} (-1)^{i+\alpha} \int\limits_{I_{(i,\alpha)}^k} \overbrace{\omega}^{k-1 \; form} \\
only\, i=j \, remaining \; \; \; &=  \sum_{\alpha=0,1} (-1)^{j+\alpha} \int\limits_{[0,1]^{k-1}} f(x^1, \dots, x^{j-1}, \alpha, x^{j+1}, \dots, x^k)dx^1 \cdots \widehat{dx^j} \cdots dx^k \\
 &=  (-1)^{j+1} \int\limits_{[0,1]^{k-1}} f(x^1, \dots, x^{j-1}, 1, x^{j+1} , \dots,  x^k)dx^1 \cdots \widehat{dx^j} \cdots dx^k \\
&+  (-1)^{j} \int\limits_{[0,1]^{k-1}} f(x^1, \dots, x^{j-1}, 0, x^{j+1} , \dots,  x^k)dx^1 \cdots \widehat{dx^j} \cdots dx^k \\
\end{align*}
\end{proof}
\end{theorem}

\begin{definition}[Pullback  $C^*(\omega)$]
If $\omega$ is a k-form on $A$ containing a singular k-cube $C$, $(C:I^k \rightarrow A)$, then:
\[ \int\limits_{C} \omega = \int\limits_{I^k} C^*(\omega)\]
\end{definition}

How do we define the pullback of a k-form on $C$?
\begin{remark}
If $S$ is linear, $ S^*(T) = T \cdot S$ linear
\[
\begin{diagram}
\node{V} \arrow{e,t}{S}  
\node{W}  \arrow{s,r}{T} \\
 \node[2]{\RR}
\end{diagram}\qquad V,W \; vector\; spaces, \; T\;linear\; functional
\]
\end{remark}

Recall the Pullback of Tensors: $T \in \maj^k(W)$, then $S \in \maj^k(W)$
\[S^*(T)(v_1, \dots, v_k) = T(S(v_1), \dots, S(v_k)), \quad v_i \in V\]
Let $\omega$ be a k-form on $\RR^m$ and let $f:\RR^n \rightarrow \RR^m$ $f^*(\omega)(p) \in \Lambda^k(\RR_{p}^n)\quad \forall p \in \RR^n$
\[f^*(\omega)(p)(\underset{v_i \in \RR_{p}^n}{v_1, \dots, v_k}) = \omega(\underset{\mathclap{\in \Lambda^k(\RR_{f(p)}^m)}}{f(p)})(Df(v_1), \dots, Df(v_k))\]
Let $f:\RR^n \rightarrow \RR^m$ be differentiable, $p \in \RR^n$, $Df(p):\RR^n \rightarrow \RR^n$ linear map. It helps us the push-forward of $\RR_{p}^k$ to $\RR_{f(p)}^m$ If $v_{p} \in \RR_p^n, \; v_p = (p,v), \; v \in \RR^n$, then $F_*(v_p) \in \RR_{f(p)}^m$ defined by
\[f_*(v_p) = (f(p), Df(p)(v))\]



\begin{proposition}$F_*(v_p):\RR_p^n \rightarrow \RR_{f(p)}^m$ is linear.
\begin{proof}
 If $v_p, w_p \in \RR_p^n, \; \lambda \in \RR$ 
\begin{align*}
f_*(\lambda v_p + w_p) &= f_*(\lambda (p,v) + (p,w)) = f_*((p ,\lambda ,v ,w)) = (f(p), Df(p)(\lambda v +w))\\
&= (f(p), \lambda Df(p)( v) +Df(p)(w)) = \lambda (f(p), Df(p)( v) ) + (f(p), Df(p)(w) )\\
&=\lambda f_*(v_p) + \lambda f_*(w_p)
\end{align*}
\end{proof}
\end{proposition}

\begin{definition}
If $T \in \maj^k(\RR_{f(p)}^m)$ then $f^*(T) \in \maj^k(\RR_{p}^k)$ will be defined by
\[f^*(\omega)(p)(v_1, \dots, v_k) = \omega(f(p))(f_*(v_1), \dots, f_*(v_k)) \quad \forall p \in \RR^n, \; v_i \in \RR_p^n\]
\end{definition}

\begin{theorem}
$f:\RR^n \rightarrow \RR^m$ differentiable
\begin{enumerate}[(i)]
\item $f^*(dx^i) = \sum_{j=1}^n D_jf^idx^j$
\item $f^*(\lambda w_1 + w_2) =\lambda f^*( w_1) + f^*( w_2)$
\item $f^*(g\cdot \omega) = (g \circ f) f^*(\omega)$
\item $f^*(\omega \wedge \eta) = f^*(\omega) \wedge f^*(\eta)$
\end{enumerate}
\begin{proof}\quad \\
Proof of (i): 
\[f^*(dx^i) = \sum_{j=1}^n D_jf^i dx^j \]
Take $p \in \RR^n$, $f^*(dx^i)(p) \in \Lambda^1(\RR_{p}^m)$
\[f^*(dx^i)(p)(v_p) = dx^i(f(p))(f_*(v_p)) = dx^i(f(p))(f(p),Df(p)(v))\]
$dx^i$ picks up the $i^{th}$ component of the vector:
\[(f(p),Df(p)(v))^i = \sum_{j=1}^n D_jf^i(p)v_j\]
$f:\RR^n \rightarrow \RR^m$ 
\[Df(p)(v) = f'(p)\cdot v = \left( \begin{array}{ccc} D_1f^1 &\dots &D_nf^1\\
\vdots &  &\vdots \\
\hline
& i^th \; row\\
\hline\\
\vdots &  &\vdots\\
 D_1f^m &\dots &D_nf^m
\end{array} \right) \cdot \begin{pmatrix} v_1 \\
\vdots \\ \vdots \\ \vdots \\
v_n \end{pmatrix} = \sum_{j=1}^n D_jf^i v_j\]
compute:
\[\left( \sum_{j=1}^n D_jf^i dx^i\right)(p)(v_p) =  \sum_{j=1}^n D_jf^i(p) \underbracket{dx^j}_{\mathclap{Picks \; up\; j^{th} \; component}} (p)(v_p) =  \sum_{j=1}^n D_jf^i(p) v_j\]
Proof of (iii):\quad \\
\[F^*(g\cdot \omega) = g \circ f \cdot f^*(\omega)\]
$f:\RR^n \rightarrow \RR^m$, $g:\RR^m\rightarrow \RR$, $p \in \RR^n$, $v_1, \dots , v_k \in \RR_{p}^n$
\begin{align*}
F^*(g\cdot \omega)(p)(v_1, \dots, v_k) &= (g\omega)(f(p))(f_*(v_1), \dots , f_*(v_k))\\
&= g(f(p)) \cdot \omega(f(p))(f_*(v_1), \dots , f_*(v_k))
\end{align*}
Compute:
\[g \circ f \cdot f^*(\omega)(p)(v_1, \dots, v_k) = (g\circ f)(p) \omega (f(p))(f_*(v_1), \dots , f_*(v_k))\]
\end{proof}
\end{theorem}

\begin{example}
$\omega$ a 1-form in $\RR^3$
\[\omega = P(x,y,z)dx + Q(x,y,z)dy + R(x,y,z) dz\]
$f:[0,1] \rightarrow\RR^3$ (parameterises curve in $\RR^3$) $f^*(\omega)$ 1-form on $[0,1]$\\
Let $v_t$ be a tangent vector on $\RR_t^1$, $v_t = (t,v)$
\begin{align*}
f^*(\omega)(t)(v_t) & = \omega(f(t))(f_*(v_t))\\
&= (Pdx + Qdy +Rdz)(f(t))(f_*(v_t))\\
&= P(f(t))dx(f(t))(f_*(v_t))+ Q(f(t))dy(f(t))(f_*(v_t)) +R(f(t))dz(f(t))(f_*(v_t))
\intertext{
\begin{framed}
\! \! \! \! \! \! \! \! \!  \begin{align*}
 f^*(v_t) &= (f(t), Df(t)(v))\\
&= (f(t), (Df^{1}(t)(v), Df^{2}(t)(v) , Df^{3}(t)(v))) \\
& \qquad \qquad  f=f^1 + f^2 + f^3 
\end{align*}  \end{framed}}
&=P(f(t))Df^1(t)(v) + Q(f(t))Df^2(t)(v)  + R(f(t))Df^3(t)(v) \\
\end{align*}
\[\Rightarrow f^*(\omega)= (P\circ f)\frac{df^1}{dt}dt + (Q\circ f)\frac{df^2}{dt}dt + (R\circ f)\frac{df^3}{dt}dt\]
\begin{align*}
f^*(\omega) &= f^*(Pdx +Qdy +Rdz) \\
&= (P\circ f)f^*(dx) + (Q\circ f)f^*(dy) + (R\circ f)f^*(dz)\\
&= (P\circ f)Df^1 dt + (Q\circ f)Df^2 dt + (R\circ f)Df^3 dt
\end{align*}
\end{example}

\begin{definition}
If $C:I^k \rightarrow A$ is a singular k-cube in $A$ and $\omega$ is a k-form on $A$, then:
\[\int\limits_{C} \omega = \int\limits_{I^k} C^*(\omega)\]
\end{definition}

\begin{example}
$\omega$ 1-form on $\RR^2$, \[\omega = xdy\] $C:[0,1] \rightarrow \RR^2$, \[C(t)=(a\cos(2\pi t), b\sin(2\pi t)), \quad a,b>0.\]
\begin{align*}
\int\limits_{C} xdy &= \int\limits_{[0,1]} C^*(xdy) = \int_0^1 (x\cdot C)(t) \frac{dC^2}{dt} dt = \int_0^1 a \cos(2\pi t) \cdot b 2 \pi \cos(2\pi t) dt\\
&=  \int_0^1 ab 2 \pi \cdot \cos^2(2\pi t) dt =  ab \pi \int_0^1  1 + \cos(4\pi t) dt = \pi ab 
\end{align*}
Stokes Theorem:
\[\int\limits_{C} \omega = \int\limits_{\tilde{C}} d\omega = \int\limits_{\tilde{C}} d(xdy) = \int\limits_{\tilde{C}} dx\wedge dy \quad \leftarrow \text{area of reigon parameterised by }\tilde{C}\]
Call $\tilde{C}$ the inside of the ellipse (2-cube)
\[\tilde{C}(u,t) = (au \cos(2\pi t), bu \sin(2\pi t)), \quad t\in [0,1], \; u \in [0,1]\]
\[\partial \tilde{C} = C\]
\end{example}

\begin{definition}
If $C:I^k \rightarrow A$ is a singular k-cube in $A$ and $\omega$ is a k-form on $A$ then, 
\[\int\limits_{C} \omega = \int\limits_{I^k} C^*(\omega)\]
If $C$ is a singular k-chain, ie 
\[C= \sum_{j=1}^m m_jC_j \quad m_j \in \ZZ, \;C_j\;singular \; k-cubes\]
then
\[\int\limits_{C} \omega = \sum_{j=1}^m m_j\int\limits_{C_j}\omega = \sum_{j=1}^m m_j\int\limits_{I^k}C_j^*(\omega )\]
\end{definition}

\setcounter{equation}{0}

\begin{theorem}[Stoke's Theorem for Singular k-chains in $\RR^k$]
If $\omega$ is a (k-1)-form on $\RR^k$, $d\omega$ is a k-form on $\RR^k$, $C$ a k-singular chain, $\partial C$ a (k-1)-singular chain. Then:
\[\int\limits_{\partial C} \omega = \int\limits_{C} d\omega\]
$C= \sum_{j=1}^m m_jC_j \quad m_j \in \ZZ, \;C_j\;singular \; k-cube, \;C_j:I^k\rightarrow \RR^k$
\[\partial C_j = C_j(\partial I^k) = \sum_{i=1}^k \sum_{\alpha =0,1} (-1)^{i+\alpha} C_j (I_{(i,\alpha)}^k)\]
\begin{proof}
\begin{equation} \sum_{j=1}^m \sum_{i=1}^k \sum_{\alpha =0,1}m_j (-1)^{i+\alpha} \int\limits_{C_j (I_{(i,\alpha)}^k)} \overset{Def^n}{=} \sum_{j=1}^m \sum_{i=1}^k \sum_{\alpha =0,1} \left( m_j (-1)^{i+\alpha} \int\limits_{I_{(i,\alpha)}^k} C_j^*(\omega)\right) \end{equation}
Now compute 
\[
\int\limits_{C}d\omega = \sum_{j=1}^m m_j \int_{C_j} d\omega \overset{Def^n}{=} \sum_{j=1}^m m_j \int_{I^k} C_j^*(d\omega) =  \sum_{j=1}^m m_j \int_{I^k} d(C_j^*(\omega)) \]
since \[\boxed{d(C_j^*(\omega)) = C_j^*(d\omega)}\]
Now apply Stoke's theorem for standard k-cube
\begin{equation}
= \sum_{j=1}^m m_j \int_{\partial I^k} C_j^*(\omega) =  \sum_{j=1}^m m_j \sum_{i=1}^k \sum_{\alpha =0,1}   (-1)^{i+\alpha} \int\limits_{I_{(i,\alpha)}^k} C_j^*(\omega). \end{equation}
\[\therefore \; (1)=(2)\]
\end{proof}
\end{theorem}

\subsection{Classical Stoke's Theorem in $\RR^2$}
\[\int\limits_{\gamma} P(x,y) dx + Q(x,y)dy = \iint_{D} \left( - \frac{dP}{dy} + \frac{dQ}{dx} \right) dxdy\]
$s^1, s^2$ are axes for the 2-cube
 \[C(s^1, s^2) = (C^1(s^1, s^2), C^2(s^1,s^2))\]
$\gamma:[0,1] \rightarrow \RR^2$
\[\gamma(t) = (\gamma^1(t), \gamma^2(t))\]
\[\partial C \overset{Def^n}{=} C(\partial I^2) = \gamma \]
\begin{align*}
\int\limits_{\gamma} P dx + Qdy &= \int\limits_{C(\partial I^2)} P dx + Qdy \overset{Def^n}{=} \int\limits_{\partial I^2} C^*(P dx + Qdy )\\
&= \int\limits_{\partial I^2} P(C^1(s^1, s^2), C^2(s^1,s^2))C^*(dx) +  Q(C^1(s^1, s^2), C^2(s^1,s^2))C^*(dy) \\
&= \int\limits_{\partial I^2} P \frac{d\gamma^1}{dt}dt + Q \frac{d\gamma^2}{dt}dt = \int\limits_{\partial I^2} \left[ P(\gamma^1(t), \gamma^2(t)) \frac{d\gamma^1}{dt} + Q(\gamma^1(t), \gamma^2(t)) \frac{d\gamma^2}{dt}\right]dt
\end{align*}
\[\int\limits_{\underset{= \partial C}{\gamma}} P dx + Qdy \underset{in \;\RR^2}{\overset{Stoke's}{=}} \int\limits_{C} \underbracket{d( P dx + Qdy)}_{d\omega} \]
where $\gamma$ is the the singular 1-cube which is the boundry of $C(I^k)$
\[\therefore \; \int\limits_{\partial C} \omega = \int\limits_{C} d \omega\]
\begin{align*}
\int\limits_{C} d(P dx + Qdy) &= \int\limits_{C}\cancel{P_x dx \wedge dx} + P_y dy \wedge dx + Q_x dx \wedge dy + \cancel{Q_y dy \wedge dy}\\
&= \int\limits_{C} - \frac{dP}{dy} dx \wedge dy + \frac{dQ}{dy} dx \wedge dy  = \int\limits_{C} \left( - \frac{dP}{dy}+ \frac{dQ}{dy} \right) dx \wedge dy\\
&\overset{def^n}{=} \int\limits_{I^2} C^*\left( - \frac{dP}{dy}+ \frac{dQ}{dy} \right) dx \wedge dy \\ &= \int\limits_{I^2} \left[ - \frac{dP}{dy}(C^1, C^2)+ \frac{dQ}{dy}(C^1, C^2) \right] C^*(dx \wedge dy) \qquad (*)
\end{align*}
\begin{framed} 
\begin{proposition}[What is $C^*(dx \wedge dy)$?]
\begin{align*}
C^*(dx \wedge dy) &= C^*(dx) \wedge C^*( dy)\\
&=(\tfrac{dC^1}{ds^1}ds^1 + \tfrac{dC^1}{ds^2}ds^2) \wedge (\tfrac{dC^2}{ds^1}ds^1 + \tfrac{dC^2}{ds^2}ds^2)\\
&= \tfrac{dC^1}{ds^1}\tfrac{dC^2}{ds^2}ds^1 \wedge ds^2 + \tfrac{dC^1}{ds^2}\tfrac{dC^2}{ds^1}ds^2 \wedge ds^1 \\
&= \left(\tfrac{dC^1}{ds^1}\tfrac{dC^2}{ds^2} - \tfrac{dC^1}{ds^2}\tfrac{dC^2}{ds^1} \right)ds^1 \wedge ds^2
&= det\left[C'(s^1,s^2)\right]ds^1 \wedge ds^2
\end{align*}
where 
\[C'(s^1,s^2) = \begin{bmatrix} \tfrac{dC^1}{ds^1} & \tfrac{dC^1}{ds^2} \\ \tfrac{dC^2}{ds^1} & \tfrac{dC^2}{ds^2} \end{bmatrix}\]
\end{proposition}
\end{framed}
\[(*) = \int\limits_{I^2} \left[ - \frac{dP}{dy}(C^1, C^2)+ \frac{dQ}{dy}(C^1, C^2) \right]  det\left(C'(s^1,s^2)\right)ds^1 \wedge ds^2\]
\[\therefore \; ordinary \;double \; integral\]
Recall the change of variables formula for n-dim integrals (2 in this case):\\
\begin{framed}
$A \subseteq \RR^n$, $g:A\rightarrow \RR^n$ injective and differentiable $det[g'(x)] \neq 0 \; \forall x \in A$. If $f:g(A) \rightarrow \RR$ is integrable 
\[\int\limits_{g(A)} f = \int\limits_{A} (f \circ g) |det\;g'|\]
\end{framed}
Assuming that anticlockwise orientation it can be shown that $det(C'(s^1,s^2)) >0$
\[\int\limits_{I^2} \left( - \frac{dP}{dy}(C^1, C^2)+ \frac{dQ}{dy}(C^1, C^2) \right)  det\left(C'(s^1,s^2)\right)ds^1 ds^2 = \int\limits_{D} \left( - \frac{dP}{dy}(x, y)+ \frac{dQ}{dy}(x,y) \right) dx dy\]

\begin{theorem}[Gauss or Divergence Theorem]
Solid $T$ in $\RR^3$ with boundry surface $S$ and a vector function $F = (F^1, F^2, F^3)$
\begin{align*}
S_x &\equiv \text{ Tangent plane to the solid at point } x \in S\\
n(x) & \equiv \text{ Outward unit normal vector}
\end{align*}
\[\int\limits_{S}\langle \vec{F}, \vec{\hat{n}} \rangle dA = \iiint\limits_{T} (div \, \vec{F}) dxdydz\]
\end{theorem}
$S_x$ has dim 2 (tangent plane at $x$)
\[dim\,\Lambda^2(S_x) =1\]
$v,w \in S_x$ 
\[\omega(v,w) = (v \times w)\cdot n = \langle v \times w , n \rangle \quad \text{Scalar triple product}\]
\[\omega(v,w) = \begin{vmatrix} v^1 &v^2 &v^3 \\ w^1 &w^2 &w^3 \\ n^1 &n^2 &n^3 \end{vmatrix}= \begin{vmatrix} 
n^1 &n^2 &n^3 \\ v^1 &v^2 &v^3 \\ w^1 &w^2 &w^3 \end{vmatrix}\]
Choose $\bar{a}, \bar{b} \in S_x$ such that $\bar{a}, \bar{b}, \bar{n}$ are an orthonormal system, right-handed. 
\begin{notation}[Orthonormal System]
Call $\omega(v,w) = dA(v,w)$ or $\omega = dA$ where $\omega(\bar{a}, \bar{b}) = 1$ \end{notation}

\begin{theorem}
\[dA = n^1dy\wedge dz+ n^2dz\wedge dx +n^3dx\wedge dy\]
\begin{proof}
\[dA(v,w) = \begin{vmatrix} n^1 &n^2 &n^3 \\ v^1 &v^2 &v^3 \\ w^1 &w^2 &w^3 \end{vmatrix} =n^1(v^2w^3 - w^2v^3) + n^2(-v^1w^3 + v^3w^1) + n^3(v^1w^2 - v^2w^1)\]
\begin{align*}
(dy \wedge dz)(v,w) &= (dy \otimes dz - dz \otimes dy)(v,w)\\
&= dy(v)dz(w) - dz(v)dy(w)\\
&= v^2w^3 - w^2v^3\\
(dx \wedge dz)(v,w) &=-v^1w^3 + v^3w^1\\
(dx \wedge dy)(v,w) &=v^1w^2 - v^2w^1
\end{align*}
\end{proof}
\end{theorem}

\begin{theorem}
\begin{align*}
n^1dA &= dy\wedge dz\\
n^2dA &= dz\wedge dx\\
n^3dA &= dx\wedge dy
\end{align*}
\begin{proof}
\[(*)\qquad (dy \wedge dz)(v,w)= v^2w^3 - w^2v^3\qquad where\; v,w \in S_x\]
Since $v$ and $w$ are perpendicular to $n$, then $v \times w = \lambda n$, $\lambda \in \RR$. 
\[n^1dA(v,w) = n^1 (\lambda n,n) = n^1 \lambda \qquad since\;|n|=1\]
\[\langle v \times w, i \rangle = \langle \lambda n, i \rangle = \lambda n^1\]
\[\langle v \times w, i \rangle = \begin{vmatrix} i &j &k \\ v^1 &v^2 &v^3 \\ w^1 &w^2 &w^3 \end{vmatrix} \cdot i =  v^2w^3 - w^2v^3 = (dy \wedge dz)(v,w) \quad by \;(*)\]
\[\therefore \;n^1dA = dy\wedge dz\]
Similarly for other equations.
\end{proof}
\end{theorem}

\begin{proof}[Proof of Divergence Theorem]
Given $\bar{F} = (F^1, F^2, F^3) = F^1\underline{i}+ F^2\underline{j} + F^3\underline{k}$
\[div(\bar{F}) = \frac{dF^1}{dx} + \frac{dF^2}{Dy} + \frac{dF^3}{dz}\]
To $\bar{F}$ assign the 2-form:
\[\eta = F^1 dy \wedge dz + F^2 dz \wedge dx + F^1 dx \wedge dy \quad (2-form)\]
Calculate:
\[d\eta = \frac{dF^1}{dx} dx \wedge dy \wedge dz + \frac{dF^2}{dy} dy \wedge dz \wedge dx + \frac{dF^3}{dz} dz \wedge dx \wedge dy\]
\[\underset{3-form}{d\eta} =\left( \frac{dF^1}{dx} + \frac{dF^2}{dy} + \frac{dF^3}{dz} \right) dx \wedge dy \wedge dz \]
\[\int\limits_{T}d\eta =\int\limits_{T}\left(div \; F \right) dx \wedge dy \wedge dz \overset{*}{=} \iiint\limits_{T}f div\;F \cdot dxdydz \] * Change of variables for $I^3$ to $T$. By Stoke's theorem
\begin{align*}
\int\limits_{T}d\eta =\int\limits_{\partial T}\eta &= \int\limits_{S}F^1dy\wedge dz+ F^2dz\wedge dx + F^3dx\wedge dy\\
&= \int\limits_{S}F^1n^1dA+ F^2 n^2 dA+ F^3 n^3 dA\\
&= \int\limits_{S} (F^1n^1+ F^2 n^2 + F^3 n^3) dA\\
&= \int\limits_{S} \bar{F}\cdot \bar{n} dA 
\end{align*}
\end{proof}


\begin{recall}

\begin{definition}
A set $M$ is a K-dim manifold in $\RR^n$ if the following condition (M) holds. For every $x \in M$:\\
(M): There exisits two open sets $U, \; V$ of $\RR^n, z\; x \in U$ and a diffeomorphsim $h:U \rightarrow V$ such that:
\[ h(U \cap M) = \{ y \in V \; s.t. \;y^{k+1} = y^{k+2} = \dots = y^{n} = 0\} \]
\end{definition}  

\begin{definition}
A subset $M$ of $\RR^n$ is a $k$-dimensional manifold iff for every point $x \in M$ the following holds:\\
(C) There exists an open set $U \in \RR^n,\; x \in U$ and an open set $W \subset \RR^k$ and an injective
differentiable map $f : W \rightarrow \RR^n$ such that
\begin{enumerate}[(i)]
\item $f(W) = U \cap M$
\item rank $f'(y) = k \quad \forall y \in W$
\item $f^{-1} : f(W) \rightarrow W$ is continuous.
\end{enumerate}
\end{definition}

\begin{definition} 
A subset $M$ of $\RR^n$ is a $k$-dimensional manifold with boundary if $\forall x \in M$ either 
(M) holds or (exclusive)
(M') holds\\ \quad \\
(M') $\exists$ open set $U$ of $\RR^n$ containing $x$, an open set $V$ contained in $\RR^n$ and a
diffeomorphism $h : U \rightarrow V$ such that
\[h(U \cap M) = V \cap (\mathbb{H}^k\times \{0\}) = \{ y \in V: \; y^k \geq 0, \;y^{k+1} = y^{k+2} = \dots = y^{n} = 0\}.\]
Moreover, $h^k(x) = 0$.The set of points where condition (M') holds is called the boundary of $M$ and is denoted
by $\partial M$.
\end{definition}
\end{recall}

\begin{definition}
$\forall v \in \RR^k$ 
\[(a,v) \rightarrow (f(a), Df(a)(v)) \in \RR_{f(a)}^n = \RR_{x}^n\]
$(a,v)$ is pushed forward to give a vector in $\RR_x^n$. $f:W \rightarrow U\cap M$, $v_a = (a,v)$. Let $a \in W$ such that $f(a)=x$ then 
\[f_*(v_a) = (x, Df(a)(v)) \in \RR_x^n\]
$\RR_a^k = \{(a,v) : v \in \RR^k\}$ a vector space.
\end{definition}

\begin{definition}
 The tangent space of $M$ at $x$ is defined to be $M_x = f_*(\RR_a^k)$ $dim\;M_x = k, \; given\; x=f(a), \;f$ is a chart.
\end{definition}

\begin{definition}
A Vector field on $M$ is a function $F$ on$M$ such that $\forall x \in M$: \[F(x) \in M_x.\] 
\end{definition}

Let $x= f(a), \; f:W \rightarrow U\;f(W) = U \cap M$. Let $G(a) \in \RR_a^k$ such that: \[f_*(G(a)) = F(f(a)) = F(x)\] $G(a)$ is unique, since $f_*:\RR_a^k \rightarrow M_x$ is injective

\begin{definition}
A vector field on $M$ is called continuous (or differentiable) if $\forall x \in M$ the vector field $G$ on $W$ is continuous (or differentiable).
\end{definition}

\begin{definition}
$\omega$ is a (differential) p-form on $M$ if $\forall x \in M$
\[\omega(x) \in \Lambda^p(M_x)\]
Then $f^*(\omega)$ is (differentiable) p-form on $W$. 
\end{definition}

If $f^*(\omega)$ is differential then $\omega$ is differential on $W \subseteq \RR^k$. 
\begin{definition}
If $\omega$ is a p-form on $M$ which k-dim in $\RR^n, \; x\in M$.
\[\omega(x) = \sum_{i_1 < \dots <i_p} \omega_{i_1, \cdots, i_p} (x) dx^{i_1} \wedge \cdots \wedge dx^{i_p} \]
$\omega$ is continuous if $f^*(\omega)$ is continuous on $W$\\
$\omega$ is differentiable if $f^*(\omega)$ is differentiable on $W$
\end{definition}
We have difficulty with $D_j(\omega_{i_1\cdots i_p})$ since $\omega_{i_1\cdots i_p}$ is not defined on an open set $U \ni x$

\begin{theorem}
Given a differential p-form on $M$ which is k-dim in $\RR^n$, there exists a unique differential (p+1)-form $d\omega$ on $M$ such that $\forall x \in M$
\[d(f^*(\omega)) = f^*(d\omega)\]
and $f:W\rightarrow U \cap M$ is a chart. $d(\omega) \in \Lambda^{p+1}(M_x), \; v_i \in M_x, \;d\omega (x)(v_1, \dots , v_{p+1})$. Since $f_*$ is a bijection, $f_*:\RR_{a}^k \rightarrow M_x$,  $\exists$ unique vectors $w_1, \dots, w_{p+1} \in \RR_{a}^k$ such that 
\[f_*(w_i) = v_i\]
\[d\omega(x)(v_1, \dots, v_{p+1}) = d\underbrace{f^*(\omega)(a)}_{\mathclap{\in \Lambda^{p+1}(\RR_a^k)}}(w_1, \dots, w_{p+1})\]
\end{theorem}

\begin{aim}
To understand Stoke's theorem for $M$ k-dim manifold in $\RR^n$ with boundry $\partial M$
\[\int\limits_{\partial M} \omega + \int\limits_{M} d\omega\]
where $\omega$ is a differential (k-1)-form on $M$.
\end{aim}

\begin{definition}[Orientation on Vector spaces ]
\begin{align*}
Bases &: \mathcal{F}  =\{v_1, \dots  , v_n\}\\
 &\, \mathcal{B} = \{w_1, \dots, w_n\}
\end{align*}
We say $\mathcal{F}$ and $\mathcal{B}$ define the same orientation if $det [Id]_{\mathcal{F}}^{\mathcal{B}} >0$ and oppisite oientation if $det [Id]_{\mathcal{F}}^{\mathcal{B}} <0$. Also we have \[det [Id]_{\mathcal{F}}^{\mathcal{B}} = \left[det [Id]_{\mathcal{B}}^{\mathcal{F}}\right]^{-1}\]
$\mathcal{F} \sim \mathcal{B}$ iff they have the same orientation. This is an equlivalence relation.\\
Standard orientation on $\RR^n$: $\mathcal{F} = [e_1, \dots , e_n]$

\begin{example} $\{e_1, e_2\}$ has oppisite orientation to $\{e_2, e_1\}$ \end{example}

This standard orientation is denoted \[\mu = [e_1, \dots, e_n]\]
When $f(a)=x$ on $\RR_a^k$ we have the standard basis $[(e_1)_a, \dots , (e_k)_a]$. \\Basis for $M_x:[f_*((e_1)_a), \dots , f_*((e_k)_a)]$
\[\mu_x = [f_*((e_1)_a), \dots , f_*((e_k)_a)]\]
If $b \in W$, then 
\[\mu_{f(b)} = [f_*((e_1)_b), \dots , f_*((e_k)_b)]\]
If we have $z=f(c)$ and $z= f(d)$ we assign two orientations at $z$
\[ [f_*((e_1)_c), \dots , f_*((e_k)_c)]  = [g_*((e_1)_d), \dots , g_*((e_k)_d)\}\]
If the two orientations are equal, ie $det (Id) >0$ on these two bases, then we say $f$ and $g$ define consistent orientations at point $z$. Hopefully this is true on $f(\omega)\cap g(\omega')$ then we call the two orientations consistent. \\\quad \\ If there exists consistent orientation on all of $M$, we say that $M$ is orientable and the manifold is oriented once we fix orientation. If $S$ is a surface and the manifold is oriented once we fix orientation. If $S$ is a surface in $\RR^3$ which is orientable, let 
\[\mu_x = [v_1, v_2] \quad x \in S \;(2-manifold)\]
Draw the line perpendicular to $S_x$ at $x$.  Pick a unit vector in $n(x)$ such that $[n(x), v_1, v_2]$ is that standard orientation in $\RR^3$, then $n(x)$ is the outer unit normal.\\
$M$ is  a k-dim manifold with boundry in $\RR^3$. $(\partial M)_x$ has a basis
\[[f_*((e_1)_a), \dots , f_*((e_{k-1})_a) ] \] then let $v_0 \in \RR_a^k$ such that $f_*(v_0)$ is perpencular at $B$, then $|f_x(v_0)| =1$ and $n(x) = f_x(v_0)$
\end{definition}

\begin{definition}[Integrals]
Let $C$ be a singular p-cube on $m$ k-dim manifold. $C:I^k \rightarrow M$. Let $\omega$ be a p-form on $M$. We define:
\[\int\limits_{C} \omega \underset{pullback}{=} \int\limits_{I^k} C^*(\omega)\]
If $C$ is a k-cube in $M$ a k-manifold and $I^k \subseteq W$, $f:W\rightarrow U\cap M$ is the chart and $C(x) = f(x) \; \forall x \in I^k$. if $f$ is perserving orientation, then we say $C$ is orientation preserving singular k-cube on $M$. If $\omega$ is a k-form on $M$ with $\omega(y)=0, \; \forall y \in C(I^k)$ then we define 
\[\int\limits_{M} \omega \underset{Def^n}{=} \int\limits_{C} \omega\]
\[\boxed{d(f^*(\omega)) = f^*(d\omega) \quad f_1^*(\omega) \; a \;k-1-form \; on \; \RR^k}\]
can partition $W$ in to sections $W_i$ then 
\[\int\limits_{M} \omega = \int\limits_{f(W_1)} \omega + \dots + \int\limits_{f(W_i)} \omega + \dots\]
use partitions of unity to define $\int\limits_{M} \underset{k-form}{\omega}, \; \int\limits_{\partial M} \underset{(k-1)-form}{\eta}$
\end{definition}

\begin{theorem}
Let $M$ be a compact, oriented k-manifold with boundry $\partial M$ and $\omega$ be a differential (k-1)-form on $M$, then:
\[\int\limits_{\partial M} \omega = \int\limits_{M} d\omega\]
\begin{proof}
\begin{recall}[Classical Stoke's theorem]
\begin{itemize}
\item $M$ is a oriented 2-dim manifold with boundary
\item $F$ differentiable vector field on $M$
\end{itemize}
\[\int\limits_{\partial M} \bar{F} \cdot \bar{T} ds  = \int\limits_{M} curl \vec{F} \cdot \underline{\vec{n}}dA\]
\end{recall}
Let $M$ be a compact orientated, 2-dim manifold with bounry $\partial M$ in $\RR^3$. Let $T$ be a vector field on $\partial M$ such that $ds(M) = 1$ where $ds$ is the length element of $\partial M$, $\vec{F}$ be a differentiable vector fied on an open set containing $M$, $\vec{n}$ be the outer unit normal on $M$. Then 
\[\int\limits_{\partial M} \vec{F} \cdot \vec{T} ds  = \int\limits_{M} curl \vec{F} \cdot \underline{\vec{n}}dA\]
if \[F=(F^1, F^2 , F^3) = F^1 \underline{i} + F^2 \underline{j}  + F^3 \underline{k} \]
we define 1-form \[\omega = F^1 dx + F^2 dy +F^3dz \] then we calculate 
\begin{align*}
d\omega &= \frac{dF^1}{dy} dy \wedge dx + \frac{dF^1}{dz} dz \wedge dx + \frac{dF^2}{dx} dx \wedge dy  + \frac{dF^2}{dz} dz \wedge dy + \frac{dF^3}{dx} dx \wedge dz + \frac{dF^3}{dy} dy \wedge dz \\
&= G^1 dy \wedge dz + G^2 dz \wedge dx + G^3 dx \wedge dy \end{align*}
\[G^1 \underline{i} + G^2 \underline{j}  + G^3 \underline{k} = curl (F) \]
\begin{framed} We Know
\begin{align*}
n^1dA &= dy\wedge dz\\
n^2dA &= dz\wedge dx\\
n^3dA &= dx\wedge dy
\end{align*}
\end{framed}
\begin{align*}
\int\limits_{M}   G^1 dy \wedge dz + G^2 dz \wedge dx + G^3 dx \wedge dy  & = \int\limits_{M} \left( G^1n^1 + G^2 n^2 + G^3 n^3\right) dA \\
&= \int\limits_{M} \vec{G} \cdot \vec{n} dA \underset{ of \, G}{\overset{Def^n}{=}} \int\limits_{M} curl (F) \cdot \vec{n} \end{align*}
According to Stoke's general theorem
\[\int\limits_{\partial M} \omega = \int\limits_{M} d\omega = \int\limits_{M} curl(\vec{F}) \cdot \vec{n} dA\]
Since $ds(T) =1$, we can prove as in that \begin{align*}
dx &= T^1 ds\\
dy &= T^2 ds\\
dz &= T^3 ds
\end{align*}
\begin{align*}
\int\limits_{\partial M} \omega &= \int\limits_{\partial M} F^1 dx + F^2 dy + F^3 dz \\
&= \int\limits_{\partial M} F^1 T^1 ds + F^2 T^2ds  + F^3 T^3 ds\\
&= \int\limits_{\partial M} \vec{F} \cdot \vec{T} ds
\end{align*}
\end{proof}
\end{theorem}


\end{document}